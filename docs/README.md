# **Llama Nuts and Bolts: Documentation**

Here is the adventure of generation of the next tokens from start to finish, documented step by step as:

[0. THE JOURNEY](./00-THE-JOURNEY.md)
<br>
[1. INITIALIZATION](./01-INITIALIZATION.md)
<br>
[2. LOADING TORCH MODEL](./02-LOADING-TORCH-MODEL.md)
<br>
[3. LOADING TORCH MODEL \(DETAILS\)](./03-LOADING-TORCH-MODEL-DETAILS.md)
<br>
[4. LOADING MODEL ARGS](./04-LOADING-MODEL-ARGS.md)
<br>
[5. LOADING TOKENIZER MODEL](./05-LOADING-TOKENIZER-MODEL.md)
<br>
[6. OBSOLETE - LOADING LLAMA 2 TOKENIZER MODEL](./06-OBSOLETE-LOADING-LLAMA-2-TOKENIZER-MODEL.md)
<br>
[7. BFLOAT16 DATA TYPE](./07-BFLOAT16-DATA-TYPE.md)
<br>
[8. TENSOR](./08-TENSOR.md)
<br>
[9. IMPLEMENTING LLAMA MODEL ARCHITECTURE](./09-IMPLEMENTING-LLAMA-MODEL-ARCHITECTURE.md)
<br>
[10. RoPE \(ROTARY POSITIONAL EMBEDDINGS\)](./10-ROPE-ROTARY-POSITIONAL-EMBEDDINGS.md)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[10.BONUS. PRECOMPUTING FREQUENCY TENSOR \(Python Notebook\)](./10.BONUS-PRECOMPUTING-FREQUENCY-TENSOR.ipynb)
<br>
[11. ASKING FOR USER INPUT](./11-ASKING-FOR-USER-INPUT.md)
<br>
[12. TOKENIZATION](./12-TOKENIZATION.md)
<br>
[13. GENERATING NEXT TOKENS](./13-GENERATING-NEXT-TOKENS.md)
<br>
[14. MAKING PREDICTION with LLAMA MODEL - 1](./14-MAKING-PREDICTION-WITH-LLAMA-MODEL-1.md)
<br>
[15. MAKING PREDICTION with LLAMA MODEL - 2](./15-MAKING-PREDICTION-WITH-LLAMA-MODEL-2.md)
<br>
[16. MAKING PREDICTION with LLAMA MODEL - 3](./16-MAKING-PREDICTION-WITH-LLAMA-MODEL-3.md)
<br>
[17. UNICODE, UTF-8 and EMOJIS](./17-UNICODE-UTF-8-EMOJIS.md)
<br>
[18. CONCLUSION](./18-CONCLUSION.md)
<br>
[19. REFERENCES](./19-REFERENCES.md)
<br>
[20. DIAGRAMS](./20-DIAGRAMS.md)
<br>

---

<div align="right">

[&lt;&lt;&nbsp;&nbsp;Home: README](../README.md)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Next chapter: THE JOURNEY&nbsp;&nbsp;&gt;](./00-THE-JOURNEY.md)

</div>

# **19. REFERENCES**

I want to thank to contributors of the awesome sources which were referred during development of this project and writing this documentation. You can find these sources below, also in between the lines in code and documentation.

## **Websites**

* [Wikipedia](https://en.wikipedia.org)
* [PyTorch Documentation](https://pytorch.org/)
* Several documents, articles, and code samples: In the code and documentation of this project, you can find several code or document links that were cited.
* [Unicode.org website](https://home.unicode.org/)
* [Codepoints.net website](https://codepoints.net/)
* [Unicode.org - Emoji Sequence Text Files](https://unicode.org/Public/emoji/15.1/)
* [Github Supported Emoji Sequence List JSON](https://raw.githubusercontent.com/github/gemoji/master/db/emoji.json)
* [Emoji dissector](https://emojidissector.com/)

## **Papers**

* [The Llama 3 Herd of Models](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/)
* [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)
* [Llama: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971v1)
* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* [Grouped Multi-Query Attention](https://paperswithcode.com/method/grouped-query-attention)
* RoFormer: Enhanced Transformer with Rotary Position Embedding: [Paper](https://arxiv.org/abs/2104.09864v5) | [Papers with Code](https://paperswithcode.com/paper/roformer-enhanced-transformer-with-rotary) | [LabML Annotated Implementation](https://nn.labml.ai/transformers/rope/index.html)
* [Pre-RMSNorm (Root Mean Square Layer Normalization)](https://paperswithcode.com/method/rmsnorm)
* [Autoregressive Model - ScienceDirect](https://www.sciencedirect.com/topics/mathematics/autoregressive-model)

## **Github Projects**

### Large Language Models

* [Meta Llama website](https://llama.meta.com/)
* [Original Llama 3.1 Python package repository of Meta](https://github.com/meta-llama/llama-models/)
* [Original Llama Toolchain Python repository of Meta](https://github.com/meta-llama/llama-toolchain)
* [Georgi Gerganov](https://github.com/ggerganov)'s [llama.cpp](https://github.com/ggerganov/llama.cpp)
* [HuggingFace model: meta-llama/Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/tree/main)

### Inspired Python Projects

* [Pickle](https://github.com/python/cpython/blob/main/Lib/pickle.py)
* [Tiktoken Rust code](https://github.com/openai/tiktoken/blob/1b9faf2779855124f05174adf1383e53689ed94b/src/lib.rs)

### Inspired Go Projects

* [Emoji - Go library lets you use emoji characters in strings](https://github.com/enescakir/emoji)

## **Wikipedia and Concept Definitions**

### Computing Concepts

* [Memory Mapping](https://en.wikipedia.org/wiki/Memory-mapped_file)
* [BFloat16 (Brain Floating Point)](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)
* [The bfloat16 numerical format](https://cloud.google.com/tpu/docs/bfloat16)
* [Floating-point arithmetic - Wikipedia](https://en.wikipedia.org/wiki/Floating-point_arithmetic)
* [Inline functions](https://www.geeksforgeeks.org/inline-functions-cpp/)
* [Endianness (Wikipedia)](https://en.wikipedia.org/wiki/Endianness)
* [What is Endianness?](https://www.freecodecamp.org/news/what-is-endianness-big-endian-vs-little-endian/)

### Machine Learning Concepts

* [Tensor](https://en.wikipedia.org/wiki/Tensor_%28machine_learning%29)
* [Cis \(mathematics\)](https://en.wikipedia.org/wiki/Cis_%28mathematics%29)
* [Polar Coordinate System](https://en.wikipedia.org/wiki/Polar_coordinate_system)
* [Logits](https://en.wikipedia.org/wiki/Logit)
* [Argmax](https://en.wikipedia.org/wiki/Arg_max)
* [Softmax](https://en.wikipedia.org/wiki/Softmax_function)
* [Temperature](https://www.promptingguide.ai/introduction/settings)

### Unicode Concepts

* [American Standard Code for Information Interchange](https://en.wikipedia.org/wiki/ASCII)
* [Code page](https://en.wikipedia.org/wiki/Code_page)
* [Unicode](https://en.wikipedia.org/wiki/Unicode)
* [Unicode Consortium - Unicode, Inc.](https://en.wikipedia.org/wiki/Unicode_Consortium)
* [Code points](https://pro.arcgis.com/en/pro-app/3.1/help/data/geodatabases/overview/a-quick-tour-of-unicode.htm)
* [Rune](https://www.geeksforgeeks.org/rune-in-golang/)
* [UTF-8](https://en.wikipedia.org/wiki/UTF-8), [UTF-16](https://en.wikipedia.org/wiki/UTF-16), [UTF-32](https://en.wikipedia.org/wiki/UTF-32)
* [Zero-width joiner](https://en.wikipedia.org/wiki/Zero-width_joiner)

## **Package Documentations**

### Go Packages

* [unsafe package of Go](https://pkg.go.dev/unsafe)
* [binary.LittleEndian](https://pkg.go.dev/encoding/binary)
* [Context package](https://pkg.go.dev/context)
* [goroutines](https://gobyexample.com/goroutines)
* [Go Channels](https://go101.org/article/channel.html) and [go channels](https://gobyexample.com/channels)

### Inspired PyTorch Functions

* [torch.Tensor.stride](https://pytorch.org/docs/stable/generated/torch.Tensor.stride.html)
* [torch.polar](https://pytorch.org/docs/stable/generated/torch.polar.html)
* [torch.view_as_complex](https://pytorch.org/docs/stable/generated/torch.view_as_complex.html)
* [torch.nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)
* [torch.nn.SiLU](https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html)
* [torch.mean](https://pytorch.org/docs/stable/generated/torch.mean.html)
* [torch.rsqrt](https://pytorch.org/docs/stable/generated/torch.rsqrt.html)

## **Articles**

### Computing Concepts

* [To Bfloat or not to Bfloat? That is the Question!](https://www.cerebras.net/machine-learning/to-bfloat-or-not-to-bfloat-that-is-the-question)
* [How to Convert a Number from Decimal to IEEE 754 Floating Point Representation](https://www.wikihow.com/Convert-a-Number-from-Decimal-to-IEEE-754-Floating-Point-Representation)
* [C Contiguous Arrays](https://stackoverflow.com/questions/26998223/what-is-the-difference-between-contiguous-and-non-contiguous-arrays)

### Go Language

* [Garbage Collector of Go](https://tip.golang.org/doc/gc-guide)
* [Is it possible to inline function, containing loop in Golang?](https://stackoverflow.com/questions/45836981/is-it-possible-to-inline-function-containing-loop-in-golang)
* [How To Use Contexts in Go](https://www.digitalocean.com/community/tutorials/how-to-use-contexts-in-go)
* [How to wait for all goroutines to finish in Golang](https://codewithyury.com/golang-wait-for-all-goroutines-to-finish/)
* [Using WaitGroup in Golang](https://www.geeksforgeeks.org/using-waitgroup-in-golang/)
* [Writing a Stream API in Go](https://betterprogramming.pub/writing-a-stream-api-in-go-afbc3c4350e2).
* [Strings, bytes, runes and characters in Go](https://go.dev/blog/strings)

### Machine Learning Concepts

* [Diving into the Python Pickle format](https://spootnik.org/entries/2014/04/05/diving-into-the-python-pickle-formatt/)
* [Explain Pytorch Tensor.stride and Tensor.storage with code examples](https://zhang-yang.medium.com/explain-pytorch-tensor-stride-and-tensor-storage-with-code-examples-50e637f1076d)
* [What's Grouped-Query attention (GQA)? a paper from Google Research](https://aliissa99.medium.com/-a596e4d86f79)
* [A Guide on Word Embeddings in NLP](https://www.turing.com/kb/guide-on-word-embeddings-in-nlp)
* [Word Embeddings in Natural Language Processing(NLP)](https://www.theaidream.com/post/word-embeddings-in-natural-language-processing-nlp)
* [A guide to prompting Llama 2](https://replicate.com/blog/how-to-prompt-llama)
* [Matrix Multiplication](https://www.learnpdc.org/PDCBeginners/5-applications/matrix-multiply.html)
* [A Gentle Introduction to Positional Encoding in Transformer Models, Part 1](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)

### **Videos**

### Large Language Models

* [Youtube - Andrej Karpathy - Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)
* [Youtube - Andrej Karpathy - 1hr Talk - Intro to Large Language Models](https://www.youtube.com/watch?v=zjkBMFhNj_g)
* [Youtube - Umar Jamil - Llama explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU](https://www.youtube.com/watch?v=Mn_9W1nCFLo)
* [Youtube - DeepLearning Hero - RoPE (Rotary positional embeddings) explained: The positional workhorse of modern LLMs](https://www.youtube.com/watch?v=GQPOtyITy54)
* [Youtube - Serrano.Academy - The Attention Mechanism in Large Language Models](https://www.youtube.com/watch?v=OxCpWwDCDFQ)
* [Youtube - Serrano.Academy - The math behind Attention: Keys, Queries, and Values matrices](https://www.youtube.com/watch?v=UPtG_38Oq8o)
* [Youtube - Serrano.Academy - What are Transformer Models and how do they work?](https://www.youtube.com/watch?v=qaWMOYf4ri8)
* [Youtube - StatQuest with Josh Starmer - Decoder-Only Transformers, ChatGPTs specific Transformer, Clearly Explained!!!](https://www.youtube.com/watch?v=bQ5BoolX9Ag)

### Unicode Concepts

* [Youtube - Intermation - Computer Organization and Design Fundamentals - Ep 020: Unicode Code Points and UTF-8 Encoding](https://www.youtube.com/watch?v=tbdym9ZtepQ&list=PLxfrSxK7P38X7XfG4X8Y9cdOURvC7ObMF)
* [Youtube - Intermation - Computer Organization and Design Fundamentals - Ep 021: UTF-8 Encoding Examples](https://www.youtube.com/watch?v=c_hfKgektt4&list=PLxfrSxK7P38X7XfG4X8Y9cdOURvC7ObMF)
* [Youtube - Studying With Alex - Unicode, in friendly terms: ASCII, UTF-8, code points, character encodings, and more](https://www.youtube.com/watch?v=ut74oHojxqo)

### Other

* Modified "Bolt, bolts, construction icon" by Rafiico Creative Studio Co., Ltd. [CC BY](https://creativecommons.org/licenses/by/3.0/)
<br>

---

<div align="right">

[&lt;&nbsp;&nbsp;Previous chapter: CONCLUSION](./18-CONCLUSION.md)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Next chapter: DIAGRAMS&nbsp;&nbsp;&gt;](./20-DIAGRAMS.md)

</div>

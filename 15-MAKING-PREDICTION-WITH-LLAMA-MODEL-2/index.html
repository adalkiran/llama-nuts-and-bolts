
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A holistic way of understanding how Llama and its components run in practice, with code and detailed documentation.">
      
      
        <meta name="author" content="Adil Alper DALKIRAN">
      
      
        <link rel="canonical" href="https://adalkiran.github.io/llama-nuts-and-bolts/15-MAKING-PREDICTION-WITH-LLAMA-MODEL-2/">
      
      
        <link rel="prev" href="../14-MAKING-PREDICTION-WITH-LLAMA-MODEL-1/">
      
      
        <link rel="next" href="../16-MAKING-PREDICTION-WITH-LLAMA-MODEL-3/">
      
      
      <link rel="icon" href="../assets/icon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.31">
    
    
      
        <title>MAKING PREDICTION with LLAMA MODEL - 2 - Llama Nuts and Bolts</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.3cba04c6.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../stylesheets/custom.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config","G-05VMCF3NF0"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","G-05VMCF3NF0",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-05VMCF3NF0",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
      
        <meta  property="og:type"  content="website" >
      
        <meta  property="og:title"  content="MAKING PREDICTION with LLAMA MODEL - 2 - Llama Nuts and Bolts" >
      
        <meta  property="og:description"  content="A holistic way of understanding how Llama and its components run in practice, with code and detailed documentation." >
      
        <meta  property="og:image"  content="https://adalkiran.github.io/llama-nuts-and-bolts/assets/images/social/15-MAKING-PREDICTION-WITH-LLAMA-MODEL-2.png" >
      
        <meta  property="og:image:type"  content="image/png" >
      
        <meta  property="og:image:width"  content="1200" >
      
        <meta  property="og:image:height"  content="630" >
      
        <meta  property="og:url"  content="https://adalkiran.github.io/llama-nuts-and-bolts/15-MAKING-PREDICTION-WITH-LLAMA-MODEL-2/" >
      
        <meta  name="twitter:card"  content="summary_large_image" >
      
        <meta  name="twitter:title"  content="MAKING PREDICTION with LLAMA MODEL - 2 - Llama Nuts and Bolts" >
      
        <meta  name="twitter:description"  content="A holistic way of understanding how Llama and its components run in practice, with code and detailed documentation." >
      
        <meta  name="twitter:image"  content="https://adalkiran.github.io/llama-nuts-and-bolts/assets/images/social/15-MAKING-PREDICTION-WITH-LLAMA-MODEL-2.png" >
      
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#15-making-prediction-with-llama-model-2" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Llama Nuts and Bolts" class="md-header__button md-logo" aria-label="Llama Nuts and Bolts" data-md-component="logo">
      
  <img src="../assets/icon.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Llama Nuts and Bolts
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              MAKING PREDICTION with LLAMA MODEL - 2
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
    
      <div class="md-header__source">
        <a href="https://github.com/adalkiran/llama-nuts-and-bolts" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    adalkiran/llama-nuts-and-bolts
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href=".." class="md-tabs__link">
          
  
  Llama Nuts and Bolts

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../webrtc-nuts-and-bolts" class="md-tabs__link">
        
  
    
  
  WebRTC Nuts and Bolts

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="https://www.linkedin.com/in/alper-dalkiran/" class="md-tabs__link">
        
  
    
  
  Contact

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Llama Nuts and Bolts" class="md-nav__button md-logo" aria-label="Llama Nuts and Bolts" data-md-component="logo">
      
  <img src="../assets/icon.svg" alt="logo">

    </a>
    Llama Nuts and Bolts
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/adalkiran/llama-nuts-and-bolts" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    adalkiran/llama-nuts-and-bolts
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" checked>
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Llama Nuts and Bolts
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Llama Nuts and Bolts
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    HOME
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../00-THE-JOURNEY/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    THE JOURNEY
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01-INITIALIZATION/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    INITIALIZATION
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02-LOADING-TORCH-MODEL/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LOADING TORCH MODEL
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-LOADING-TORCH-MODEL-DETAILS/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LOADING TORCH MODEL (DETAILS)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04-LOADING-MODEL-ARGS/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LOADING MODEL ARGS
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05-LOADING-TOKENIZER-MODEL/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LOADING TOKENIZER MODEL
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06-OBSOLETE-LOADING-LLAMA-2-TOKENIZER-MODEL/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    OBSOLETE - LOADING LLAMA 2 TOKENIZER MODEL
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07-BFLOAT16-DATA-TYPE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BFLOAT16 DATA TYPE
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../08-TENSOR/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TENSOR
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../09-IMPLEMENTING-LLAMA-MODEL-ARCHITECTURE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    IMPLEMENTING LLAMA MODEL ARCHITECTURE
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../10-ROPE-ROTARY-POSITIONAL-EMBEDDINGS/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    RoPE (ROTARY POSITIONAL EMBEDDINGS)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11-ASKING-FOR-USER-INPUT/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ASKING FOR USER INPUT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../12-TOKENIZATION/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TOKENIZATION
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../13-GENERATING-NEXT-TOKENS/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GENERATING NEXT TOKENS
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../14-MAKING-PREDICTION-WITH-LLAMA-MODEL-1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MAKING PREDICTION with LLAMA MODEL - 1
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    MAKING PREDICTION with LLAMA MODEL - 2
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    MAKING PREDICTION with LLAMA MODEL - 2
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#151-performing-forward-pass-through-attention-prenormalization-rmsnormforward" class="md-nav__link">
    <span class="md-ellipsis">
      15.1. Performing Forward Pass Through Attention Prenormalization - RMSNorm.Forward(...)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#152-performing-forward-pass-through-attention-module-calling" class="md-nav__link">
    <span class="md-ellipsis">
      15.2. Performing Forward Pass Through Attention Module - Calling
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#153-performing-forward-pass-through-attention-module-llamaattentionforward" class="md-nav__link">
    <span class="md-ellipsis">
      15.3. Performing Forward Pass Through Attention Module - LlamaAttention.Forward(...)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="15.3. Performing Forward Pass Through Attention Module - LlamaAttention.Forward(...)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1531-recall-structure-of-llamaattention" class="md-nav__link">
    <span class="md-ellipsis">
      15.3.1. Recall: Structure of LlamaAttention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1532-structure-to-run-multiple-linear-transformations" class="md-nav__link">
    <span class="md-ellipsis">
      15.3.2. Structure to run multiple linear transformations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1533-calculating-xq-xk-and-xv" class="md-nav__link">
    <span class="md-ellipsis">
      15.3.3. Calculating xq, xk, and xv
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1534-do-reshapings" class="md-nav__link">
    <span class="md-ellipsis">
      15.3.4. Do reshapings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1535-apply-rotary-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      15.3.5. Apply Rotary Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1536-update-kv-cache" class="md-nav__link">
    <span class="md-ellipsis">
      15.3.6. Update KV cache
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1537-retrieve-cached-kv-so-far" class="md-nav__link">
    <span class="md-ellipsis">
      15.3.7. Retrieve cached KV so far
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1538-repeat-kv-heads-if-n_kvheads-n_heads" class="md-nav__link">
    <span class="md-ellipsis">
      15.3.8. Repeat K/V heads if N_KVHeads &lt; N_Heads
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1539-do-transposes" class="md-nav__link">
    <span class="md-ellipsis">
      15.3.9. Do transposes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#15310-calculate-scores" class="md-nav__link">
    <span class="md-ellipsis">
      15.3.10. Calculate scores
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#15311-perform-masking-on-scores" class="md-nav__link">
    <span class="md-ellipsis">
      15.3.11. Perform masking on scores
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#15312-apply-softmax-over-scores" class="md-nav__link">
    <span class="md-ellipsis">
      15.3.12. Apply Softmax over scores
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#15313-multiply-values-tensor-and-scores-tensor" class="md-nav__link">
    <span class="md-ellipsis">
      15.3.13. Multiply values tensor and scores tensor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#15314-apply-attention-output-weights" class="md-nav__link">
    <span class="md-ellipsis">
      15.3.14. Apply attention output weights
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#154-adding-the-attention-module-output-to-current-tensor" class="md-nav__link">
    <span class="md-ellipsis">
      15.4. Adding the attention module output to current tensor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#155-performing-forward-pass-through-feed-forward-prenormalization-rmsnormforward" class="md-nav__link">
    <span class="md-ellipsis">
      15.5. Performing Forward Pass Through Feed-Forward Prenormalization - RMSNorm.Forward(...)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#156-performing-forward-pass-through-feed-forward-module-calling" class="md-nav__link">
    <span class="md-ellipsis">
      15.6. Performing Forward Pass Through Feed-Forward Module - Calling
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#157-performing-forward-pass-through-feed-forward-module-llamafeedforwardforward" class="md-nav__link">
    <span class="md-ellipsis">
      15.7. Performing Forward Pass Through Feed-Forward Module - LlamaFeedForward.Forward(...)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#158-adding-the-feed-forward-network-module-output-to-current-tensor" class="md-nav__link">
    <span class="md-ellipsis">
      15.8. Adding the feed-forward network module output to current tensor
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../16-MAKING-PREDICTION-WITH-LLAMA-MODEL-3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MAKING PREDICTION with LLAMA MODEL - 3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../17-UNICODE-UTF-8-EMOJIS/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    UNICODE, UTF-8 and EMOJIS
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../18-CONCLUSION/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CONCLUSION
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../19-REFERENCES/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    REFERENCES
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../20-DIAGRAMS/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DIAGRAMS
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../webrtc-nuts-and-bolts" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    WebRTC Nuts and Bolts
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://www.linkedin.com/in/alper-dalkiran/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contact
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="15-making-prediction-with-llama-model-2"><strong>15. MAKING PREDICTION with LLAMA MODEL - 2</strong><a class="headerlink" href="#15-making-prediction-with-llama-model-2" title="Permanent link">&para;</a></h1>
<p>In previous <a href="../14-MAKING-PREDICTION-WITH-LLAMA-MODEL-1/">14. MAKING PREDICTION with LLAMA MODEL - 1</a> chapter, this chapter, and following <a href="../16-MAKING-PREDICTION-WITH-LLAMA-MODEL-3/">16. MAKING PREDICTION with LLAMA MODEL - 3</a> chapter, we walk through the <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">LlamaTransformer.Forward(...)</a> method and its components.</p>
<p>In the previous chapter, we have initiated a for loop to iterate over 32 layers defined at <code>lt.Layers</code> and called <code>LlamaTransformerBlock.Forward(...)</code> method.</p>
<p>In this chapter, we will delve into details of the <code>LlamaTransformerBlock.Forward(...)</code> method.</p>
<h2 id="151-performing-forward-pass-through-attention-prenormalization-rmsnormforward"><strong>15.1. Performing Forward Pass Through Attention Prenormalization - RMSNorm.Forward(...)</strong><a class="headerlink" href="#151-performing-forward-pass-through-attention-prenormalization-rmsnormforward" title="Permanent link">&para;</a></h2>
<blockquote>
<p><strong>Recap:</strong><br>
The Llama models use <a href="https://paperswithcode.com/method/rmsnorm" target="_blank">Pre-RMSNorm (Root Mean Square Layer Normalization)</a>. Because of we perform Root Mean Square Layer Normalization before performing multiplication of current tensor with normalization weights tensor, we call this normalization stage as "pre-normalization".</p>
</blockquote>
<p><img alt="STAGE 6: Forward Pass Through Attention Pre-normalization Diagram" src="../images/DIAG01-STAGE06-forward-pass-through-attention-prenormalization.drawio.svg" />
<sup><em>Diagram: </em><em>Forward Pass Through Attention Pre-normalization</em><em>. For the complete diagram, <a href="../20-DIAGRAMS/#complete-model-diagram">click here</a>.</em></sup></p>
<p>In this chapter, we only cover how to call RMSNorm succinctly, to give more space for other details. The details of prenormalization and RMSNorm (Root Mean Square Layer Normalization) will be explained in the chapter <a href="../16-MAKING-PREDICTION-WITH-LLAMA-MODEL-3/">16.1. Performing Forward Pass Through Output Prenormalization - RMSNorm.Forward(...)</a>.</p>
<p>Now, we have the <code>x</code> tensor argument. In our case, at the first iteration, <code>x</code> is input tensor, at the other iterations, <code>x</code> is output of previous <code>LlamaTransformerBlock</code>. In our case, at the first iteration, the shape of this tensor is <code>{22, 4096}</code>. 22 stands for sequence length, 4096 stands for the embedding layer dimension. <code>normalizedX</code> which is the resulting tensor will have same shape as the input, <code>{22, 4096}</code>.</p>
<p><sup>from <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">src/model/llamatransformer.go</a></sup></p>
<div class="language-go highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">ltb</span><span class="w"> </span><span class="o">*</span><span class="nx">LlamaTransformerBlock</span><span class="p">)</span><span class="w"> </span><span class="nx">Forward</span><span class="p">(</span><span class="nx">infContext</span><span class="w"> </span><span class="o">*</span><span class="nx">InferenceContext</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">startPos</span><span class="w"> </span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="nx">freqsCis</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">mask</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="w">    </span><span class="kd">var</span><span class="w"> </span><span class="nx">maskSize</span><span class="w"> </span><span class="p">[]</span><span class="kt">int</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">mask</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="w">        </span><span class="nx">maskSize</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">mask</span><span class="p">.</span><span class="nx">Size</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;LlamaTransformerBlock.Forward started for x: shape(%v), startPos: %d, freqsCis: shape(%v), mask: shape(%v)&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">startPos</span><span class="p">,</span><span class="w"> </span><span class="nx">freqsCis</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">maskSize</span><span class="p">)</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Calling RMSNorm for tensor x shape(%v) and LlamaTransformerBlock.attn_norm weights shape(%v) -&gt; tensor normalizedX&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">ltb</span><span class="p">.</span><span class="nx">attn_norm</span><span class="p">.</span><span class="nx">weights</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-0-8"><a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="w">    </span><span class="nx">normalizedX</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">ltb</span><span class="p">.</span><span class="nx">attn_norm</span><span class="p">.</span><span class="nx">Forward</span><span class="p">(</span><span class="nx">infContext</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="p">)</span>
</span><span id="__span-0-9"><a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-0-10"><a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a><span class="p">}</span>
</span></code></pre></div>
<p>We can see output lines in the "debug.log" file if debugging is enabled, as follows:</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>Calling<span class="w"> </span>LlamaTransformerBlock.Forward<span class="w"> </span><span class="k">for</span><span class="w"> </span>layer:<span class="w"> </span><span class="m">1</span><span class="w"> </span>/<span class="w"> </span><span class="m">32</span>,<span class="w"> </span>startPos:<span class="w"> </span><span class="m">0</span><span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>currentTensor<span class="w"> </span>...
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>LlamaTransformerBlock.Forward<span class="w"> </span>started<span class="w"> </span><span class="k">for</span><span class="w"> </span>x:<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span>,<span class="w"> </span>startPos:<span class="w"> </span><span class="m">0</span>,<span class="w"> </span>freqsCis:<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">64</span><span class="o">])</span>,<span class="w"> </span>mask:<span class="w"> </span>shape<span class="o">([])</span><span class="w"> </span>...
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>Calling<span class="w"> </span>RMSNorm<span class="w"> </span><span class="k">for</span><span class="w"> </span>tensor<span class="w"> </span>x<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>and<span class="w"> </span>LlamaTransformerBlock.attn_norm<span class="w"> </span>weights<span class="w"> </span>shape<span class="o">([</span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>normalizedX<span class="w"> </span>...
</span></code></pre></div>
<h2 id="152-performing-forward-pass-through-attention-module-calling"><strong>15.2. Performing Forward Pass Through Attention Module - Calling</strong><a class="headerlink" href="#152-performing-forward-pass-through-attention-module-calling" title="Permanent link">&para;</a></h2>
<p><sup>from <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">src/model/llamatransformer.go</a></sup></p>
<div class="language-go highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">ltb</span><span class="w"> </span><span class="o">*</span><span class="nx">LlamaTransformerBlock</span><span class="p">)</span><span class="w"> </span><span class="nx">Forward</span><span class="p">(</span><span class="nx">infContext</span><span class="w"> </span><span class="o">*</span><span class="nx">InferenceContext</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">startPos</span><span class="w"> </span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="nx">freqsCis</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">mask</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-2-2"><a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-2-3"><a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Calling LlamaAttention.Forward for tensor normalizedX shape(%v) and startPos: %d, freqsCis: shape(%v), mask: shape(%v) -&gt; tensor h&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">normalizedX</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">startPos</span><span class="p">,</span><span class="w"> </span><span class="nx">freqsCis</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">maskSize</span><span class="p">)</span>
</span><span id="__span-2-4"><a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a><span class="w">    </span><span class="nx">h</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">ltb</span><span class="p">.</span><span class="nx">attention</span><span class="p">.</span><span class="nx">Forward</span><span class="p">(</span><span class="nx">infContext</span><span class="p">,</span><span class="w"> </span><span class="nx">normalizedX</span><span class="p">,</span><span class="w"> </span><span class="nx">startPos</span><span class="p">,</span><span class="w"> </span><span class="nx">freqsCis</span><span class="p">,</span><span class="w"> </span><span class="nx">mask</span><span class="p">)</span>
</span><span id="__span-2-5"><a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-2-6"><a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a><span class="p">}</span>
</span></code></pre></div>
<p>We can see output lines in the "debug.log" file if debugging is enabled, as follows:</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>Calling<span class="w"> </span>LlamaAttention.Forward<span class="w"> </span><span class="k">for</span><span class="w"> </span>tensor<span class="w"> </span>normalizedX<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>and<span class="w"> </span>startPos:<span class="w"> </span><span class="m">0</span>,<span class="w"> </span>freqsCis:<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">64</span><span class="o">])</span>,<span class="w"> </span>mask:<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">22</span><span class="o">])</span><span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>h<span class="w"> </span>...
</span></code></pre></div>
<h2 id="153-performing-forward-pass-through-attention-module-llamaattentionforward"><strong>15.3. Performing Forward Pass Through Attention Module - LlamaAttention.Forward(...)</strong><a class="headerlink" href="#153-performing-forward-pass-through-attention-module-llamaattentionforward" title="Permanent link">&para;</a></h2>
<blockquote>
<p><strong>Recap:</strong><br>
The most important part of the transformer models that provide accurate outputs is <a href="https://arxiv.org/abs/1706.03762" target="_blank">the attention mechanism</a>. Each "block" of Llama consists of a self-attention and a feed-forward neural network parts. The details will be explained further, but also we call these "block"s as "layer"s.</p>
</blockquote>
<p>The attention mechanism is one of the important inventions that made language models more improved. Llama models have implemented "multi-head attention", so in our model case (Llama 3.1 8B-Instruct) we have 32 attention heads with some other supportive components. In the following steps, we will walk through details of an "attention module".</p>
<blockquote>
<p><strong>Important note:</strong><br>
In our case model Llama 3.1 8B-Instruct has 32 layers of transformer blocks and each block contains an attention module containing 32 attention heads.  Both numbers are 32, but they specify numbers of different concepts, so pay more attention to avoid any confusion.</p>
</blockquote>
<p><img alt="STAGE 7: Forward Pass Through Attention Module Diagram" src="../images/DIAG01-STAGE07-forward-pass-through-attention-module.drawio.svg" />
<sup><em>Diagram: </em><em>Forward Pass Through Attention Module</em><em>. For the complete diagram, <a href="../20-DIAGRAMS/#complete-model-diagram">click here</a>.</em></sup></p>
<h3 id="1531-recall-structure-of-llamaattention"><strong>15.3.1. Recall: Structure of LlamaAttention</strong><a class="headerlink" href="#1531-recall-structure-of-llamaattention" title="Permanent link">&para;</a></h3>
<p>The <code>LlamaAttention</code> object consists of:</p>
<ul>
<li><code>attn_wq</code>: Attention query weights tensor with shape of <code>{N_Heads * HeadDim, Dim} = {32 * 128, 4096} = {4096, 4096}</code>,</li>
<li><code>attn_wk</code>: Attention key weights tensor with shape of <code>{N_KVHeads * HeadDim, Dim} = {8 * 128, 4096} = {1024, 4096}</code>,</li>
<li><code>attn_wv</code>: Attention value weights tensor with shape of <code>{N_KVHeads * HeadDim, Dim} = {8 * 128, 4096} = {1024, 4096}</code>,</li>
<li><code>attn_wo</code>: Attention output weights tensor with shape of <code>{N_Heads * HeadDim, Dim} = {32 * 128, 4096} = {4096, 4096}</code>.</li>
</ul>
<p><strong><u>Type definition:</u></strong></p>
<p><sup>from <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">src/model/llamatransformer.go</a></sup></p>
<div class="language-go highlight"><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="kd">type</span><span class="w"> </span><span class="nx">LlamaAttention</span><span class="w"> </span><span class="kd">struct</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-4-2"><a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a><span class="w">    </span><span class="nx">LayerIndex</span><span class="w"> </span><span class="kt">int</span>
</span><span id="__span-4-3"><a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a>
</span><span id="__span-4-4"><a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a><span class="w">    </span><span class="nx">N_Heads</span><span class="w">   </span><span class="kt">int</span>
</span><span id="__span-4-5"><a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a><span class="w">    </span><span class="nx">N_KVHeads</span><span class="w"> </span><span class="kt">int</span>
</span><span id="__span-4-6"><a id="__codelineno-4-6" name="__codelineno-4-6" href="#__codelineno-4-6"></a><span class="w">    </span><span class="nx">N_Rep</span><span class="w">     </span><span class="kt">int</span>
</span><span id="__span-4-7"><a id="__codelineno-4-7" name="__codelineno-4-7" href="#__codelineno-4-7"></a><span class="w">    </span><span class="nx">HeadDim</span><span class="w">   </span><span class="kt">int</span>
</span><span id="__span-4-8"><a id="__codelineno-4-8" name="__codelineno-4-8" href="#__codelineno-4-8"></a>
</span><span id="__span-4-9"><a id="__codelineno-4-9" name="__codelineno-4-9" href="#__codelineno-4-9"></a><span class="w">    </span><span class="nx">attn_wq</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="w"> </span><span class="c1">// Original: &quot;layers.0.attention.wq.weight&quot;  |  ggml: &quot;blk.0.attn_q.weight&quot; | [out_features, in_features] -&gt; shape: [4096 4096] -&gt; [N_Heads * HeadDim, Dim]</span>
</span><span id="__span-4-10"><a id="__codelineno-4-10" name="__codelineno-4-10" href="#__codelineno-4-10"></a><span class="w">    </span><span class="nx">attn_wk</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="w"> </span><span class="c1">// Original: &quot;layers.0.attention.wk.weight&quot;  |  ggml: &quot;blk.0.attn_k.weight&quot; | [out_features, in_features] -&gt; shape: [1024 4096] -&gt; [N_KVHeads * HeadDim, Dim]</span>
</span><span id="__span-4-11"><a id="__codelineno-4-11" name="__codelineno-4-11" href="#__codelineno-4-11"></a><span class="w">    </span><span class="nx">attn_wv</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="w"> </span><span class="c1">// Original: &quot;layers.0.attention.wv.weight&quot;  |  ggml: &quot;blk.0.attn_v.weight&quot; | [out_features, in_features] -&gt; shape: [1024 4096] -&gt; [N_KVHeads * HeadDim, Dim]</span>
</span><span id="__span-4-12"><a id="__codelineno-4-12" name="__codelineno-4-12" href="#__codelineno-4-12"></a><span class="w">    </span><span class="nx">attn_wo</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="w"> </span><span class="c1">// Original: &quot;layers.0.attention.wo.weight&quot;  |  ggml: &quot;blk.0.attn_output.weight&quot; | [out_features, in_features] -&gt; shape: [4096 4096] -&gt; [N_Heads * HeadDim, Dim]</span>
</span><span id="__span-4-13"><a id="__codelineno-4-13" name="__codelineno-4-13" href="#__codelineno-4-13"></a><span class="p">}</span>
</span></code></pre></div>
<h3 id="1532-structure-to-run-multiple-linear-transformations"><strong>15.3.2. Structure to run multiple linear transformations</strong><a class="headerlink" href="#1532-structure-to-run-multiple-linear-transformations" title="Permanent link">&para;</a></h3>
<p>Now, we have the <code>x</code> tensor argument. In our case, at the first iteration, <code>x</code> is <u>the normalized form of</u> input tensor, at the other iterations, <code>x</code> is <u>the normalized form of</u> output of previous <code>LlamaTransformerBlock</code>. In our case, at the first iteration, the shape of this tensor is <code>{22, 4096}</code>. 22 stands for sequence length, 4096 stands for the embedding layer dimension.</p>
<p>In our attention module, we have <code>LlamaAttention.attn_wq</code>, <code>LlamaAttention.attn_wk</code>, and <code>LlamaAttention.attn_wv</code>. They are weight tensors of current (one of 32 layers) layer, which stand for "attention query weights", "attention key weights", and "attention value weights" respectively.</p>
<p>We need to perform a linear transformation with our <code>x</code> tensor with each of these three weight tensors independently, then take the results into <code>xq</code>, <code>xk</code>, and <code>xv</code> tensors respectively. These operations can be done independently, so we can run them parallelly. In this step, we provide a structure to call them parallelly as follows.</p>
<p>The concepts that is used here were described in the chapter <a href="../13-GENERATING-NEXT-TOKENS/">13.1. Preliminary Concepts</a>. For now, know that, the <a href="https://pkg.go.dev/context" target="_blank">context</a> and <a href="https://www.geeksforgeeks.org/using-waitgroup-in-golang/" target="_blank">WaitGroup</a> are used to manage parallel operations. We call the <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/ml/operations_impl.go" target="_blank">ml.LinearTransformation</a> as <a href="https://gobyexample.com/goroutines" target="_blank">goroutines</a>.</p>
<p>Then, these 3 goroutines are performed and finished, we take the results <code>xq</code>, <code>xk</code>, and <code>xv</code> tensors from the <code>parallelResults</code> map.</p>
<p><sup>from <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">src/model/llamatransformer.go</a></sup></p>
<div class="language-go highlight"><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">lat</span><span class="w"> </span><span class="o">*</span><span class="nx">LlamaAttention</span><span class="p">)</span><span class="w"> </span><span class="nx">Forward</span><span class="p">(</span><span class="nx">infContext</span><span class="w"> </span><span class="o">*</span><span class="nx">InferenceContext</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">startPos</span><span class="w"> </span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="nx">freqsCis</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">mask</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-5-2"><a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a><span class="w">    </span><span class="nx">sequenceLength</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">x</span><span class="p">.</span><span class="nx">Size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="__span-5-3"><a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a>
</span><span id="__span-5-4"><a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a><span class="w">    </span><span class="nx">ctx</span><span class="p">,</span><span class="w"> </span><span class="nx">cancel</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">context</span><span class="p">.</span><span class="nx">WithCancelCause</span><span class="p">(</span><span class="nx">context</span><span class="p">.</span><span class="nx">Background</span><span class="p">())</span>
</span><span id="__span-5-5"><a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a><span class="w">    </span><span class="kd">var</span><span class="w"> </span><span class="nx">wg</span><span class="w"> </span><span class="nx">sync</span><span class="p">.</span><span class="nx">WaitGroup</span>
</span><span id="__span-5-6"><a id="__codelineno-5-6" name="__codelineno-5-6" href="#__codelineno-5-6"></a><span class="w">    </span><span class="kd">var</span><span class="w"> </span><span class="nx">mu</span><span class="w"> </span><span class="nx">sync</span><span class="p">.</span><span class="nx">Mutex</span>
</span><span id="__span-5-7"><a id="__codelineno-5-7" name="__codelineno-5-7" href="#__codelineno-5-7"></a><span class="w">    </span><span class="nx">parallelResults</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">map</span><span class="p">[</span><span class="kt">string</span><span class="p">]</span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">)</span>
</span><span id="__span-5-8"><a id="__codelineno-5-8" name="__codelineno-5-8" href="#__codelineno-5-8"></a>
</span><span id="__span-5-9"><a id="__codelineno-5-9" name="__codelineno-5-9" href="#__codelineno-5-9"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;[Scheduling goroutine] ml.LinearTransformation for x shape(%v) and LlamaAttention.attn_wq weights shape(%v) -&gt; tensor xq&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">lat</span><span class="p">.</span><span class="nx">attn_wq</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-5-10"><a id="__codelineno-5-10" name="__codelineno-5-10" href="#__codelineno-5-10"></a><span class="w">    </span><span class="nx">wg</span><span class="p">.</span><span class="nx">Add</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-5-11"><a id="__codelineno-5-11" name="__codelineno-5-11" href="#__codelineno-5-11"></a><span class="w">    </span><span class="k">go</span><span class="w"> </span><span class="kd">func</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-5-12"><a id="__codelineno-5-12" name="__codelineno-5-12" href="#__codelineno-5-12"></a><span class="w">        </span><span class="k">defer</span><span class="w"> </span><span class="nx">wg</span><span class="p">.</span><span class="nx">Done</span><span class="p">()</span>
</span><span id="__span-5-13"><a id="__codelineno-5-13" name="__codelineno-5-13" href="#__codelineno-5-13"></a><span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nx">ctx</span><span class="p">.</span><span class="nx">Err</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-5-14"><a id="__codelineno-5-14" name="__codelineno-5-14" href="#__codelineno-5-14"></a><span class="w">            </span><span class="k">return</span>
</span><span id="__span-5-15"><a id="__codelineno-5-15" name="__codelineno-5-15" href="#__codelineno-5-15"></a><span class="w">        </span><span class="p">}</span>
</span><span id="__span-5-16"><a id="__codelineno-5-16" name="__codelineno-5-16" href="#__codelineno-5-16"></a><span class="w">        </span><span class="o">...</span>
</span><span id="__span-5-17"><a id="__codelineno-5-17" name="__codelineno-5-17" href="#__codelineno-5-17"></a><span class="w">    </span><span class="p">}()</span>
</span><span id="__span-5-18"><a id="__codelineno-5-18" name="__codelineno-5-18" href="#__codelineno-5-18"></a>
</span><span id="__span-5-19"><a id="__codelineno-5-19" name="__codelineno-5-19" href="#__codelineno-5-19"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;[Scheduling goroutine] ml.LinearTransformation for x shape(%v) and LlamaAttention.attn_wk weights shape(%v) -&gt; tensor xk&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">lat</span><span class="p">.</span><span class="nx">attn_wk</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-5-20"><a id="__codelineno-5-20" name="__codelineno-5-20" href="#__codelineno-5-20"></a><span class="w">    </span><span class="nx">wg</span><span class="p">.</span><span class="nx">Add</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-5-21"><a id="__codelineno-5-21" name="__codelineno-5-21" href="#__codelineno-5-21"></a><span class="w">    </span><span class="k">go</span><span class="w"> </span><span class="kd">func</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-5-22"><a id="__codelineno-5-22" name="__codelineno-5-22" href="#__codelineno-5-22"></a><span class="w">        </span><span class="k">defer</span><span class="w"> </span><span class="nx">wg</span><span class="p">.</span><span class="nx">Done</span><span class="p">()</span>
</span><span id="__span-5-23"><a id="__codelineno-5-23" name="__codelineno-5-23" href="#__codelineno-5-23"></a><span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nx">ctx</span><span class="p">.</span><span class="nx">Err</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-5-24"><a id="__codelineno-5-24" name="__codelineno-5-24" href="#__codelineno-5-24"></a><span class="w">            </span><span class="k">return</span>
</span><span id="__span-5-25"><a id="__codelineno-5-25" name="__codelineno-5-25" href="#__codelineno-5-25"></a><span class="w">        </span><span class="p">}</span>
</span><span id="__span-5-26"><a id="__codelineno-5-26" name="__codelineno-5-26" href="#__codelineno-5-26"></a><span class="w">        </span><span class="o">...</span>
</span><span id="__span-5-27"><a id="__codelineno-5-27" name="__codelineno-5-27" href="#__codelineno-5-27"></a><span class="w">    </span><span class="p">}()</span>
</span><span id="__span-5-28"><a id="__codelineno-5-28" name="__codelineno-5-28" href="#__codelineno-5-28"></a>
</span><span id="__span-5-29"><a id="__codelineno-5-29" name="__codelineno-5-29" href="#__codelineno-5-29"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;[Scheduling goroutine] ml.LinearTransformation for x shape(%v) and LlamaAttention.attn_wv weights shape(%v) -&gt; tensor xv&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">lat</span><span class="p">.</span><span class="nx">attn_wv</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-5-30"><a id="__codelineno-5-30" name="__codelineno-5-30" href="#__codelineno-5-30"></a><span class="w">    </span><span class="nx">wg</span><span class="p">.</span><span class="nx">Add</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-5-31"><a id="__codelineno-5-31" name="__codelineno-5-31" href="#__codelineno-5-31"></a><span class="w">    </span><span class="k">go</span><span class="w"> </span><span class="kd">func</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-5-32"><a id="__codelineno-5-32" name="__codelineno-5-32" href="#__codelineno-5-32"></a><span class="w">        </span><span class="k">defer</span><span class="w"> </span><span class="nx">wg</span><span class="p">.</span><span class="nx">Done</span><span class="p">()</span>
</span><span id="__span-5-33"><a id="__codelineno-5-33" name="__codelineno-5-33" href="#__codelineno-5-33"></a><span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nx">ctx</span><span class="p">.</span><span class="nx">Err</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-5-34"><a id="__codelineno-5-34" name="__codelineno-5-34" href="#__codelineno-5-34"></a><span class="w">            </span><span class="k">return</span>
</span><span id="__span-5-35"><a id="__codelineno-5-35" name="__codelineno-5-35" href="#__codelineno-5-35"></a><span class="w">        </span><span class="p">}</span>
</span><span id="__span-5-36"><a id="__codelineno-5-36" name="__codelineno-5-36" href="#__codelineno-5-36"></a><span class="w">        </span><span class="o">...</span>
</span><span id="__span-5-37"><a id="__codelineno-5-37" name="__codelineno-5-37" href="#__codelineno-5-37"></a><span class="w">    </span><span class="p">}()</span>
</span><span id="__span-5-38"><a id="__codelineno-5-38" name="__codelineno-5-38" href="#__codelineno-5-38"></a>
</span><span id="__span-5-39"><a id="__codelineno-5-39" name="__codelineno-5-39" href="#__codelineno-5-39"></a><span class="w">    </span><span class="nx">runtime</span><span class="p">.</span><span class="nx">Gosched</span><span class="p">()</span>
</span><span id="__span-5-40"><a id="__codelineno-5-40" name="__codelineno-5-40" href="#__codelineno-5-40"></a>
</span><span id="__span-5-41"><a id="__codelineno-5-41" name="__codelineno-5-41" href="#__codelineno-5-41"></a><span class="w">    </span><span class="k">select</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-5-42"><a id="__codelineno-5-42" name="__codelineno-5-42" href="#__codelineno-5-42"></a><span class="w">    </span><span class="k">case</span><span class="w"> </span><span class="o">&lt;-</span><span class="nx">ctx</span><span class="p">.</span><span class="nx">Done</span><span class="p">():</span>
</span><span id="__span-5-43"><a id="__codelineno-5-43" name="__codelineno-5-43" href="#__codelineno-5-43"></a><span class="w">        </span><span class="c1">// Cancellation signal was received</span>
</span><span id="__span-5-44"><a id="__codelineno-5-44" name="__codelineno-5-44" href="#__codelineno-5-44"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">context</span><span class="p">.</span><span class="nx">Cause</span><span class="p">(</span><span class="nx">ctx</span><span class="p">)</span>
</span><span id="__span-5-45"><a id="__codelineno-5-45" name="__codelineno-5-45" href="#__codelineno-5-45"></a><span class="w">    </span><span class="k">case</span><span class="w"> </span><span class="o">&lt;-</span><span class="nx">common</span><span class="p">.</span><span class="nx">WaitGroupDone</span><span class="p">(</span><span class="o">&amp;</span><span class="nx">wg</span><span class="p">):</span>
</span><span id="__span-5-46"><a id="__codelineno-5-46" name="__codelineno-5-46" href="#__codelineno-5-46"></a><span class="w">        </span><span class="nx">runtime</span><span class="p">.</span><span class="nx">Gosched</span><span class="p">()</span>
</span><span id="__span-5-47"><a id="__codelineno-5-47" name="__codelineno-5-47" href="#__codelineno-5-47"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-5-48"><a id="__codelineno-5-48" name="__codelineno-5-48" href="#__codelineno-5-48"></a>
</span><span id="__span-5-49"><a id="__codelineno-5-49" name="__codelineno-5-49" href="#__codelineno-5-49"></a><span class="w">    </span><span class="nx">xq</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">parallelResults</span><span class="p">[</span><span class="s">&quot;xq&quot;</span><span class="p">]</span>
</span><span id="__span-5-50"><a id="__codelineno-5-50" name="__codelineno-5-50" href="#__codelineno-5-50"></a><span class="w">    </span><span class="nx">xk</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">parallelResults</span><span class="p">[</span><span class="s">&quot;xk&quot;</span><span class="p">]</span>
</span><span id="__span-5-51"><a id="__codelineno-5-51" name="__codelineno-5-51" href="#__codelineno-5-51"></a><span class="w">    </span><span class="nx">xv</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">parallelResults</span><span class="p">[</span><span class="s">&quot;xv&quot;</span><span class="p">]</span>
</span><span id="__span-5-52"><a id="__codelineno-5-52" name="__codelineno-5-52" href="#__codelineno-5-52"></a><span class="p">}</span>
</span></code></pre></div>
<h3 id="1533-calculating-xq-xk-and-xv"><strong>15.3.3. Calculating xq, xk, and xv</strong><a class="headerlink" href="#1533-calculating-xq-xk-and-xv" title="Permanent link">&para;</a></h3>
<p>We perform three <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/ml/operations_impl.go" target="_blank">ml.LinearTransformation</a> which set their results into the <code>parallelResults</code> map with mutex locks. The resulting <code>xq</code> tensor has shape of <code>{22, 4096}</code>, <code>xk</code> and <code>xv</code> tensors are with same shape of <code>{22, 1024}</code>.</p>
<p><img alt="STAGE 8: Forward Pass Through Attention Module - Calculating xq, xk, and xv Diagram" src="../images/DIAG01-STAGE08-attention-fwd-calculating-xq-xk-xv.drawio.svg" />
<sup><em>Diagram: </em><em>Forward Pass Through Attention Module - Calculating xq, xk, and xv</em><em>. For the complete diagram, <a href="../20-DIAGRAMS/#complete-model-diagram">click here</a>.</em></sup></p>
<p><sup>from <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">src/model/llamatransformer.go</a></sup></p>
<div class="language-go highlight"><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">lat</span><span class="w"> </span><span class="o">*</span><span class="nx">LlamaAttention</span><span class="p">)</span><span class="w"> </span><span class="nx">Forward</span><span class="p">(</span><span class="nx">infContext</span><span class="w"> </span><span class="o">*</span><span class="nx">InferenceContext</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">startPos</span><span class="w"> </span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="nx">freqsCis</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">mask</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-6-2"><a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a><span class="w">    </span><span class="k">go</span><span class="w"> </span><span class="kd">func</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-6-3"><a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a><span class="w">        </span><span class="o">...</span>
</span><span id="__span-6-4"><a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a><span class="w">        </span><span class="nx">xq</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">ml</span><span class="p">.</span><span class="nx">LinearTransformation</span><span class="p">(</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">lat</span><span class="p">.</span><span class="nx">attn_wq</span><span class="p">)</span>
</span><span id="__span-6-5"><a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a><span class="w">        </span><span class="o">...</span>
</span><span id="__span-6-6"><a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a><span class="w">        </span><span class="nx">mu</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span>
</span><span id="__span-6-7"><a id="__codelineno-6-7" name="__codelineno-6-7" href="#__codelineno-6-7"></a><span class="w">        </span><span class="nx">parallelResults</span><span class="p">[</span><span class="s">&quot;xq&quot;</span><span class="p">]</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">xq</span>
</span><span id="__span-6-8"><a id="__codelineno-6-8" name="__codelineno-6-8" href="#__codelineno-6-8"></a><span class="w">        </span><span class="nx">mu</span><span class="p">.</span><span class="nx">Unlock</span><span class="p">()</span>
</span><span id="__span-6-9"><a id="__codelineno-6-9" name="__codelineno-6-9" href="#__codelineno-6-9"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-6-10"><a id="__codelineno-6-10" name="__codelineno-6-10" href="#__codelineno-6-10"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-6-11"><a id="__codelineno-6-11" name="__codelineno-6-11" href="#__codelineno-6-11"></a><span class="w">     </span><span class="k">go</span><span class="w"> </span><span class="kd">func</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-6-12"><a id="__codelineno-6-12" name="__codelineno-6-12" href="#__codelineno-6-12"></a><span class="w">        </span><span class="o">...</span>
</span><span id="__span-6-13"><a id="__codelineno-6-13" name="__codelineno-6-13" href="#__codelineno-6-13"></a><span class="w">        </span><span class="nx">xk</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">ml</span><span class="p">.</span><span class="nx">LinearTransformation</span><span class="p">(</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">lat</span><span class="p">.</span><span class="nx">attn_wk</span><span class="p">)</span>
</span><span id="__span-6-14"><a id="__codelineno-6-14" name="__codelineno-6-14" href="#__codelineno-6-14"></a><span class="w">        </span><span class="o">...</span>
</span><span id="__span-6-15"><a id="__codelineno-6-15" name="__codelineno-6-15" href="#__codelineno-6-15"></a><span class="w">        </span><span class="nx">mu</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span>
</span><span id="__span-6-16"><a id="__codelineno-6-16" name="__codelineno-6-16" href="#__codelineno-6-16"></a><span class="w">        </span><span class="nx">parallelResults</span><span class="p">[</span><span class="s">&quot;xk&quot;</span><span class="p">]</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">xk</span>
</span><span id="__span-6-17"><a id="__codelineno-6-17" name="__codelineno-6-17" href="#__codelineno-6-17"></a><span class="w">        </span><span class="nx">mu</span><span class="p">.</span><span class="nx">Unlock</span><span class="p">()</span>
</span><span id="__span-6-18"><a id="__codelineno-6-18" name="__codelineno-6-18" href="#__codelineno-6-18"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-6-19"><a id="__codelineno-6-19" name="__codelineno-6-19" href="#__codelineno-6-19"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-6-20"><a id="__codelineno-6-20" name="__codelineno-6-20" href="#__codelineno-6-20"></a><span class="w">     </span><span class="k">go</span><span class="w"> </span><span class="kd">func</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-6-21"><a id="__codelineno-6-21" name="__codelineno-6-21" href="#__codelineno-6-21"></a><span class="w">        </span><span class="o">...</span>
</span><span id="__span-6-22"><a id="__codelineno-6-22" name="__codelineno-6-22" href="#__codelineno-6-22"></a><span class="w">        </span><span class="nx">xv</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">ml</span><span class="p">.</span><span class="nx">LinearTransformation</span><span class="p">(</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">lat</span><span class="p">.</span><span class="nx">attn_wv</span><span class="p">)</span>
</span><span id="__span-6-23"><a id="__codelineno-6-23" name="__codelineno-6-23" href="#__codelineno-6-23"></a><span class="w">        </span><span class="o">...</span>
</span><span id="__span-6-24"><a id="__codelineno-6-24" name="__codelineno-6-24" href="#__codelineno-6-24"></a><span class="w">        </span><span class="nx">mu</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span>
</span><span id="__span-6-25"><a id="__codelineno-6-25" name="__codelineno-6-25" href="#__codelineno-6-25"></a><span class="w">        </span><span class="nx">parallelResults</span><span class="p">[</span><span class="s">&quot;xv&quot;</span><span class="p">]</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">xv</span>
</span><span id="__span-6-26"><a id="__codelineno-6-26" name="__codelineno-6-26" href="#__codelineno-6-26"></a><span class="w">        </span><span class="nx">mu</span><span class="p">.</span><span class="nx">Unlock</span><span class="p">()</span>
</span><span id="__span-6-27"><a id="__codelineno-6-27" name="__codelineno-6-27" href="#__codelineno-6-27"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-6-28"><a id="__codelineno-6-28" name="__codelineno-6-28" href="#__codelineno-6-28"></a><span class="p">}</span>
</span></code></pre></div>
<p>We can see output lines in the "debug.log" file if debugging is enabled, as follows:</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-7-1"><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span><span class="o">[</span>Scheduling<span class="w"> </span>goroutine<span class="o">]</span><span class="w"> </span>ml.LinearTransformation<span class="w"> </span><span class="k">for</span><span class="w"> </span>x<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>and<span class="w"> </span>LlamaAttention.attn_wq<span class="w"> </span>weights<span class="w"> </span>shape<span class="o">([</span><span class="m">4096</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>xq<span class="w"> </span>...
</span><span id="__span-7-2"><a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span><span class="o">[</span>Scheduling<span class="w"> </span>goroutine<span class="o">]</span><span class="w"> </span>ml.LinearTransformation<span class="w"> </span><span class="k">for</span><span class="w"> </span>x<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>and<span class="w"> </span>LlamaAttention.attn_wk<span class="w"> </span>weights<span class="w"> </span>shape<span class="o">([</span><span class="m">1024</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>xk<span class="w"> </span>...
</span><span id="__span-7-3"><a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span><span class="o">[</span>Scheduling<span class="w"> </span>goroutine<span class="o">]</span><span class="w"> </span>ml.LinearTransformation<span class="w"> </span><span class="k">for</span><span class="w"> </span>x<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>and<span class="w"> </span>LlamaAttention.attn_wv<span class="w"> </span>weights<span class="w"> </span>shape<span class="o">([</span><span class="m">1024</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>xv<span class="w"> </span>...
</span><span id="__span-7-4"><a id="__codelineno-7-4" name="__codelineno-7-4" href="#__codelineno-7-4"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span><span class="o">[</span>Calling<span class="w"> </span><span class="k">in</span><span class="w"> </span>goroutine<span class="o">]</span><span class="w"> </span>ml.LinearTransformation<span class="w"> </span><span class="k">for</span><span class="w"> </span>x<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>and<span class="w"> </span>LlamaAttention.attn_wv<span class="w"> </span>weights<span class="w"> </span>shape<span class="o">([</span><span class="m">1024</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>xv<span class="w"> </span>...
</span><span id="__span-7-5"><a id="__codelineno-7-5" name="__codelineno-7-5" href="#__codelineno-7-5"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span><span class="o">[</span>Calling<span class="w"> </span><span class="k">in</span><span class="w"> </span>goroutine<span class="o">]</span><span class="w"> </span>ml.LinearTransformation<span class="w"> </span><span class="k">for</span><span class="w"> </span>x<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>and<span class="w"> </span>LlamaAttention.attn_wq<span class="w"> </span>weights<span class="w"> </span>shape<span class="o">([</span><span class="m">4096</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>xq<span class="w"> </span>...
</span><span id="__span-7-6"><a id="__codelineno-7-6" name="__codelineno-7-6" href="#__codelineno-7-6"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span><span class="o">[</span>Calling<span class="w"> </span><span class="k">in</span><span class="w"> </span>goroutine<span class="o">]</span><span class="w"> </span>ml.LinearTransformation<span class="w"> </span><span class="k">for</span><span class="w"> </span>x<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>and<span class="w"> </span>LlamaAttention.attn_wk<span class="w"> </span>weights<span class="w"> </span>shape<span class="o">([</span><span class="m">1024</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>xk<span class="w"> </span>...
</span><span id="__span-7-7"><a id="__codelineno-7-7" name="__codelineno-7-7" href="#__codelineno-7-7"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>Parallel<span class="w"> </span>results,<span class="w"> </span>xq:<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span>,<span class="w"> </span>xk:<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">1024</span><span class="o">])</span>,<span class="w"> </span>xv:<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">1024</span><span class="o">])</span><span class="w"> </span>...
</span></code></pre></div>
<blockquote>
<p>Note: As you can see the logs above, the order of <code>[Scheduling goroutine]</code> and <code>[Calling in goroutine]</code> lines are different, it shows they were executed parallelly.</p>
</blockquote>
<h3 id="1534-do-reshapings"><strong>15.3.4. Do reshapings</strong><a class="headerlink" href="#1534-do-reshapings" title="Permanent link">&para;</a></h3>
<p><img alt="STAGE 9: Forward Pass Through Attention Module - Do reshapings Diagram" src="../images/DIAG01-STAGE09-attention-do-reshapings.drawio.svg" />
<sup><em>Diagram: </em><em>Forward Pass Through Attention Module - Do reshapings</em><em>. For the complete diagram, <a href="../20-DIAGRAMS/#complete-model-diagram">click here</a>.</em></sup></p>
<p>The resulting <code>xq</code> tensor has shape of <code>{22, 4096}</code>, <code>xk</code> and <code>xv</code> tensors are with same shape of <code>{22, 1024}</code>. In our case, we implement "multi-head attention":
* On <code>xq</code> tensor with <code>32</code> attention heads (according to <code>modelArgs.N_Heads = 32</code>),
* On <code>xk</code> and <code>xv</code> tensors with <code>8</code> attention key/value heads (according to <code>modelArgs.N_KVHeads = 8</code>).</p>
<p>Our resulting tensors have the values of each attention head combined into the dimension with size of <code>4096</code>. Our <code>modelArgs.HeadDim = 128</code>, so the dimension of each attention head is 128.</p>
<p>Now, we need to reshape our tensors to differentiate each attention head.</p>
<ul>
<li>On <code>xq</code> tensor with shape of <code>{22, 4096}</code> will be in shape of <code>{22, 32, 128}</code>. The first <code>22</code> stands for sequence length, the second <code>32</code> stands for "attention head count" <code>modelArgs.N_Heads</code>, the <code>128</code> stands for "attention head dimension" <code>modelArgs.HeadDim</code>,</li>
<li>On <code>xk</code> and <code>xv</code> tensors with shape of <code>{22, 1024}</code> will be in shape of <code>{22, 8, 128}</code>. The first <code>22</code> stands for sequence length, the second <code>8</code> stands for "attention key/value head count" <code>modelArgs.N_KVHeads</code>, the <code>128</code> stands for "attention head dimension" <code>modelArgs.HeadDim</code>,</li>
</ul>
<p>But, wait! We have mentioned the "attention head count" with <code>modelArgs.N_Heads</code>, but in the code there are two concepts: <code>modelArgs.N_Heads</code> and <code>modelArgs.N_KVHeads</code>. The <code>modelArgs.N_Heads</code> is used for specifying the shape of query tensor <code>xq</code>. The <code>modelArgs.N_KVHeads</code> is used for specifying the shape of key <code>xk</code> and value <code>xv</code> tensors. At one of further steps, we will apply "Repeat K/V heads" operation to make our key <code>xk</code> and value <code>xv</code> tensor shapes equal to query tensor <code>xq</code>.</p>
<p><sup>from <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">src/model/llamatransformer.go</a></sup></p>
<div class="language-go highlight"><pre><span></span><code><span id="__span-8-1"><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">lat</span><span class="w"> </span><span class="o">*</span><span class="nx">LlamaAttention</span><span class="p">)</span><span class="w"> </span><span class="nx">Forward</span><span class="p">(</span><span class="nx">infContext</span><span class="w"> </span><span class="o">*</span><span class="nx">InferenceContext</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">startPos</span><span class="w"> </span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="nx">freqsCis</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">mask</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-8-2"><a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Parallel results, xq: shape(%v), xk: shape(%v), xv: shape(%v)&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">xq</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">xk</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">xv</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-8-3"><a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a>
</span><span id="__span-8-4"><a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a><span class="w">    </span><span class="cm">/*</span>
</span><span id="__span-8-5"><a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a><span class="cm">        Do reshapings</span>
</span><span id="__span-8-6"><a id="__codelineno-8-6" name="__codelineno-8-6" href="#__codelineno-8-6"></a><span class="cm">    */</span>
</span><span id="__span-8-7"><a id="__codelineno-8-7" name="__codelineno-8-7" href="#__codelineno-8-7"></a><span class="w">    </span><span class="kd">var</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="kt">error</span>
</span><span id="__span-8-8"><a id="__codelineno-8-8" name="__codelineno-8-8" href="#__codelineno-8-8"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">xq</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">xq</span><span class="p">.</span><span class="nx">Reshape</span><span class="p">([]</span><span class="kt">int</span><span class="p">{</span><span class="nx">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="nx">lat</span><span class="p">.</span><span class="nx">N_Heads</span><span class="p">,</span><span class="w"> </span><span class="nx">lat</span><span class="p">.</span><span class="nx">HeadDim</span><span class="p">});</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-8-9"><a id="__codelineno-8-9" name="__codelineno-8-9" href="#__codelineno-8-9"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-8-10"><a id="__codelineno-8-10" name="__codelineno-8-10" href="#__codelineno-8-10"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-8-11"><a id="__codelineno-8-11" name="__codelineno-8-11" href="#__codelineno-8-11"></a>
</span><span id="__span-8-12"><a id="__codelineno-8-12" name="__codelineno-8-12" href="#__codelineno-8-12"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">xk</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">xk</span><span class="p">.</span><span class="nx">Reshape</span><span class="p">([]</span><span class="kt">int</span><span class="p">{</span><span class="nx">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="nx">lat</span><span class="p">.</span><span class="nx">N_KVHeads</span><span class="p">,</span><span class="w"> </span><span class="nx">lat</span><span class="p">.</span><span class="nx">HeadDim</span><span class="p">});</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-8-13"><a id="__codelineno-8-13" name="__codelineno-8-13" href="#__codelineno-8-13"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-8-14"><a id="__codelineno-8-14" name="__codelineno-8-14" href="#__codelineno-8-14"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-8-15"><a id="__codelineno-8-15" name="__codelineno-8-15" href="#__codelineno-8-15"></a>
</span><span id="__span-8-16"><a id="__codelineno-8-16" name="__codelineno-8-16" href="#__codelineno-8-16"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">xv</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">xv</span><span class="p">.</span><span class="nx">Reshape</span><span class="p">([]</span><span class="kt">int</span><span class="p">{</span><span class="nx">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="nx">lat</span><span class="p">.</span><span class="nx">N_KVHeads</span><span class="p">,</span><span class="w"> </span><span class="nx">lat</span><span class="p">.</span><span class="nx">HeadDim</span><span class="p">});</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-8-17"><a id="__codelineno-8-17" name="__codelineno-8-17" href="#__codelineno-8-17"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-8-18"><a id="__codelineno-8-18" name="__codelineno-8-18" href="#__codelineno-8-18"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-8-19"><a id="__codelineno-8-19" name="__codelineno-8-19" href="#__codelineno-8-19"></a>
</span><span id="__span-8-20"><a id="__codelineno-8-20" name="__codelineno-8-20" href="#__codelineno-8-20"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Reshaping results, xq: shape(%v), xk: shape(%v), xv: shape(%v)&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">xq</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">xk</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">xv</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-8-21"><a id="__codelineno-8-21" name="__codelineno-8-21" href="#__codelineno-8-21"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-8-22"><a id="__codelineno-8-22" name="__codelineno-8-22" href="#__codelineno-8-22"></a><span class="p">}</span>
</span></code></pre></div>
<p>We can see output lines in the "debug.log" file if debugging is enabled, as follows:</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-9-1"><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>Parallel<span class="w"> </span>results,<span class="w"> </span>xq:<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span>,<span class="w"> </span>xk:<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">1024</span><span class="o">])</span>,<span class="w"> </span>xv:<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">1024</span><span class="o">])</span><span class="w"> </span>...
</span><span id="__span-9-2"><a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>Reshaping<span class="w"> </span>results,<span class="w"> </span>xq:<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">32</span><span class="w"> </span><span class="m">128</span><span class="o">])</span>,<span class="w"> </span>xk:<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">128</span><span class="o">])</span>,<span class="w"> </span>xv:<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">128</span><span class="o">])</span><span class="w"> </span>...
</span></code></pre></div>
<h3 id="1535-apply-rotary-embeddings"><strong>15.3.5. Apply Rotary Embeddings</strong><a class="headerlink" href="#1535-apply-rotary-embeddings" title="Permanent link">&para;</a></h3>
<p><img alt="STAGE 10: Forward Pass Through Attention Module - Apply Rotary Embeddings Diagram" src="../images/DIAG01-STAGE10-attention-apply-rotary-embeddings.drawio.svg" />
<sup><em>Diagram: </em><em>Forward Pass Through Attention Module - Apply Rotary Embeddings</em><em>. For the complete diagram, <a href="../20-DIAGRAMS/#complete-model-diagram">click here</a>.</em></sup></p>
<p><sup>from <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">src/model/llamatransformer.go</a></sup></p>
<div class="language-go highlight"><pre><span></span><code><span id="__span-10-1"><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">lat</span><span class="w"> </span><span class="o">*</span><span class="nx">LlamaAttention</span><span class="p">)</span><span class="w"> </span><span class="nx">Forward</span><span class="p">(</span><span class="nx">infContext</span><span class="w"> </span><span class="o">*</span><span class="nx">InferenceContext</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">startPos</span><span class="w"> </span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="nx">freqsCis</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">mask</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-10-2"><a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-10-3"><a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a><span class="w">    </span><span class="cm">/*</span>
</span><span id="__span-10-4"><a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a><span class="cm">        Apply rotary embeddings</span>
</span><span id="__span-10-5"><a id="__codelineno-10-5" name="__codelineno-10-5" href="#__codelineno-10-5"></a><span class="cm">    */</span>
</span><span id="__span-10-6"><a id="__codelineno-10-6" name="__codelineno-10-6" href="#__codelineno-10-6"></a>
</span><span id="__span-10-7"><a id="__codelineno-10-7" name="__codelineno-10-7" href="#__codelineno-10-7"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">xq</span><span class="p">,</span><span class="w"> </span><span class="nx">xk</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">applyRotaryEmbeddings</span><span class="p">(</span><span class="nx">xq</span><span class="p">,</span><span class="w"> </span><span class="nx">xk</span><span class="p">,</span><span class="w"> </span><span class="nx">freqsCis</span><span class="p">);</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-10-8"><a id="__codelineno-10-8" name="__codelineno-10-8" href="#__codelineno-10-8"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-10-9"><a id="__codelineno-10-9" name="__codelineno-10-9" href="#__codelineno-10-9"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-10-10"><a id="__codelineno-10-10" name="__codelineno-10-10" href="#__codelineno-10-10"></a>
</span><span id="__span-10-11"><a id="__codelineno-10-11" name="__codelineno-10-11" href="#__codelineno-10-11"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;applyRotaryEmbeddings results, xq: shape(%v), xk: shape(%v)&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">xq</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">xk</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-10-12"><a id="__codelineno-10-12" name="__codelineno-10-12" href="#__codelineno-10-12"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-10-13"><a id="__codelineno-10-13" name="__codelineno-10-13" href="#__codelineno-10-13"></a><span class="p">}</span>
</span></code></pre></div>
<blockquote>
<p>For more information about how the <code>freqsCis</code> tensor is initiated, refer to <a href="../10-ROPE-ROTARY-POSITIONAL-EMBEDDINGS/">10. ROPE ROTARY POSITIONAL EMBEDDINGS</a> and <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/docs/10.BONUS-PRECOMPUTING-FREQUENCY-TENSOR.ipynb" target="_blank">10.BONUS. PRECOMPUTING FREQUENCY TENSOR</a>.</p>
</blockquote>
<p>During this step, we apply the <em>RoPE (Rotary Positional Embeddings)</em> approach propoed by <a href="https://arxiv.org/abs/2104.09864v5" target="_blank">RoFormer</a> paper over our query <code>xq</code> and key <code>xk</code> tensors.</p>
<blockquote>
<p>Note: The shape information here is for the first iteration of our sample case. The shape samples written in code descriptions are for a different case. The first dimension of the shapes stands for sequence length which varies by the prompt tokens count.</p>
</blockquote>
<ul>
<li>Have the query tensor <code>xq</code> with shape of <code>{22, 32, 128}</code>. Convert the tensor's data type from <code>DT_BF16</code> to <code>DT_COMPLEX</code>, and change the shape to  <code>{22, 32, 64}</code> via <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/ml/tensor.go" target="_blank">Tensor.ViewAsComplex64WithReshape(...)</a> method, the result is assigned into <code>xq_</code> variable,<br>
  This method:<br></li>
<li>Converts the data type of the tensor to <code>DT_F32</code> (float32), the shape remains as  <code>{22, 32, 128}</code>,</li>
<li>Reshapes the tensor with shape of <code>{22, 32, 64, 2}</code>,</li>
<li>Converts each pair of float32 in the last dimension into a <code>complex64</code> data type via <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/ml/tensor.go" target="_blank">Tensor.ViewAsComplex64(...)</a>, the new shape is <code>{22, 32, 64}</code> with data type of <code>DT_COMPLEX</code>.</li>
</ul>
<p>See <a href="https://pytorch.org/docs/stable/generated/torch.view_as_complex.html" target="_blank">torch.view_as_complex</a> documentation for more information.</p>
<div class="language-text highlight"><pre><span></span><code>&gt; Comment from Pytorch&#39;s documentation (link above):&lt;br&gt;
&gt;Torch&#39;s view_as_complex() is only supported for tensors with torch.dtype torch.float64 and torch.float32.&lt;br&gt;
&gt;The input is expected to have the last dimension of size 2. In addition, the tensor must have a stride of 1 for its last dimension. The strides of all other dimensions must be even numbers.
</code></pre></div>
<ul>
<li>Have the key tensor <code>xk</code> with shape of <code>{22, 8, 128}</code>. Convert the tensor's data type from <code>DT_BF16</code> to <code>DT_COMPLEX</code>, and change the shape to  <code>{22, 8, 64}</code> via <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/ml/tensor.go" target="_blank">Tensor.ViewAsComplex64WithReshape(...)</a> method, the result is assigned into <code>xk_</code> variable,</li>
<li>Reshape the <code>freqs_cis</code> tensor with shape of <code>{22, 64}</code> to the shape <code>{22, 1, 64}</code>,</li>
<li>Process the <code>xqOut</code>:<ul>
<li>Perform an element-wise multiplication with <code>xq_</code> tensor with shape of <code>{22, 32, 64}</code> and <code>freqs_cis</code> tensor with shape of <code>{22, 1, 64}</code> via <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/ml/operations_impl.go" target="_blank">ml.MultiplyElementwise</a>. Output shape is <code>{22, 32, 64}</code>, assign the result into <code>xqOut</code> variable,</li>
<li>Convert the <code>xqOut</code> tensor's data type from <code>DT_COMPLEX</code> to <code>DT_F32</code> (float32), and change the shape to  <code>{22, 32, 128}</code> via <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/ml/tensor.go" target="_blank">Tensor.ViewAsComplex64WithReshape(...)</a> method (think as packing-unpacking the pairs in the last dimension),</li>
<li>Convert the <code>xqOut</code> tensor's data type from <code>DT_F32</code> (float32) to <code>DT_BF16</code> with same shape <code>{22, 32, 128}</code>,</li>
</ul>
</li>
<li>Process the <code>xkOut</code>:<ul>
<li>Perform an element-wise multiplication with <code>xk_</code> tensor with shape of <code>{22, 8, 64}</code> and <code>freqs_cis</code> tensor with shape of <code>{22, 1, 64}</code> via <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/ml/operations_impl.go" target="_blank">ml.MultiplyElementwise</a>. Output shape is <code>{22, 8, 64}</code>, assign the result into <code>xkOut</code> variable,</li>
<li>Convert the <code>xkOut</code> tensor's data type from <code>DT_COMPLEX</code> to <code>DT_F32</code> (float32), and change the shape to  <code>{22, 8, 128}</code> via <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/ml/tensor.go" target="_blank">Tensor.ViewAsComplex64WithReshape(...)</a> method (think as packing-unpacking the pairs in the last dimension),</li>
<li>Convert the <code>xkOut</code> tensor's data type from <code>DT_F32</code> (float32) to <code>DT_BF16</code> with same shape <code>{22, 8, 128}</code>,</li>
</ul>
</li>
<li>Return the tensors <code>xqOut</code> and <code>xkOut</code> tensors with shape <code>{22, 8, 128}</code> together as result.</li>
</ul>
<p><sup>from <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">src/model/llamatransformer.go</a></sup></p>
<div class="language-go highlight"><pre><span></span><code><span id="__span-11-1"><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a><span class="kd">func</span><span class="w"> </span><span class="nx">applyRotaryEmbeddings</span><span class="p">(</span><span class="nx">xq</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">xk</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">freqs_cis</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">xqOut</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">xkOut</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-11-2"><a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a><span class="w">    </span><span class="c1">// xq shape=[5,32,128] dtype=DT_BF16</span>
</span><span id="__span-11-3"><a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a><span class="w">    </span><span class="nx">xq_</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">xq</span><span class="p">.</span><span class="nx">ViewAsComplex64WithReshape</span><span class="p">()</span><span class="w"> </span><span class="c1">// shape=[5,32,64] dtype=DT_COMPLEX</span>
</span><span id="__span-11-4"><a id="__codelineno-11-4" name="__codelineno-11-4" href="#__codelineno-11-4"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-11-5"><a id="__codelineno-11-5" name="__codelineno-11-5" href="#__codelineno-11-5"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-11-6"><a id="__codelineno-11-6" name="__codelineno-11-6" href="#__codelineno-11-6"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-11-7"><a id="__codelineno-11-7" name="__codelineno-11-7" href="#__codelineno-11-7"></a><span class="w">    </span><span class="c1">// xk shape=[5,8,128] dtype=DT_BF16</span>
</span><span id="__span-11-8"><a id="__codelineno-11-8" name="__codelineno-11-8" href="#__codelineno-11-8"></a><span class="w">    </span><span class="nx">xk_</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">xk</span><span class="p">.</span><span class="nx">ViewAsComplex64WithReshape</span><span class="p">()</span><span class="w"> </span><span class="c1">// shape=[5,8,64] dtype=DT_COMPLEX</span>
</span><span id="__span-11-9"><a id="__codelineno-11-9" name="__codelineno-11-9" href="#__codelineno-11-9"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-11-10"><a id="__codelineno-11-10" name="__codelineno-11-10" href="#__codelineno-11-10"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-11-11"><a id="__codelineno-11-11" name="__codelineno-11-11" href="#__codelineno-11-11"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-11-12"><a id="__codelineno-11-12" name="__codelineno-11-12" href="#__codelineno-11-12"></a>
</span><span id="__span-11-13"><a id="__codelineno-11-13" name="__codelineno-11-13" href="#__codelineno-11-13"></a><span class="w">    </span><span class="c1">// freqs_cis shape=[5, 64] dtype=DT_COMPLEX</span>
</span><span id="__span-11-14"><a id="__codelineno-11-14" name="__codelineno-11-14" href="#__codelineno-11-14"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">freqs_cis</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">freqs_cis</span><span class="p">.</span><span class="nx">Reshape</span><span class="p">([]</span><span class="kt">int</span><span class="p">{</span><span class="nx">xq_</span><span class="p">.</span><span class="nx">Size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nx">xq_</span><span class="p">.</span><span class="nx">Size</span><span class="p">[</span><span class="mi">2</span><span class="p">]});</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="c1">// shape=[5,1,64] dtype=DT_COMPLEX</span>
</span><span id="__span-11-15"><a id="__codelineno-11-15" name="__codelineno-11-15" href="#__codelineno-11-15"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-11-16"><a id="__codelineno-11-16" name="__codelineno-11-16" href="#__codelineno-11-16"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-11-17"><a id="__codelineno-11-17" name="__codelineno-11-17" href="#__codelineno-11-17"></a>
</span><span id="__span-11-18"><a id="__codelineno-11-18" name="__codelineno-11-18" href="#__codelineno-11-18"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">xqOut</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">ml</span><span class="p">.</span><span class="nx">MultiplyElementwise</span><span class="p">(</span><span class="nx">xq_</span><span class="p">,</span><span class="w"> </span><span class="nx">freqs_cis</span><span class="p">);</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="c1">// shape=[5,32,64] dtype=DT_COMPLEX</span>
</span><span id="__span-11-19"><a id="__codelineno-11-19" name="__codelineno-11-19" href="#__codelineno-11-19"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-11-20"><a id="__codelineno-11-20" name="__codelineno-11-20" href="#__codelineno-11-20"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-11-21"><a id="__codelineno-11-21" name="__codelineno-11-21" href="#__codelineno-11-21"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">xqOut</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">xqOut</span><span class="p">.</span><span class="nx">ViewAsFloat32WithReshape</span><span class="p">();</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="c1">// shape=[5,32,128] dtype=DT_F32</span>
</span><span id="__span-11-22"><a id="__codelineno-11-22" name="__codelineno-11-22" href="#__codelineno-11-22"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-11-23"><a id="__codelineno-11-23" name="__codelineno-11-23" href="#__codelineno-11-23"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-11-24"><a id="__codelineno-11-24" name="__codelineno-11-24" href="#__codelineno-11-24"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">xqOut</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">xqOut</span><span class="p">.</span><span class="nx">ToBFloat16</span><span class="p">();</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="c1">// shape=[5,32,128] dtype=DT_BF16</span>
</span><span id="__span-11-25"><a id="__codelineno-11-25" name="__codelineno-11-25" href="#__codelineno-11-25"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-11-26"><a id="__codelineno-11-26" name="__codelineno-11-26" href="#__codelineno-11-26"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-11-27"><a id="__codelineno-11-27" name="__codelineno-11-27" href="#__codelineno-11-27"></a>
</span><span id="__span-11-28"><a id="__codelineno-11-28" name="__codelineno-11-28" href="#__codelineno-11-28"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">xkOut</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">ml</span><span class="p">.</span><span class="nx">MultiplyElementwise</span><span class="p">(</span><span class="nx">xk_</span><span class="p">,</span><span class="w"> </span><span class="nx">freqs_cis</span><span class="p">);</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="c1">// shape=[5,8,64] dtype=DT_COMPLEX</span>
</span><span id="__span-11-29"><a id="__codelineno-11-29" name="__codelineno-11-29" href="#__codelineno-11-29"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-11-30"><a id="__codelineno-11-30" name="__codelineno-11-30" href="#__codelineno-11-30"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-11-31"><a id="__codelineno-11-31" name="__codelineno-11-31" href="#__codelineno-11-31"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">xkOut</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">xkOut</span><span class="p">.</span><span class="nx">ViewAsFloat32WithReshape</span><span class="p">();</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="c1">// shape=[5,8,128] dtype=DT_F32</span>
</span><span id="__span-11-32"><a id="__codelineno-11-32" name="__codelineno-11-32" href="#__codelineno-11-32"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-11-33"><a id="__codelineno-11-33" name="__codelineno-11-33" href="#__codelineno-11-33"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-11-34"><a id="__codelineno-11-34" name="__codelineno-11-34" href="#__codelineno-11-34"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">xkOut</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">xkOut</span><span class="p">.</span><span class="nx">ToBFloat16</span><span class="p">();</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="c1">// shape=[5,8,128] dtype=DT_BF16</span>
</span><span id="__span-11-35"><a id="__codelineno-11-35" name="__codelineno-11-35" href="#__codelineno-11-35"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-11-36"><a id="__codelineno-11-36" name="__codelineno-11-36" href="#__codelineno-11-36"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-11-37"><a id="__codelineno-11-37" name="__codelineno-11-37" href="#__codelineno-11-37"></a><span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nx">xqOut</span><span class="p">,</span><span class="w"> </span><span class="nx">xkOut</span><span class="p">,</span><span class="w"> </span><span class="kc">nil</span>
</span><span id="__span-11-38"><a id="__codelineno-11-38" name="__codelineno-11-38" href="#__codelineno-11-38"></a><span class="p">}</span>
</span></code></pre></div>
<p>We can see output lines in the "debug.log" file if debugging is enabled, as follows:</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-12-1"><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>applyRotaryEmbeddings<span class="w"> </span>results,<span class="w"> </span>xq:<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">32</span><span class="w"> </span><span class="m">128</span><span class="o">])</span>,<span class="w"> </span>xk:<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">128</span><span class="o">])</span><span class="w"> </span>...
</span></code></pre></div>
<h3 id="1536-update-kv-cache"><strong>15.3.6. Update KV cache</strong><a class="headerlink" href="#1536-update-kv-cache" title="Permanent link">&para;</a></h3>
<p><img alt="STAGE 11: Forward Pass Through Attention Module - Update KV cache Diagram" src="../images/DIAG01-STAGE11-attention-update-kv-cache.drawio.svg" />
<sup><em>Diagram: </em><em>Forward Pass Through Attention Module - Update KV cache</em><em>. For the complete diagram, <a href="../20-DIAGRAMS/#complete-model-diagram">click here</a>.</em></sup></p>
<p>We have initiated an "inference context" with type of <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/inferencecontext.go" target="_blank">model.InferenceContext</a> which we keep the state of one inference process. In this context object, we have two cache arrays: <code>InferenceContext.CacheK</code> and <code>InferenceContext.CacheV</code> which stand for "cache of keys" and "cache of values" respectively. These arrays have 32 items correspond to 32 layers. Each of these items consists of tensors with shape of <code>{200, 8, 128}</code>. 200 stands for the maximum sequence length <code>inferenceArgs.SequenceLength</code>, 8 stands for <code>modelArgs.N_KVHeads</code>, 128 stands for <code>modelArgs.HeadDim</code>.</p>
<p>Here, in our case of the first iteration, we set the cache of the <code>0th</code> layer. We set the slices of the <code>CacheK</code> and <code>CacheV</code> with index range <code>0</code> (startPos) to <code>22</code> (startPos + sequenceLength) to <code>xk</code> and <code>xv</code> tensors respectively. The <code>sequenceLength</code> is the first dimension of the shape of <code>x</code> tensor argument.</p>
<p><sup>from <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">src/model/llamatransformer.go</a></sup></p>
<div class="language-go highlight"><pre><span></span><code><span id="__span-13-1"><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">lat</span><span class="w"> </span><span class="o">*</span><span class="nx">LlamaAttention</span><span class="p">)</span><span class="w"> </span><span class="nx">Forward</span><span class="p">(</span><span class="nx">infContext</span><span class="w"> </span><span class="o">*</span><span class="nx">InferenceContext</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">startPos</span><span class="w"> </span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="nx">freqsCis</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">mask</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-13-2"><a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-13-3"><a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a><span class="w">    </span><span class="cm">/*</span>
</span><span id="__span-13-4"><a id="__codelineno-13-4" name="__codelineno-13-4" href="#__codelineno-13-4"></a><span class="cm">        Update KV cache</span>
</span><span id="__span-13-5"><a id="__codelineno-13-5" name="__codelineno-13-5" href="#__codelineno-13-5"></a><span class="cm">    */</span>
</span><span id="__span-13-6"><a id="__codelineno-13-6" name="__codelineno-13-6" href="#__codelineno-13-6"></a>
</span><span id="__span-13-7"><a id="__codelineno-13-7" name="__codelineno-13-7" href="#__codelineno-13-7"></a><span class="w">    </span><span class="nx">infContext</span><span class="p">.</span><span class="nx">CacheK</span><span class="p">[</span><span class="nx">lat</span><span class="p">.</span><span class="nx">LayerIndex</span><span class="p">].</span><span class="nx">SetSlice</span><span class="p">([]</span><span class="kt">int</span><span class="p">{</span><span class="nx">startPos</span><span class="p">},</span><span class="w"> </span><span class="p">[]</span><span class="kt">int</span><span class="p">{</span><span class="nx">startPos</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">sequenceLength</span><span class="p">},</span><span class="w"> </span><span class="nx">xk</span><span class="p">)</span>
</span><span id="__span-13-8"><a id="__codelineno-13-8" name="__codelineno-13-8" href="#__codelineno-13-8"></a><span class="w">    </span><span class="nx">infContext</span><span class="p">.</span><span class="nx">CacheV</span><span class="p">[</span><span class="nx">lat</span><span class="p">.</span><span class="nx">LayerIndex</span><span class="p">].</span><span class="nx">SetSlice</span><span class="p">([]</span><span class="kt">int</span><span class="p">{</span><span class="nx">startPos</span><span class="p">},</span><span class="w"> </span><span class="p">[]</span><span class="kt">int</span><span class="p">{</span><span class="nx">startPos</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">sequenceLength</span><span class="p">},</span><span class="w"> </span><span class="nx">xv</span><span class="p">)</span>
</span><span id="__span-13-9"><a id="__codelineno-13-9" name="__codelineno-13-9" href="#__codelineno-13-9"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-13-10"><a id="__codelineno-13-10" name="__codelineno-13-10" href="#__codelineno-13-10"></a><span class="p">}</span>
</span></code></pre></div>
<h3 id="1537-retrieve-cached-kv-so-far"><strong>15.3.7. Retrieve cached KV so far</strong><a class="headerlink" href="#1537-retrieve-cached-kv-so-far" title="Permanent link">&para;</a></h3>
<p>To make easy to understand how the KV cache is updated, think of a sample:</p>
<ul>
<li>Prompt tokens count is 22,</li>
<li>While generation of the 1st token:<ul>
<li><code>startPos</code> is <code>0</code>,</li>
<li>The shape of <code>x</code> argument of <code>LlamaAttention.Forward(...)</code> is <code>{22, 4096}</code>,</li>
<li>The shapes of <code>xk</code> and <code>xv</code> are <code>{22, 8, 128}</code>,</li>
<li>We update the indices of each cache <code>0:22</code> with <code>xk</code> and <code>xv</code>.</li>
</ul>
</li>
<li>While generation of the 2nd token:<ul>
<li><code>startPos</code> is <code>22</code>,</li>
<li>The shape of <code>x</code> argument of <code>LlamaAttention.Forward(...)</code> is <code>{1, 4096}</code> (in the iterations except first, the tokens are processed one by one, because of this, the first dimension is 1),</li>
<li>The shapes of <code>xk</code> and <code>xv</code> are <code>{1, 8, 128}</code>,</li>
<li>We update the indices of each cache <code>22:23</code> with <code>xk</code> and <code>xv</code>.</li>
</ul>
</li>
<li>While generation of the 3rd token:<ul>
<li><code>startPos</code> is <code>23</code>,</li>
<li>The shape of <code>x</code> argument of <code>LlamaAttention.Forward(...)</code> is <code>{1, 4096}</code> (in the iterations except first, the tokens are processed one by one, because of this, the first dimension is 1),</li>
<li>The shapes of <code>xk</code> and <code>xv</code> are <code>{1, 8, 128}</code>,</li>
<li>We update the indices of each cache <code>23:24</code> with <code>xk</code> and <code>xv</code>.</li>
</ul>
</li>
<li>So on...</li>
</ul>
<p>Now, we take the cached keys and values for the all positions so far. To make easy to understand:</p>
<ul>
<li>Prompt tokens count is 22,</li>
<li>While generation of the 1st token:<ul>
<li><code>startPos</code> is <code>0</code>,</li>
<li>The shape of <code>x</code> argument of <code>LlamaAttention.Forward(...)</code> is <code>{22, 4096}</code>,</li>
<li>We take items at indices <code>0:22</code> of <code>CacheK</code> and <code>CacheV</code> into <code>keys</code> and <code>values</code> tensors respectively, because <code>startPos + sequenceLength = 22</code>. The <code>keys</code> and <code>values</code> are with the shape of <code>{22, 8, 128}</code>.</li>
</ul>
</li>
<li>While generation of the 2nd token:<ul>
<li><code>startPos</code> is <code>22</code>,</li>
<li>The shape of <code>x</code> argument of <code>LlamaAttention.Forward(...)</code> is <code>{1, 4096}</code> (in the iterations except first, the tokens are processed one by one, because of this, the first dimension is 1),</li>
<li>We take items at indices <code>0:23</code> of <code>CacheK</code> and <code>CacheV</code> into <code>keys</code> and <code>values</code> tensors respectively, because <code>startPos + sequenceLength = 23</code>. The <code>keys</code> and <code>values</code> are with the shape of <code>{23, 8, 128}</code>.</li>
</ul>
</li>
<li>While generation of the 3rd token:<ul>
<li><code>startPos</code> is <code>23</code>,</li>
<li>The shape of <code>x</code> argument of <code>LlamaAttention.Forward(...)</code> is <code>{1, 4096}</code> (in the iterations except first, the tokens are processed one by one, because of this, the first dimension is 1),</li>
<li>We take items at indices <code>0:24</code> of <code>CacheK</code> and <code>CacheV</code> into <code>keys</code> and <code>values</code> tensors respectively, because <code>startPos + sequenceLength = 24</code>. The <code>keys</code> and <code>values</code> are with the shape of <code>{24, 8, 128}</code>.</li>
</ul>
</li>
</ul>
<p>In this documentation, we cover only the first iteration of generating the first token. So, in our case, we retrieve items at indices <code>0:22</code>, the shapes of our <code>keys</code> and <code>values</code> are <code>{22, 8, 128}</code>.</p>
<p><sup>from <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">src/model/llamatransformer.go</a></sup></p>
<div class="language-go highlight"><pre><span></span><code><span id="__span-14-1"><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">lat</span><span class="w"> </span><span class="o">*</span><span class="nx">LlamaAttention</span><span class="p">)</span><span class="w"> </span><span class="nx">Forward</span><span class="p">(</span><span class="nx">infContext</span><span class="w"> </span><span class="o">*</span><span class="nx">InferenceContext</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">startPos</span><span class="w"> </span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="nx">freqsCis</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">mask</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-14-2"><a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-14-3"><a id="__codelineno-14-3" name="__codelineno-14-3" href="#__codelineno-14-3"></a><span class="w">    </span><span class="cm">/*</span>
</span><span id="__span-14-4"><a id="__codelineno-14-4" name="__codelineno-14-4" href="#__codelineno-14-4"></a><span class="cm">        Retrieve cached KV so far</span>
</span><span id="__span-14-5"><a id="__codelineno-14-5" name="__codelineno-14-5" href="#__codelineno-14-5"></a><span class="cm">    */</span>
</span><span id="__span-14-6"><a id="__codelineno-14-6" name="__codelineno-14-6" href="#__codelineno-14-6"></a>
</span><span id="__span-14-7"><a id="__codelineno-14-7" name="__codelineno-14-7" href="#__codelineno-14-7"></a><span class="w">    </span><span class="nx">keys</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">infContext</span><span class="p">.</span><span class="nx">CacheK</span><span class="p">[</span><span class="nx">lat</span><span class="p">.</span><span class="nx">LayerIndex</span><span class="p">].</span><span class="nx">Slice</span><span class="p">([]</span><span class="kt">int</span><span class="p">{</span><span class="mi">0</span><span class="p">},</span><span class="w"> </span><span class="p">[]</span><span class="kt">int</span><span class="p">{</span><span class="nx">startPos</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">sequenceLength</span><span class="p">})</span>
</span><span id="__span-14-8"><a id="__codelineno-14-8" name="__codelineno-14-8" href="#__codelineno-14-8"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-14-9"><a id="__codelineno-14-9" name="__codelineno-14-9" href="#__codelineno-14-9"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-14-10"><a id="__codelineno-14-10" name="__codelineno-14-10" href="#__codelineno-14-10"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-14-11"><a id="__codelineno-14-11" name="__codelineno-14-11" href="#__codelineno-14-11"></a><span class="w">    </span><span class="nx">values</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">infContext</span><span class="p">.</span><span class="nx">CacheV</span><span class="p">[</span><span class="nx">lat</span><span class="p">.</span><span class="nx">LayerIndex</span><span class="p">].</span><span class="nx">Slice</span><span class="p">([]</span><span class="kt">int</span><span class="p">{</span><span class="mi">0</span><span class="p">},</span><span class="w"> </span><span class="p">[]</span><span class="kt">int</span><span class="p">{</span><span class="nx">startPos</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">sequenceLength</span><span class="p">})</span>
</span><span id="__span-14-12"><a id="__codelineno-14-12" name="__codelineno-14-12" href="#__codelineno-14-12"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-14-13"><a id="__codelineno-14-13" name="__codelineno-14-13" href="#__codelineno-14-13"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-14-14"><a id="__codelineno-14-14" name="__codelineno-14-14" href="#__codelineno-14-14"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-14-15"><a id="__codelineno-14-15" name="__codelineno-14-15" href="#__codelineno-14-15"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-14-16"><a id="__codelineno-14-16" name="__codelineno-14-16" href="#__codelineno-14-16"></a><span class="p">}</span>
</span></code></pre></div>
<h3 id="1538-repeat-kv-heads-if-n_kvheads-n_heads"><strong>15.3.8. Repeat K/V heads if N_KVHeads &lt; N_Heads</strong><a class="headerlink" href="#1538-repeat-kv-heads-if-n_kvheads-n_heads" title="Permanent link">&para;</a></h3>
<p>Repeating K/V heads step is only required if <code>N_KVHeads &lt; N_Heads</code>, in our case (Llama 3.1 8B-Instruct model) <code>N_Rep = N_Heads / N_KVHeads = 4</code>, so we need to apply this step. </p>
<p>This operation is applied to <code>keys</code> and <code>values</code> tensors. In our case, our <code>keys</code> and <code>value</code> tensors are in shape of <code>{22, 8, 128}</code>. The first <code>22</code> stands for sequence length, the second <code>8</code> stands for "attention key/value head count" <code>modelArgs.N_KVHeads</code>, the <code>128</code> stands for "attention head dimension" <code>modelArgs.HeadDim</code>.</p>
<p>Then, we need to equalize the <code>attention head</code> counts of <code>keys</code> and <code>values</code> to <code>query</code> tensors.</p>
<p>We've defined function to achieve this: <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">attentionRepeatKV</a>. This function:</p>
<ul>
<li>Creates a new tensor (named <code>expanded</code>) with shape of <code>{sequenceLength, modelArgs.N_KVHeads, N_Rep, modelArgs.HeadDim} = {22, 8, 4, 128}</code>,</li>
<li>Reshapes input tensor from shape of <code>{sequenceLength, modelArgs.N_KVHeads, modelArgs.HeadDim} = {22, 8, 128}</code> to <code>{sequenceLength, modelArgs.N_KVHeads, 1, modelArgs.HeadDim} = {22, 8, 1, 128}</code>,</li>
<li>By looping over <code>sequenceLength</code>, <code>modelArgs.N_KVHeads</code>, and <code>N_Rep</code>, copies the last dimension (<code>modelArgs.HeadDim</code>) parts in count of <code>N_Rep</code> into the new <code>expanded</code> tensor,</li>
<li>Reshapes the <code>expanded</code> tensor from shape of <code>{sequenceLength, modelArgs.N_KVHeads, N_Rep, modelArgs.HeadDim} = {22, 8, 4, 128}</code> to <code>{sequenceLength, modelArgs.N_KVHeads * N_Rep, modelArgs.HeadDim} = {22, 32, 128}</code>,</li>
<li>Now, we have a repeated (key or value) tensor with same shape of <code>query</code> tensor.</li>
</ul>
<p><sup>from <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">src/model/llamatransformer.go</a></sup></p>
<div class="language-go highlight"><pre><span></span><code><span id="__span-15-1"><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">lat</span><span class="w"> </span><span class="o">*</span><span class="nx">LlamaAttention</span><span class="p">)</span><span class="w"> </span><span class="nx">Forward</span><span class="p">(</span><span class="nx">infContext</span><span class="w"> </span><span class="o">*</span><span class="nx">InferenceContext</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">startPos</span><span class="w"> </span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="nx">freqsCis</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">mask</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-15-2"><a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-15-3"><a id="__codelineno-15-3" name="__codelineno-15-3" href="#__codelineno-15-3"></a><span class="w">    </span><span class="cm">/*</span>
</span><span id="__span-15-4"><a id="__codelineno-15-4" name="__codelineno-15-4" href="#__codelineno-15-4"></a><span class="cm">        Repeat k/v heads if N_KVHeads &lt; N_Heads</span>
</span><span id="__span-15-5"><a id="__codelineno-15-5" name="__codelineno-15-5" href="#__codelineno-15-5"></a><span class="cm">    */</span>
</span><span id="__span-15-6"><a id="__codelineno-15-6" name="__codelineno-15-6" href="#__codelineno-15-6"></a>
</span><span id="__span-15-7"><a id="__codelineno-15-7" name="__codelineno-15-7" href="#__codelineno-15-7"></a><span class="w">    </span><span class="c1">// example shape=[5, 8, 128] (cacheLen + sequenceLength, N_KVHeads, HeadDim)</span>
</span><span id="__span-15-8"><a id="__codelineno-15-8" name="__codelineno-15-8" href="#__codelineno-15-8"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">keys</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">attentionRepeatKV</span><span class="p">(</span><span class="nx">keys</span><span class="p">,</span><span class="w"> </span><span class="nx">lat</span><span class="p">.</span><span class="nx">N_Rep</span><span class="p">);</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="c1">// example shape=[5, 32, 128] (cacheLen + sequenceLength, N_Heads, HeadDim)</span>
</span><span id="__span-15-9"><a id="__codelineno-15-9" name="__codelineno-15-9" href="#__codelineno-15-9"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-15-10"><a id="__codelineno-15-10" name="__codelineno-15-10" href="#__codelineno-15-10"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-15-11"><a id="__codelineno-15-11" name="__codelineno-15-11" href="#__codelineno-15-11"></a><span class="w">    </span><span class="c1">// example shape=[5, 8, 128] (cacheLen + sequenceLength, N_KVHeads, HeadDim)</span>
</span><span id="__span-15-12"><a id="__codelineno-15-12" name="__codelineno-15-12" href="#__codelineno-15-12"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">values</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">attentionRepeatKV</span><span class="p">(</span><span class="nx">values</span><span class="p">,</span><span class="w"> </span><span class="nx">lat</span><span class="p">.</span><span class="nx">N_Rep</span><span class="p">);</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="c1">// example shape=[5, 32, 128] (cacheLen + sequenceLength, N_Heads, HeadDim)</span>
</span><span id="__span-15-13"><a id="__codelineno-15-13" name="__codelineno-15-13" href="#__codelineno-15-13"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-15-14"><a id="__codelineno-15-14" name="__codelineno-15-14" href="#__codelineno-15-14"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-15-15"><a id="__codelineno-15-15" name="__codelineno-15-15" href="#__codelineno-15-15"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-15-16"><a id="__codelineno-15-16" name="__codelineno-15-16" href="#__codelineno-15-16"></a><span class="p">}</span>
</span></code></pre></div>
<p><sup>from <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">src/model/llamatransformer.go</a></sup></p>
<div class="language-go highlight"><pre><span></span><code><span id="__span-16-1"><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a><span class="kd">func</span><span class="w"> </span><span class="nx">attentionRepeatKV</span><span class="p">(</span><span class="nx">x</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">N_Rep</span><span class="w"> </span><span class="kt">int</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-16-2"><a id="__codelineno-16-2" name="__codelineno-16-2" href="#__codelineno-16-2"></a><span class="w">    </span><span class="c1">// See: https://github.com/meta-llama/llama-models/blob/f45cdfd624b98b6655540f7101d8d9cb432e631c/models/llama3_1/reference_impl/model.py#L103</span>
</span><span id="__span-16-3"><a id="__codelineno-16-3" name="__codelineno-16-3" href="#__codelineno-16-3"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">N_Rep</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-16-4"><a id="__codelineno-16-4" name="__codelineno-16-4" href="#__codelineno-16-4"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="kc">nil</span>
</span><span id="__span-16-5"><a id="__codelineno-16-5" name="__codelineno-16-5" href="#__codelineno-16-5"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-16-6"><a id="__codelineno-16-6" name="__codelineno-16-6" href="#__codelineno-16-6"></a><span class="w">    </span><span class="nx">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="nx">n_KVHeads</span><span class="p">,</span><span class="w"> </span><span class="nx">headDim</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">x</span><span class="p">.</span><span class="nx">Size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="nx">x</span><span class="p">.</span><span class="nx">Size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="w"> </span><span class="nx">x</span><span class="p">.</span><span class="nx">Size</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
</span><span id="__span-16-7"><a id="__codelineno-16-7" name="__codelineno-16-7" href="#__codelineno-16-7"></a>
</span><span id="__span-16-8"><a id="__codelineno-16-8" name="__codelineno-16-8" href="#__codelineno-16-8"></a><span class="w">    </span><span class="nx">expanded</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">ml</span><span class="p">.</span><span class="nx">NewEmptyTensor</span><span class="p">([]</span><span class="kt">int</span><span class="p">{</span><span class="nx">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="nx">n_KVHeads</span><span class="p">,</span><span class="w"> </span><span class="nx">N_Rep</span><span class="p">,</span><span class="w"> </span><span class="nx">headDim</span><span class="p">},</span><span class="w"> </span><span class="nx">x</span><span class="p">.</span><span class="nx">DataType</span><span class="p">)</span>
</span><span id="__span-16-9"><a id="__codelineno-16-9" name="__codelineno-16-9" href="#__codelineno-16-9"></a><span class="w">    </span><span class="kd">var</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="kt">error</span>
</span><span id="__span-16-10"><a id="__codelineno-16-10" name="__codelineno-16-10" href="#__codelineno-16-10"></a><span class="w">    </span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">x</span><span class="p">.</span><span class="nx">Reshape</span><span class="p">([]</span><span class="kt">int</span><span class="p">{</span><span class="nx">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="nx">n_KVHeads</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nx">headDim</span><span class="p">})</span>
</span><span id="__span-16-11"><a id="__codelineno-16-11" name="__codelineno-16-11" href="#__codelineno-16-11"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-16-12"><a id="__codelineno-16-12" name="__codelineno-16-12" href="#__codelineno-16-12"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-16-13"><a id="__codelineno-16-13" name="__codelineno-16-13" href="#__codelineno-16-13"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-16-14"><a id="__codelineno-16-14" name="__codelineno-16-14" href="#__codelineno-16-14"></a><span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="nx">sequenceLength</span><span class="p">;</span><span class="w"> </span><span class="nx">i</span><span class="o">++</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-16-15"><a id="__codelineno-16-15" name="__codelineno-16-15" href="#__codelineno-16-15"></a><span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="nx">j</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="nx">j</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="nx">n_KVHeads</span><span class="p">;</span><span class="w"> </span><span class="nx">j</span><span class="o">++</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-16-16"><a id="__codelineno-16-16" name="__codelineno-16-16" href="#__codelineno-16-16"></a><span class="w">            </span><span class="nx">slice</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">x</span><span class="p">.</span><span class="nx">Slice</span><span class="p">([]</span><span class="kt">int</span><span class="p">{</span><span class="nx">i</span><span class="p">,</span><span class="w"> </span><span class="nx">j</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">},</span><span class="w"> </span><span class="p">[]</span><span class="kt">int</span><span class="p">{</span><span class="nx">i</span><span class="p">,</span><span class="w"> </span><span class="nx">j</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">})</span>
</span><span id="__span-16-17"><a id="__codelineno-16-17" name="__codelineno-16-17" href="#__codelineno-16-17"></a><span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-16-18"><a id="__codelineno-16-18" name="__codelineno-16-18" href="#__codelineno-16-18"></a><span class="w">                </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-16-19"><a id="__codelineno-16-19" name="__codelineno-16-19" href="#__codelineno-16-19"></a><span class="w">            </span><span class="p">}</span>
</span><span id="__span-16-20"><a id="__codelineno-16-20" name="__codelineno-16-20" href="#__codelineno-16-20"></a><span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="nx">rep</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="nx">rep</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="nx">N_Rep</span><span class="p">;</span><span class="w"> </span><span class="nx">rep</span><span class="o">++</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-16-21"><a id="__codelineno-16-21" name="__codelineno-16-21" href="#__codelineno-16-21"></a><span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">expanded</span><span class="p">.</span><span class="nx">SetSlice</span><span class="p">([]</span><span class="kt">int</span><span class="p">{</span><span class="nx">i</span><span class="p">,</span><span class="w"> </span><span class="nx">j</span><span class="p">,</span><span class="w"> </span><span class="nx">rep</span><span class="p">},</span><span class="w"> </span><span class="p">[]</span><span class="kt">int</span><span class="p">{</span><span class="nx">i</span><span class="p">,</span><span class="w"> </span><span class="nx">j</span><span class="p">,</span><span class="w"> </span><span class="nx">rep</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="nx">slice</span><span class="p">);</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-16-22"><a id="__codelineno-16-22" name="__codelineno-16-22" href="#__codelineno-16-22"></a><span class="w">                    </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-16-23"><a id="__codelineno-16-23" name="__codelineno-16-23" href="#__codelineno-16-23"></a><span class="w">                </span><span class="p">}</span>
</span><span id="__span-16-24"><a id="__codelineno-16-24" name="__codelineno-16-24" href="#__codelineno-16-24"></a><span class="w">            </span><span class="p">}</span>
</span><span id="__span-16-25"><a id="__codelineno-16-25" name="__codelineno-16-25" href="#__codelineno-16-25"></a><span class="w">        </span><span class="p">}</span>
</span><span id="__span-16-26"><a id="__codelineno-16-26" name="__codelineno-16-26" href="#__codelineno-16-26"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-16-27"><a id="__codelineno-16-27" name="__codelineno-16-27" href="#__codelineno-16-27"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">expanded</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">expanded</span><span class="p">.</span><span class="nx">Reshape</span><span class="p">([]</span><span class="kt">int</span><span class="p">{</span><span class="nx">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="nx">n_KVHeads</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nx">N_Rep</span><span class="p">,</span><span class="w"> </span><span class="nx">headDim</span><span class="p">});</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-16-28"><a id="__codelineno-16-28" name="__codelineno-16-28" href="#__codelineno-16-28"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-16-29"><a id="__codelineno-16-29" name="__codelineno-16-29" href="#__codelineno-16-29"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-16-30"><a id="__codelineno-16-30" name="__codelineno-16-30" href="#__codelineno-16-30"></a><span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nx">expanded</span><span class="p">,</span><span class="w"> </span><span class="kc">nil</span>
</span><span id="__span-16-31"><a id="__codelineno-16-31" name="__codelineno-16-31" href="#__codelineno-16-31"></a><span class="p">}</span>
</span></code></pre></div>
<h3 id="1539-do-transposes"><strong>15.3.9. Do transposes</strong><a class="headerlink" href="#1539-do-transposes" title="Permanent link">&para;</a></h3>
<p><img alt="STAGE 12: Forward Pass Through Attention Module - Do transposes Diagram" src="../images/DIAG01-STAGE12-attention-do-transposes.drawio.svg" />
<sup><em>Diagram: </em><em>Forward Pass Through Attention Module - Do transposes</em><em>. For the complete diagram, <a href="../20-DIAGRAMS/#complete-model-diagram">click here</a>.</em></sup></p>
<blockquote>
<p>Note: After applying previous "Repeat K/V heads" operation, all of our attention counts will be equal to <code>modelArgs.N_Heads = 32</code>, no more shapes with <code>modelArgs.N_KVHeads = 8</code>.</p>
</blockquote>
<p>In this step, we need to perform some transpose operations:</p>
<ul>
<li>Transpose <code>xq</code>'s <code>0th</code> and <code>1st</code> dimensions: from <code>{sequenceLength, N_Heads, HeadDim} = {22, 32, 128}</code> to <code>{N_Heads, sequenceLength, HeadDim} = {32, 22, 128}</code>,<br>
    &gt;In the sample at the code comments, sequenceLength is 5, and the operation is from <code>{5, 32, 128}</code> to <code>{32, 5, 128}</code></li>
<li>Transpose <code>keys</code>'s <code>0th</code> and <code>1st</code> dimensions: from <code>{sequenceLength, N_Heads, HeadDim} = {22, 32, 128}</code> to <code>{N_Heads, sequenceLength, HeadDim} = {32, 22, 128}</code>,<br>
    &gt;In the sample at the code comments, sequenceLength is 5, and the operation is from <code>{5, 32, 128}</code> to <code>{32, 5, 128}</code></li>
<li>Transpose <code>values</code>'s <code>0th</code> and <code>1st</code> dimensions: from <code>{sequenceLength, N_Heads, HeadDim} = {22, 32, 128}</code> to <code>{N_Heads, sequenceLength, HeadDim} = {32, 22, 128}</code>,<br>
    &gt;In the sample at the code comments, sequenceLength is 5, and the operation is from <code>{5, 32, 128}</code> to <code>{32, 5, 128}</code></li>
<li>Transpose <code>keys</code>'s <code>1st</code> and <code>2nd</code> dimensions: from <code>{N_Heads, sequenceLength, HeadDim} = {32, 22, 128}</code> to <code>{N_Heads, HeadDim, sequenceLength} = {32, 128, 22}</code>.<br>
    &gt;In the sample at the code comments, sequenceLength is 5, and the operation is from <code>{32, 5, 128}</code> to <code>{32, 128, 5}</code></li>
</ul>
<p><sup>from <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">src/model/llamatransformer.go</a></sup></p>
<div class="language-go highlight"><pre><span></span><code><span id="__span-17-1"><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">lat</span><span class="w"> </span><span class="o">*</span><span class="nx">LlamaAttention</span><span class="p">)</span><span class="w"> </span><span class="nx">Forward</span><span class="p">(</span><span class="nx">infContext</span><span class="w"> </span><span class="o">*</span><span class="nx">InferenceContext</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">startPos</span><span class="w"> </span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="nx">freqsCis</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">mask</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-17-2"><a id="__codelineno-17-2" name="__codelineno-17-2" href="#__codelineno-17-2"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-17-3"><a id="__codelineno-17-3" name="__codelineno-17-3" href="#__codelineno-17-3"></a><span class="w">    </span><span class="cm">/*</span>
</span><span id="__span-17-4"><a id="__codelineno-17-4" name="__codelineno-17-4" href="#__codelineno-17-4"></a><span class="cm">        Do transposes</span>
</span><span id="__span-17-5"><a id="__codelineno-17-5" name="__codelineno-17-5" href="#__codelineno-17-5"></a><span class="cm">    */</span>
</span><span id="__span-17-6"><a id="__codelineno-17-6" name="__codelineno-17-6" href="#__codelineno-17-6"></a>
</span><span id="__span-17-7"><a id="__codelineno-17-7" name="__codelineno-17-7" href="#__codelineno-17-7"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">xq</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">xq</span><span class="p">.</span><span class="nx">Transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="c1">// from [5, 32, 128] -&gt; example shape=[32, 5, 128] (N_Heads, sequenceLength, HeadDim)</span>
</span><span id="__span-17-8"><a id="__codelineno-17-8" name="__codelineno-17-8" href="#__codelineno-17-8"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-17-9"><a id="__codelineno-17-9" name="__codelineno-17-9" href="#__codelineno-17-9"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-17-10"><a id="__codelineno-17-10" name="__codelineno-17-10" href="#__codelineno-17-10"></a>
</span><span id="__span-17-11"><a id="__codelineno-17-11" name="__codelineno-17-11" href="#__codelineno-17-11"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">keys</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">keys</span><span class="p">.</span><span class="nx">Transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="c1">// from [5, 32, 128] -&gt; example shape=[32, 5, 128] (N_Heads, sequenceLength, HeadDim)</span>
</span><span id="__span-17-12"><a id="__codelineno-17-12" name="__codelineno-17-12" href="#__codelineno-17-12"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-17-13"><a id="__codelineno-17-13" name="__codelineno-17-13" href="#__codelineno-17-13"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-17-14"><a id="__codelineno-17-14" name="__codelineno-17-14" href="#__codelineno-17-14"></a>
</span><span id="__span-17-15"><a id="__codelineno-17-15" name="__codelineno-17-15" href="#__codelineno-17-15"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">values</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">values</span><span class="p">.</span><span class="nx">Transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="c1">// from [5, 32, 128] -&gt; example shape=[32, 5, 128] (N_Heads, sequenceLength, HeadDim)</span>
</span><span id="__span-17-16"><a id="__codelineno-17-16" name="__codelineno-17-16" href="#__codelineno-17-16"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-17-17"><a id="__codelineno-17-17" name="__codelineno-17-17" href="#__codelineno-17-17"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-17-18"><a id="__codelineno-17-18" name="__codelineno-17-18" href="#__codelineno-17-18"></a>
</span><span id="__span-17-19"><a id="__codelineno-17-19" name="__codelineno-17-19" href="#__codelineno-17-19"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">keys</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">keys</span><span class="p">.</span><span class="nx">Transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="c1">// from [32, 5, 128] -&gt; example shape=[32, 128, 5] (N_Heads, HeadDim, sequenceLength)</span>
</span><span id="__span-17-20"><a id="__codelineno-17-20" name="__codelineno-17-20" href="#__codelineno-17-20"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-17-21"><a id="__codelineno-17-21" name="__codelineno-17-21" href="#__codelineno-17-21"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-17-22"><a id="__codelineno-17-22" name="__codelineno-17-22" href="#__codelineno-17-22"></a>
</span><span id="__span-17-23"><a id="__codelineno-17-23" name="__codelineno-17-23" href="#__codelineno-17-23"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Multiple transposing results, xq: shape(%v), keys: shape(%v), values: shape(%v)&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">xq</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">keys</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">values</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-17-24"><a id="__codelineno-17-24" name="__codelineno-17-24" href="#__codelineno-17-24"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-17-25"><a id="__codelineno-17-25" name="__codelineno-17-25" href="#__codelineno-17-25"></a><span class="p">}</span>
</span></code></pre></div>
<p>We can see output lines in the "debug.log" file if debugging is enabled, as follows:</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-18-1"><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>Multiple<span class="w"> </span>transposing<span class="w"> </span>results,<span class="w"> </span>xq:<span class="w"> </span>shape<span class="o">([</span><span class="m">32</span><span class="w"> </span><span class="m">22</span><span class="w"> </span><span class="m">128</span><span class="o">])</span>,<span class="w"> </span>keys:<span class="w"> </span>shape<span class="o">([</span><span class="m">32</span><span class="w"> </span><span class="m">128</span><span class="w"> </span><span class="m">22</span><span class="o">])</span>,<span class="w"> </span>values:<span class="w"> </span>shape<span class="o">([</span><span class="m">32</span><span class="w"> </span><span class="m">22</span><span class="w"> </span><span class="m">128</span><span class="o">])</span><span class="w"> </span>...
</span></code></pre></div>
<h3 id="15310-calculate-scores"><strong>15.3.10. Calculate scores</strong><a class="headerlink" href="#15310-calculate-scores" title="Permanent link">&para;</a></h3>
<p><img alt="STAGE 13: Forward Pass Through Attention Module - Calculate scores Diagram" src="../images/DIAG01-STAGE13-attention-calculate-scores.drawio.svg" />
<sup><em>Diagram: </em><em>Forward Pass Through Attention Module - Calculate scores</em><em>. For the complete diagram, <a href="../20-DIAGRAMS/#complete-model-diagram">click here</a>.</em></sup></p>
<div class="language-py highlight"><pre><span></span><code><span id="__span-19-1"><a id="__codelineno-19-1" name="__codelineno-19-1" href="#__codelineno-19-1"></a><span class="c1">#Goal in Python manner:</span>
</span><span id="__span-19-2"><a id="__codelineno-19-2" name="__codelineno-19-2" href="#__codelineno-19-2"></a><span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">xq</span><span class="p">,</span> <span class="n">keys</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
</span></code></pre></div>
<p>We calculate the scores which we will perform <a href="https://en.wikipedia.org/wiki/Softmax_function" target="_blank">Softmax</a> operation over.</p>
<ul>
<li>Perform a matrix multiplication over <code>xq</code> with shape of <code>{32, 22, 128}</code> and <code>keys</code> with shape of <code>{32, 128, 22}</code> (transpose operation has been performed in previous step already),</li>
<li>Take square root of <code>lat.HeadDim = 128</code> is <code>11.3125</code> in BFloat16 form,</li>
<li>Divide all items of the result of matrix multiplication <code>xqMatMulKeys</code> to <code>11.3125</code>, assign the result into <code>scores</code>.</li>
</ul>
<p><sup>from <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">src/model/llamatransformer.go</a></sup></p>
<div class="language-go highlight"><pre><span></span><code><span id="__span-20-1"><a id="__codelineno-20-1" name="__codelineno-20-1" href="#__codelineno-20-1"></a><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">lat</span><span class="w"> </span><span class="o">*</span><span class="nx">LlamaAttention</span><span class="p">)</span><span class="w"> </span><span class="nx">Forward</span><span class="p">(</span><span class="nx">infContext</span><span class="w"> </span><span class="o">*</span><span class="nx">InferenceContext</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">startPos</span><span class="w"> </span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="nx">freqsCis</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">mask</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-20-2"><a id="__codelineno-20-2" name="__codelineno-20-2" href="#__codelineno-20-2"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-20-3"><a id="__codelineno-20-3" name="__codelineno-20-3" href="#__codelineno-20-3"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Calling ml.MatMul for xq shape(%v) and keys shape(%v) -&gt; tensor xqMatMulKeys&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">xq</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">keys</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-20-4"><a id="__codelineno-20-4" name="__codelineno-20-4" href="#__codelineno-20-4"></a><span class="w">    </span><span class="nx">xqMatMulKeys</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">ml</span><span class="p">.</span><span class="nx">MatMul</span><span class="p">(</span><span class="nx">xq</span><span class="p">,</span><span class="w"> </span><span class="nx">keys</span><span class="p">)</span><span class="w"> </span><span class="c1">// matmul([32,5,128], [32,128,5]) -&gt; example shape=[32,5,5] (N_Heads, sequenceLength, sequenceLength)</span>
</span><span id="__span-20-5"><a id="__codelineno-20-5" name="__codelineno-20-5" href="#__codelineno-20-5"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-20-6"><a id="__codelineno-20-6" name="__codelineno-20-6" href="#__codelineno-20-6"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-20-7"><a id="__codelineno-20-7" name="__codelineno-20-7" href="#__codelineno-20-7"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-20-8"><a id="__codelineno-20-8" name="__codelineno-20-8" href="#__codelineno-20-8"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Calling ml.DivToScalar for xqMatMulKeys shape(%v) and scalar -&gt; tensor scores&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">xqMatMulKeys</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-20-9"><a id="__codelineno-20-9" name="__codelineno-20-9" href="#__codelineno-20-9"></a><span class="w">    </span><span class="nx">scores</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">ml</span><span class="p">.</span><span class="nx">DivToScalar</span><span class="p">(</span><span class="nx">xqMatMulKeys</span><span class="p">,</span><span class="w"> </span><span class="nx">dtype</span><span class="p">.</span><span class="nx">BFloat16fromFloat32</span><span class="p">(</span><span class="nb">float32</span><span class="p">(</span><span class="nx">math</span><span class="p">.</span><span class="nx">Sqrt</span><span class="p">(</span><span class="nb">float64</span><span class="p">(</span><span class="nx">lat</span><span class="p">.</span><span class="nx">HeadDim</span><span class="p">)))))</span><span class="w"> </span><span class="c1">// example shape=[32,5,5]</span>
</span><span id="__span-20-10"><a id="__codelineno-20-10" name="__codelineno-20-10" href="#__codelineno-20-10"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-20-11"><a id="__codelineno-20-11" name="__codelineno-20-11" href="#__codelineno-20-11"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-20-12"><a id="__codelineno-20-12" name="__codelineno-20-12" href="#__codelineno-20-12"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-20-13"><a id="__codelineno-20-13" name="__codelineno-20-13" href="#__codelineno-20-13"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-20-14"><a id="__codelineno-20-14" name="__codelineno-20-14" href="#__codelineno-20-14"></a><span class="p">}</span>
</span></code></pre></div>
<p>We can see output lines in the "debug.log" file if debugging is enabled, as follows:</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-21-1"><a id="__codelineno-21-1" name="__codelineno-21-1" href="#__codelineno-21-1"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>Calling<span class="w"> </span>ml.MatMul<span class="w"> </span><span class="k">for</span><span class="w"> </span>xq<span class="w"> </span>shape<span class="o">([</span><span class="m">32</span><span class="w"> </span><span class="m">22</span><span class="w"> </span><span class="m">128</span><span class="o">])</span><span class="w"> </span>and<span class="w"> </span>keys<span class="w"> </span>shape<span class="o">([</span><span class="m">32</span><span class="w"> </span><span class="m">128</span><span class="w"> </span><span class="m">22</span><span class="o">])</span><span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>xqMatMulKeys<span class="w"> </span>...
</span><span id="__span-21-2"><a id="__codelineno-21-2" name="__codelineno-21-2" href="#__codelineno-21-2"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>Calling<span class="w"> </span>ml.DivToScalar<span class="w"> </span><span class="k">for</span><span class="w"> </span>xqMatMulKeys<span class="w"> </span>shape<span class="o">([</span><span class="m">32</span><span class="w"> </span><span class="m">22</span><span class="w"> </span><span class="m">22</span><span class="o">])</span><span class="w"> </span>and<span class="w"> </span>scalar<span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>scores<span class="w"> </span>...
</span></code></pre></div>
<h3 id="15311-perform-masking-on-scores"><strong>15.3.11. Perform masking on scores</strong><a class="headerlink" href="#15311-perform-masking-on-scores" title="Permanent link">&para;</a></h3>
<p>If there is a given <code>mask</code> argument, perform masking operation. This is because Llama is an auto-regressive model and our mask contains triangular matrix consisting of <code>0</code>s and <code>-Inf (negative infinity)</code>s. For more information, refer to <a href="../14-MAKING-PREDICTION-WITH-LLAMA-MODEL-1/">14.2.3. Creating the mask tensor</a>.</p>
<p>By performing <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/ml/operations_impl.go" target="_blank">ml.Add(...)</a> operation over <code>scores</code> with shape of <code>{32, 22, 22}</code> and <code>mask</code> tensor with shape of <code>{22, 22}</code>, we take the items corresponding on <code>0</code> mask values and ignore the items corresponding on <code>-Inf</code> mask values (adding <code>-Inf</code> to a number makes the number <code>-Inf</code>).</p>
<p><sup>from <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">src/model/llamatransformer.go</a></sup></p>
<div class="language-go highlight"><pre><span></span><code><span id="__span-22-1"><a id="__codelineno-22-1" name="__codelineno-22-1" href="#__codelineno-22-1"></a><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">lat</span><span class="w"> </span><span class="o">*</span><span class="nx">LlamaAttention</span><span class="p">)</span><span class="w"> </span><span class="nx">Forward</span><span class="p">(</span><span class="nx">infContext</span><span class="w"> </span><span class="o">*</span><span class="nx">InferenceContext</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">startPos</span><span class="w"> </span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="nx">freqsCis</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">mask</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-22-2"><a id="__codelineno-22-2" name="__codelineno-22-2" href="#__codelineno-22-2"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-22-3"><a id="__codelineno-22-3" name="__codelineno-22-3" href="#__codelineno-22-3"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">mask</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-22-4"><a id="__codelineno-22-4" name="__codelineno-22-4" href="#__codelineno-22-4"></a><span class="w">        </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Calling ml.Add to calculate scores shape(%v) + mask shape(%v) -&gt; tensor scores&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">scores</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">mask</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-22-5"><a id="__codelineno-22-5" name="__codelineno-22-5" href="#__codelineno-22-5"></a><span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nx">scores</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">ml</span><span class="p">.</span><span class="nx">Add</span><span class="p">(</span><span class="nx">scores</span><span class="p">,</span><span class="w"> </span><span class="nx">mask</span><span class="p">);</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="c1">// example shape=[32,5,5]</span>
</span><span id="__span-22-6"><a id="__codelineno-22-6" name="__codelineno-22-6" href="#__codelineno-22-6"></a><span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-22-7"><a id="__codelineno-22-7" name="__codelineno-22-7" href="#__codelineno-22-7"></a><span class="w">        </span><span class="p">}</span>
</span><span id="__span-22-8"><a id="__codelineno-22-8" name="__codelineno-22-8" href="#__codelineno-22-8"></a><span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-22-9"><a id="__codelineno-22-9" name="__codelineno-22-9" href="#__codelineno-22-9"></a><span class="w">        </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Skipping addition scores + mask&quot;</span><span class="p">)</span>
</span><span id="__span-22-10"><a id="__codelineno-22-10" name="__codelineno-22-10" href="#__codelineno-22-10"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-22-11"><a id="__codelineno-22-11" name="__codelineno-22-11" href="#__codelineno-22-11"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-22-12"><a id="__codelineno-22-12" name="__codelineno-22-12" href="#__codelineno-22-12"></a><span class="p">}</span>
</span></code></pre></div>
<p>We can see output lines in the "debug.log" file if debugging is enabled, as follows:</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-23-1"><a id="__codelineno-23-1" name="__codelineno-23-1" href="#__codelineno-23-1"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>Calling<span class="w"> </span>ml.Add<span class="w"> </span>to<span class="w"> </span>calculate<span class="w"> </span>scores<span class="w"> </span>shape<span class="o">([</span><span class="m">32</span><span class="w"> </span><span class="m">22</span><span class="w"> </span><span class="m">22</span><span class="o">])</span><span class="w"> </span>+<span class="w"> </span>mask<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">22</span><span class="o">])</span><span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>scores<span class="w"> </span>...
</span></code></pre></div>
<h3 id="15312-apply-softmax-over-scores"><strong>15.3.12. Apply Softmax over scores</strong><a class="headerlink" href="#15312-apply-softmax-over-scores" title="Permanent link">&para;</a></h3>
<div class="language-py highlight"><pre><span></span><code><span id="__span-24-1"><a id="__codelineno-24-1" name="__codelineno-24-1" href="#__codelineno-24-1"></a><span class="c1">#Goal in Python manner:</span>
</span><span id="__span-24-2"><a id="__codelineno-24-2" name="__codelineno-24-2" href="#__codelineno-24-2"></a><span class="n">scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">xq</span><span class="p">)</span>
</span></code></pre></div>
<p>In this step, we perform <a href="https://en.wikipedia.org/wiki/Softmax_function" target="_blank">Softmax</a> operation over the scores.</p>
<blockquote>
<p>For more information, refer to: <a href="https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html" target="_blank">torch.nn.Softmax</a>.</p>
</blockquote>
<p>To achieve this:</p>
<ul>
<li>Convert the <code>scores</code> tensor data type to <code>DT_F32</code> (float32),</li>
<li>Call <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/ml/operations_impl.go" target="_blank">ml.Softmax</a> function,</li>
<li>Convert the result data type to <code>DT_BF16</code> and assign into <code>scores</code> tensor.</li>
</ul>
<p><sup>from <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">src/model/llamatransformer.go</a></sup></p>
<div class="language-go highlight"><pre><span></span><code><span id="__span-25-1"><a id="__codelineno-25-1" name="__codelineno-25-1" href="#__codelineno-25-1"></a><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">lat</span><span class="w"> </span><span class="o">*</span><span class="nx">LlamaAttention</span><span class="p">)</span><span class="w"> </span><span class="nx">Forward</span><span class="p">(</span><span class="nx">infContext</span><span class="w"> </span><span class="o">*</span><span class="nx">InferenceContext</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">startPos</span><span class="w"> </span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="nx">freqsCis</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">mask</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-25-2"><a id="__codelineno-25-2" name="__codelineno-25-2" href="#__codelineno-25-2"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-25-3"><a id="__codelineno-25-3" name="__codelineno-25-3" href="#__codelineno-25-3"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Converting scores tensor shape(%v) to Float32 tensor -&gt; tensor scores&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">scores</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-25-4"><a id="__codelineno-25-4" name="__codelineno-25-4" href="#__codelineno-25-4"></a><span class="w">    </span><span class="nx">scores</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">scores</span><span class="p">.</span><span class="nx">ToFloat32</span><span class="p">()</span><span class="w"> </span><span class="c1">// example shape=[32,5,5] dtype=DT_F32</span>
</span><span id="__span-25-5"><a id="__codelineno-25-5" name="__codelineno-25-5" href="#__codelineno-25-5"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-25-6"><a id="__codelineno-25-6" name="__codelineno-25-6" href="#__codelineno-25-6"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-25-7"><a id="__codelineno-25-7" name="__codelineno-25-7" href="#__codelineno-25-7"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-25-8"><a id="__codelineno-25-8" name="__codelineno-25-8" href="#__codelineno-25-8"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Calling ml.Softmax for scores shape(%v) and dim %d -&gt; tensor scores&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">scores</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="nx">scores</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-25-9"><a id="__codelineno-25-9" name="__codelineno-25-9" href="#__codelineno-25-9"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">scores</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">ml</span><span class="p">.</span><span class="nx">Softmax</span><span class="p">(</span><span class="nx">scores</span><span class="p">,</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="nx">scores</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">);</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="c1">// example shape=[32,5,5] dtype=DT_F32</span>
</span><span id="__span-25-10"><a id="__codelineno-25-10" name="__codelineno-25-10" href="#__codelineno-25-10"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-25-11"><a id="__codelineno-25-11" name="__codelineno-25-11" href="#__codelineno-25-11"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-25-12"><a id="__codelineno-25-12" name="__codelineno-25-12" href="#__codelineno-25-12"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Converting scores tensor shape(%v) to BFloat16 tensor -&gt; tensor scores&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">scores</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-25-13"><a id="__codelineno-25-13" name="__codelineno-25-13" href="#__codelineno-25-13"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">scores</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">scores</span><span class="p">.</span><span class="nx">ToBFloat16</span><span class="p">();</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="c1">// example shape=[32,5,5] (N_Heads, sequenceLength, sequenceLength) dtype=DT_BF16</span>
</span><span id="__span-25-14"><a id="__codelineno-25-14" name="__codelineno-25-14" href="#__codelineno-25-14"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-25-15"><a id="__codelineno-25-15" name="__codelineno-25-15" href="#__codelineno-25-15"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-25-16"><a id="__codelineno-25-16" name="__codelineno-25-16" href="#__codelineno-25-16"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-25-17"><a id="__codelineno-25-17" name="__codelineno-25-17" href="#__codelineno-25-17"></a><span class="p">}</span>
</span></code></pre></div>
<p>We can see output lines in the "debug.log" file if debugging is enabled, as follows:</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-26-1"><a id="__codelineno-26-1" name="__codelineno-26-1" href="#__codelineno-26-1"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>Converting<span class="w"> </span>scores<span class="w"> </span>tensor<span class="w"> </span>shape<span class="o">([</span><span class="m">32</span><span class="w"> </span><span class="m">22</span><span class="w"> </span><span class="m">22</span><span class="o">])</span><span class="w"> </span>to<span class="w"> </span>Float32<span class="w"> </span>tensor<span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>scores<span class="w"> </span>...
</span><span id="__span-26-2"><a id="__codelineno-26-2" name="__codelineno-26-2" href="#__codelineno-26-2"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>Calling<span class="w"> </span>ml.Softmax<span class="w"> </span><span class="k">for</span><span class="w"> </span>scores<span class="w"> </span>shape<span class="o">([</span><span class="m">32</span><span class="w"> </span><span class="m">22</span><span class="w"> </span><span class="m">22</span><span class="o">])</span><span class="w"> </span>and<span class="w"> </span>dim<span class="w"> </span><span class="m">2</span><span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>scores<span class="w"> </span>...
</span><span id="__span-26-3"><a id="__codelineno-26-3" name="__codelineno-26-3" href="#__codelineno-26-3"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>Converting<span class="w"> </span>scores<span class="w"> </span>tensor<span class="w"> </span>shape<span class="o">([</span><span class="m">32</span><span class="w"> </span><span class="m">22</span><span class="w"> </span><span class="m">22</span><span class="o">])</span><span class="w"> </span>to<span class="w"> </span>BFloat16<span class="w"> </span>tensor<span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>scores<span class="w"> </span>...
</span></code></pre></div>
<h3 id="15313-multiply-values-tensor-and-scores-tensor"><strong>15.3.13. Multiply values tensor and scores tensor</strong><a class="headerlink" href="#15313-multiply-values-tensor-and-scores-tensor" title="Permanent link">&para;</a></h3>
<p><img alt="STAGE 14: Forward Pass Through Attention Module - Calculate output Diagram" src="../images/DIAG01-STAGE14-attention-calculate-output.drawio.svg" />
<sup><em>Diagram: </em><em>Forward Pass Through Attention Module - Calculate output</em><em>. For the complete diagram, <a href="../20-DIAGRAMS/#complete-model-diagram">click here</a>.</em></sup></p>
<div class="language-py highlight"><pre><span></span><code><span id="__span-27-1"><a id="__codelineno-27-1" name="__codelineno-27-1" href="#__codelineno-27-1"></a><span class="c1">#Goal in Python manner:</span>
</span><span id="__span-27-2"><a id="__codelineno-27-2" name="__codelineno-27-2" href="#__codelineno-27-2"></a><span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span>
</span><span id="__span-27-3"><a id="__codelineno-27-3" name="__codelineno-27-3" href="#__codelineno-27-3"></a><span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></code></pre></div>
<ul>
<li>Perform a matrix multiplication over <code>scores</code> with shape of <code>{32, 22, 22}</code> and <code>values</code> with shape of <code>{32, 32, 128}</code>, assign the result with shape of <code>{32, 22, 128}</code> into <code>output</code> tensor,</li>
<li>Transpose the <code>output</code>'s <code>0th</code> and <code>1st</code> dimensions to shape of <code>{22, 32, 128}</code>,</li>
<li>Reshape the <code>output</code> to the shape of <code>{22, 4096} = {sequenceLength, output.GetElementCount() / sequenceLength}</code>,</li>
</ul>
<p><sup>from <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">src/model/llamatransformer.go</a></sup></p>
<div class="language-go highlight"><pre><span></span><code><span id="__span-28-1"><a id="__codelineno-28-1" name="__codelineno-28-1" href="#__codelineno-28-1"></a><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">lat</span><span class="w"> </span><span class="o">*</span><span class="nx">LlamaAttention</span><span class="p">)</span><span class="w"> </span><span class="nx">Forward</span><span class="p">(</span><span class="nx">infContext</span><span class="w"> </span><span class="o">*</span><span class="nx">InferenceContext</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">startPos</span><span class="w"> </span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="nx">freqsCis</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">mask</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-28-2"><a id="__codelineno-28-2" name="__codelineno-28-2" href="#__codelineno-28-2"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-28-3"><a id="__codelineno-28-3" name="__codelineno-28-3" href="#__codelineno-28-3"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Calling ml.MatMul for scores shape(%v) and values shape(%v) -&gt; tensor output&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">scores</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">values</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-28-4"><a id="__codelineno-28-4" name="__codelineno-28-4" href="#__codelineno-28-4"></a><span class="w">    </span><span class="nx">output</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">ml</span><span class="p">.</span><span class="nx">MatMul</span><span class="p">(</span><span class="nx">scores</span><span class="p">,</span><span class="w"> </span><span class="nx">values</span><span class="p">)</span>
</span><span id="__span-28-5"><a id="__codelineno-28-5" name="__codelineno-28-5" href="#__codelineno-28-5"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-28-6"><a id="__codelineno-28-6" name="__codelineno-28-6" href="#__codelineno-28-6"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-28-7"><a id="__codelineno-28-7" name="__codelineno-28-7" href="#__codelineno-28-7"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-28-8"><a id="__codelineno-28-8" name="__codelineno-28-8" href="#__codelineno-28-8"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">output</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">output</span><span class="p">.</span><span class="nx">Transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-28-9"><a id="__codelineno-28-9" name="__codelineno-28-9" href="#__codelineno-28-9"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-28-10"><a id="__codelineno-28-10" name="__codelineno-28-10" href="#__codelineno-28-10"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-28-11"><a id="__codelineno-28-11" name="__codelineno-28-11" href="#__codelineno-28-11"></a><span class="w">    </span><span class="nx">outputTrailingSize</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">output</span><span class="p">.</span><span class="nx">GetElementCount</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nx">sequenceLength</span>
</span><span id="__span-28-12"><a id="__codelineno-28-12" name="__codelineno-28-12" href="#__codelineno-28-12"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">output</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">output</span><span class="p">.</span><span class="nx">Reshape</span><span class="p">([]</span><span class="kt">int</span><span class="p">{</span><span class="nx">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="nx">outputTrailingSize</span><span class="p">});</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-28-13"><a id="__codelineno-28-13" name="__codelineno-28-13" href="#__codelineno-28-13"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-28-14"><a id="__codelineno-28-14" name="__codelineno-28-14" href="#__codelineno-28-14"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-28-15"><a id="__codelineno-28-15" name="__codelineno-28-15" href="#__codelineno-28-15"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-28-16"><a id="__codelineno-28-16" name="__codelineno-28-16" href="#__codelineno-28-16"></a><span class="p">}</span>
</span></code></pre></div>
<p>We can see output lines in the "debug.log" file if debugging is enabled, as follows:</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-29-1"><a id="__codelineno-29-1" name="__codelineno-29-1" href="#__codelineno-29-1"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>Calling<span class="w"> </span>ml.MatMul<span class="w"> </span><span class="k">for</span><span class="w"> </span>scores<span class="w"> </span>shape<span class="o">([</span><span class="m">32</span><span class="w"> </span><span class="m">22</span><span class="w"> </span><span class="m">22</span><span class="o">])</span><span class="w"> </span>and<span class="w"> </span>values<span class="w"> </span>shape<span class="o">([</span><span class="m">32</span><span class="w"> </span><span class="m">22</span><span class="w"> </span><span class="m">128</span><span class="o">])</span><span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>output<span class="w"> </span>...
</span></code></pre></div>
<h3 id="15314-apply-attention-output-weights"><strong>15.3.14. Apply attention output weights</strong><a class="headerlink" href="#15314-apply-attention-output-weights" title="Permanent link">&para;</a></h3>
<p>We have the output weights of our attention module in the <code>lat.attn_wo</code> tensor. We perform a linear transformation with our <code>output</code> tensor (with the shape of <code>{32, 4096}</code>) with the <code>lat.attn_wo</code> weights tensor (with shape of <code>{4096, 4096}</code>). Then, we return this result with the shape of <code>{32, 4096}</code> as output of the attention model.</p>
<p><sup>from <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">src/model/llamatransformer.go</a></sup></p>
<div class="language-go highlight"><pre><span></span><code><span id="__span-30-1"><a id="__codelineno-30-1" name="__codelineno-30-1" href="#__codelineno-30-1"></a><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">lat</span><span class="w"> </span><span class="o">*</span><span class="nx">LlamaAttention</span><span class="p">)</span><span class="w"> </span><span class="nx">Forward</span><span class="p">(</span><span class="nx">infContext</span><span class="w"> </span><span class="o">*</span><span class="nx">InferenceContext</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">startPos</span><span class="w"> </span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="nx">freqsCis</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">mask</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-30-2"><a id="__codelineno-30-2" name="__codelineno-30-2" href="#__codelineno-30-2"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-30-3"><a id="__codelineno-30-3" name="__codelineno-30-3" href="#__codelineno-30-3"></a><span class="w">    </span><span class="cm">/*</span>
</span><span id="__span-30-4"><a id="__codelineno-30-4" name="__codelineno-30-4" href="#__codelineno-30-4"></a><span class="cm">        Apply lat.attn_wo weights to output</span>
</span><span id="__span-30-5"><a id="__codelineno-30-5" name="__codelineno-30-5" href="#__codelineno-30-5"></a><span class="cm">    */</span>
</span><span id="__span-30-6"><a id="__codelineno-30-6" name="__codelineno-30-6" href="#__codelineno-30-6"></a>
</span><span id="__span-30-7"><a id="__codelineno-30-7" name="__codelineno-30-7" href="#__codelineno-30-7"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Calling ml.LinearTransformation for output shape(%v) and LlamaAttention.attn_wo weights shape(%v) -&gt; tensor output&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">output</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">lat</span><span class="p">.</span><span class="nx">attn_wo</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-30-8"><a id="__codelineno-30-8" name="__codelineno-30-8" href="#__codelineno-30-8"></a><span class="w">    </span><span class="c1">// lat.attn_wo: [out_features, in_features] -&gt; shape: [4096 4096] -&gt; [N_Heads * HeadDim, Dim]</span>
</span><span id="__span-30-9"><a id="__codelineno-30-9" name="__codelineno-30-9" href="#__codelineno-30-9"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">output</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">ml</span><span class="p">.</span><span class="nx">LinearTransformation</span><span class="p">(</span><span class="nx">output</span><span class="p">,</span><span class="w"> </span><span class="nx">lat</span><span class="p">.</span><span class="nx">attn_wo</span><span class="p">);</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-30-10"><a id="__codelineno-30-10" name="__codelineno-30-10" href="#__codelineno-30-10"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-30-11"><a id="__codelineno-30-11" name="__codelineno-30-11" href="#__codelineno-30-11"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-30-12"><a id="__codelineno-30-12" name="__codelineno-30-12" href="#__codelineno-30-12"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Returning tensor output: shape(%v)&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">output</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-30-13"><a id="__codelineno-30-13" name="__codelineno-30-13" href="#__codelineno-30-13"></a><span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nx">output</span><span class="p">,</span><span class="w"> </span><span class="kc">nil</span>
</span><span id="__span-30-14"><a id="__codelineno-30-14" name="__codelineno-30-14" href="#__codelineno-30-14"></a><span class="p">}</span>
</span></code></pre></div>
<p>We can see output lines in the "debug.log" file if debugging is enabled, as follows:</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-31-1"><a id="__codelineno-31-1" name="__codelineno-31-1" href="#__codelineno-31-1"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>Calling<span class="w"> </span>ml.LinearTransformation<span class="w"> </span><span class="k">for</span><span class="w"> </span>output<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>and<span class="w"> </span>LlamaAttention.attn_wo<span class="w"> </span>weights<span class="w"> </span>shape<span class="o">([</span><span class="m">4096</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>output<span class="w"> </span>...
</span><span id="__span-31-2"><a id="__codelineno-31-2" name="__codelineno-31-2" href="#__codelineno-31-2"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>Returning<span class="w"> </span>tensor<span class="w"> </span>output:<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>...
</span></code></pre></div>
<h2 id="154-adding-the-attention-module-output-to-current-tensor"><strong>15.4. Adding the attention module output to current tensor</strong><a class="headerlink" href="#154-adding-the-attention-module-output-to-current-tensor" title="Permanent link">&para;</a></h2>
<p><img alt="STAGE 15: Add attention module output and current tensor Diagram" src="../images/DIAG01-STAGE15-add-attention-output-and-current-tensor.drawio.svg" />
<sup><em>Diagram: </em><em>Add attention module output and current tensor</em><em>. For the complete diagram, <a href="../20-DIAGRAMS/#complete-model-diagram">click here</a>.</em></sup></p>
<p>Now, we returned to our latest position in the <code>LlamaTransformerBlock.Forward(...)</code> method.</p>
<p>We had the <code>x</code> tensor argument. In our case, at the first iteration, <code>x</code> is input tensor, at the other iterations, <code>x</code> is output of previous <code>LlamaTransformerBlock</code>. In our case, at the first iteration, the shape of this tensor is <code>{22, 4096}</code>. 22 stands for sequence length, 4096 stands for the embedding layer dimension. <code>normalizedX</code> which is the resulting tensor will have same shape as the input, <code>{22, 4096}</code>.</p>
<p>Also, we have the <code>h</code> tensor with the shape of <code>{22, 4096}</code>, which is the output of our attention module <code>LlamaAttention</code>.</p>
<p>We add <code>x</code> and <code>h</code> tensors via <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/ml/operations_impl.go" target="_blank">ml.Add(...)</a> function and assign the result into <code>h</code> tensor.</p>
<p><sup>from <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">src/model/llamatransformer.go</a></sup></p>
<div class="language-go highlight"><pre><span></span><code><span id="__span-32-1"><a id="__codelineno-32-1" name="__codelineno-32-1" href="#__codelineno-32-1"></a><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">ltb</span><span class="w"> </span><span class="o">*</span><span class="nx">LlamaTransformerBlock</span><span class="p">)</span><span class="w"> </span><span class="nx">Forward</span><span class="p">(</span><span class="nx">infContext</span><span class="w"> </span><span class="o">*</span><span class="nx">InferenceContext</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">startPos</span><span class="w"> </span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="nx">freqsCis</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">mask</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-32-2"><a id="__codelineno-32-2" name="__codelineno-32-2" href="#__codelineno-32-2"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-32-3"><a id="__codelineno-32-3" name="__codelineno-32-3" href="#__codelineno-32-3"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Calling ml.Add to calculate x shape(%v) + h shape(%v) -&gt; tensor h&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">h</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-32-4"><a id="__codelineno-32-4" name="__codelineno-32-4" href="#__codelineno-32-4"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">h</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">ml</span><span class="p">.</span><span class="nx">Add</span><span class="p">(</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">h</span><span class="p">);</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-32-5"><a id="__codelineno-32-5" name="__codelineno-32-5" href="#__codelineno-32-5"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-32-6"><a id="__codelineno-32-6" name="__codelineno-32-6" href="#__codelineno-32-6"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-32-7"><a id="__codelineno-32-7" name="__codelineno-32-7" href="#__codelineno-32-7"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-32-8"><a id="__codelineno-32-8" name="__codelineno-32-8" href="#__codelineno-32-8"></a><span class="p">}</span>
</span></code></pre></div>
<p>We can see output lines in the "debug.log" file if debugging is enabled, as follows:</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-33-1"><a id="__codelineno-33-1" name="__codelineno-33-1" href="#__codelineno-33-1"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>Calling<span class="w"> </span>ml.Add<span class="w"> </span>to<span class="w"> </span>calculate<span class="w"> </span>x<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>+<span class="w"> </span>h<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>h<span class="w"> </span>...
</span></code></pre></div>
<h2 id="155-performing-forward-pass-through-feed-forward-prenormalization-rmsnormforward"><strong>15.5. Performing Forward Pass Through Feed-Forward Prenormalization - RMSNorm.Forward(...)</strong><a class="headerlink" href="#155-performing-forward-pass-through-feed-forward-prenormalization-rmsnormforward" title="Permanent link">&para;</a></h2>
<p><img alt="STAGE 16: Forward Pass Through Feed-Forward Pre-normalization Diagram" src="../images/DIAG01-STAGE16-forward-pass-through-ffn-prenormalization.drawio.svg" />
<sup><em>Diagram: </em><em>Forward Pass Through Feed-Forward Pre-normalization</em><em>. For the complete diagram, <a href="../20-DIAGRAMS/#complete-model-diagram">click here</a>.</em></sup></p>
<p>Now, we have the <code>h</code> tensor. We perform RMSNorm over <code>h</code> tensor with normalization weights of <code>ltb.ffn_norm</code>, and assign the result into <code>normalizedH</code> which is the resulting tensor will have the same shape as the <code>h</code>, <code>{22, 4096}</code>.</p>
<p><sup>from <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">src/model/llamatransformer.go</a></sup></p>
<div class="language-go highlight"><pre><span></span><code><span id="__span-34-1"><a id="__codelineno-34-1" name="__codelineno-34-1" href="#__codelineno-34-1"></a><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">ltb</span><span class="w"> </span><span class="o">*</span><span class="nx">LlamaTransformerBlock</span><span class="p">)</span><span class="w"> </span><span class="nx">Forward</span><span class="p">(</span><span class="nx">infContext</span><span class="w"> </span><span class="o">*</span><span class="nx">InferenceContext</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">startPos</span><span class="w"> </span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="nx">freqsCis</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">mask</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-34-2"><a id="__codelineno-34-2" name="__codelineno-34-2" href="#__codelineno-34-2"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-34-3"><a id="__codelineno-34-3" name="__codelineno-34-3" href="#__codelineno-34-3"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Calling RMSNorm for tensor h shape(%v) and LlamaTransformerBlock.ffn_norm weights shape(%v) -&gt; tensor normalizedH&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">ltb</span><span class="p">.</span><span class="nx">ffn_norm</span><span class="p">.</span><span class="nx">weights</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-34-4"><a id="__codelineno-34-4" name="__codelineno-34-4" href="#__codelineno-34-4"></a><span class="w">    </span><span class="nx">normalizedH</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">ltb</span><span class="p">.</span><span class="nx">ffn_norm</span><span class="p">.</span><span class="nx">Forward</span><span class="p">(</span><span class="nx">infContext</span><span class="p">,</span><span class="w"> </span><span class="nx">h</span><span class="p">)</span>
</span><span id="__span-34-5"><a id="__codelineno-34-5" name="__codelineno-34-5" href="#__codelineno-34-5"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-34-6"><a id="__codelineno-34-6" name="__codelineno-34-6" href="#__codelineno-34-6"></a><span class="p">}</span>
</span></code></pre></div>
<p>We can see output lines in the "debug.log" file if debugging is enabled, as follows:</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-35-1"><a id="__codelineno-35-1" name="__codelineno-35-1" href="#__codelineno-35-1"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>Calling<span class="w"> </span>RMSNorm<span class="w"> </span><span class="k">for</span><span class="w"> </span>tensor<span class="w"> </span>h<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>and<span class="w"> </span>LlamaTransformerBlock.ffn_norm<span class="w"> </span>weights<span class="w"> </span>shape<span class="o">([</span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>normalizedH<span class="w"> </span>...
</span></code></pre></div>
<h2 id="156-performing-forward-pass-through-feed-forward-module-calling"><strong>15.6. Performing Forward Pass Through Feed-Forward Module - Calling</strong><a class="headerlink" href="#156-performing-forward-pass-through-feed-forward-module-calling" title="Permanent link">&para;</a></h2>
<p><sup>from <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">src/model/llamatransformer.go</a></sup></p>
<div class="language-go highlight"><pre><span></span><code><span id="__span-36-1"><a id="__codelineno-36-1" name="__codelineno-36-1" href="#__codelineno-36-1"></a><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">ltb</span><span class="w"> </span><span class="o">*</span><span class="nx">LlamaTransformerBlock</span><span class="p">)</span><span class="w"> </span><span class="nx">Forward</span><span class="p">(</span><span class="nx">infContext</span><span class="w"> </span><span class="o">*</span><span class="nx">InferenceContext</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">startPos</span><span class="w"> </span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="nx">freqsCis</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">mask</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-36-2"><a id="__codelineno-36-2" name="__codelineno-36-2" href="#__codelineno-36-2"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-36-3"><a id="__codelineno-36-3" name="__codelineno-36-3" href="#__codelineno-36-3"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Calling LlamaFeedForward.Forward for tensor normalizedH shape(%v) -&gt; tensor ffnOutput&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">normalizedH</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-36-4"><a id="__codelineno-36-4" name="__codelineno-36-4" href="#__codelineno-36-4"></a><span class="w">    </span><span class="nx">ffnOutput</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">ltb</span><span class="p">.</span><span class="nx">feedForward</span><span class="p">.</span><span class="nx">Forward</span><span class="p">(</span><span class="nx">normalizedH</span><span class="p">)</span>
</span><span id="__span-36-5"><a id="__codelineno-36-5" name="__codelineno-36-5" href="#__codelineno-36-5"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-36-6"><a id="__codelineno-36-6" name="__codelineno-36-6" href="#__codelineno-36-6"></a><span class="p">}</span>
</span></code></pre></div>
<p>We can see output lines in the "debug.log" file if debugging is enabled, as follows:</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-37-1"><a id="__codelineno-37-1" name="__codelineno-37-1" href="#__codelineno-37-1"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>Calling<span class="w"> </span>LlamaFeedForward.Forward<span class="w"> </span><span class="k">for</span><span class="w"> </span>tensor<span class="w"> </span>normalizedH<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>ffnOutput<span class="w"> </span>...
</span></code></pre></div>
<h2 id="157-performing-forward-pass-through-feed-forward-module-llamafeedforwardforward"><strong>15.7. Performing Forward Pass Through Feed-Forward Module - LlamaFeedForward.Forward(...)</strong><a class="headerlink" href="#157-performing-forward-pass-through-feed-forward-module-llamafeedforwardforward" title="Permanent link">&para;</a></h2>
<p><img alt="STAGE 17: Forward Pass Through Feed-Forward Module Diagram" src="../images/DIAG01-STAGE17-forward-pass-through-ffn-module.drawio.svg" />
<sup><em>Diagram: </em><em>Forward Pass Through Feed-Forward Module</em><em>. For the complete diagram, <a href="../20-DIAGRAMS/#complete-model-diagram">click here</a>.</em></sup></p>
<div class="language-py highlight"><pre><span></span><code><span id="__span-38-1"><a id="__codelineno-38-1" name="__codelineno-38-1" href="#__codelineno-38-1"></a><span class="c1">#Goal in Python manner:</span>
</span><span id="__span-38-2"><a id="__codelineno-38-2" name="__codelineno-38-2" href="#__codelineno-38-2"></a><span class="bp">self</span><span class="o">.</span><span class="n">w2</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">w3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span><span id="__span-38-3"><a id="__codelineno-38-3" name="__codelineno-38-3" href="#__codelineno-38-3"></a>
</span><span id="__span-38-4"><a id="__codelineno-38-4" name="__codelineno-38-4" href="#__codelineno-38-4"></a><span class="c1"># Python code with our variable names:</span>
</span><span id="__span-38-5"><a id="__codelineno-38-5" name="__codelineno-38-5" href="#__codelineno-38-5"></a><span class="bp">self</span><span class="o">.</span><span class="n">ffn_down</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ffn_gate</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_up</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span></code></pre></div>
<p>In this stage, we have a feed-forward neural network module consisting multiple weight tensors for each of 32 transformer block layers, with names: <code>w1</code>, <code>w2</code>, and <code>w3</code> in original Python repository, <code>ffn_gate</code>, <code>ffn_down</code>, and <code>ffn_up</code> in our project, respectively.</p>
<p>The steps are:</p>
<ul>
<li>Perform a linear transformation over <code>x</code> with shape of <code>{22, 4096}</code> and <code>lff.ffn_gate</code> weights with shape of <code>{14336, 4096}</code>, assign the resulting tensor with shape of <code>{22, 14336}</code> into <code>h</code>,</li>
<li>Perform Sigmoid Linear Unit (SiLU) function over the <code>h</code> tensor via <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/ml/activations.go" target="_blank">ml.Silu(...)</a> function,
    &gt;For more information, refer to: <a href="https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html" target="_blank">torch.nn.SiLU</a>.</li>
<li>Perform a linear transformation over <code>x</code> with shape of <code>{22, 4096}</code> and <code>lff.ffn_up</code> weights with shape of <code>{14336, 4096}</code>, assign the resulting tensor with shape of <code>{22, 14336}</code> into <code>ffnUpX</code>,</li>
<li>Perform an element-wise multiplication with <code>h</code> tensor with shape of <code>{22, 14336}</code> and <code>ffnUpX</code> tensor with shape of <code>{22, 14336}</code> via <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/ml/operations_impl.go" target="_blank">ml.MultiplyElementwise</a>. Output shape is <code>{22, 14336}</code>, assign the result into <code>h</code> variable,</li>
<li>Perform a linear transformation over <code>h</code> with shape of <code>{22, 14336}</code> and <code>lff.ffn_down</code> weights with shape of <code>{4096, 14336}</code>, assign the resulting tensor with shape of <code>{22, 4096}</code> into <code>output</code>,</li>
<li>Return the <code>output</code> tensor.</li>
</ul>
<p><sup>from <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">src/model/llamatransformer.go</a></sup></p>
<div class="language-go highlight"><pre><span></span><code><span id="__span-39-1"><a id="__codelineno-39-1" name="__codelineno-39-1" href="#__codelineno-39-1"></a><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">lff</span><span class="w"> </span><span class="o">*</span><span class="nx">LlamaFeedForward</span><span class="p">)</span><span class="w"> </span><span class="nx">Forward</span><span class="p">(</span><span class="nx">x</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-39-2"><a id="__codelineno-39-2" name="__codelineno-39-2" href="#__codelineno-39-2"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-39-3"><a id="__codelineno-39-3" name="__codelineno-39-3" href="#__codelineno-39-3"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Calling ml.LinearTransformation for x shape(%v) and LlamaFeedForward.ffn_gate weights shape(%v) -&gt; tensor h&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">lff</span><span class="p">.</span><span class="nx">ffn_gate</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-39-4"><a id="__codelineno-39-4" name="__codelineno-39-4" href="#__codelineno-39-4"></a><span class="w">    </span><span class="nx">h</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">ml</span><span class="p">.</span><span class="nx">LinearTransformation</span><span class="p">(</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">lff</span><span class="p">.</span><span class="nx">ffn_gate</span><span class="p">)</span>
</span><span id="__span-39-5"><a id="__codelineno-39-5" name="__codelineno-39-5" href="#__codelineno-39-5"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-39-6"><a id="__codelineno-39-6" name="__codelineno-39-6" href="#__codelineno-39-6"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-39-7"><a id="__codelineno-39-7" name="__codelineno-39-7" href="#__codelineno-39-7"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-39-8"><a id="__codelineno-39-8" name="__codelineno-39-8" href="#__codelineno-39-8"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Calling ml.Silu for h shape(%v) -&gt; tensor h&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">h</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-39-9"><a id="__codelineno-39-9" name="__codelineno-39-9" href="#__codelineno-39-9"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">h</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">ml</span><span class="p">.</span><span class="nx">Silu</span><span class="p">(</span><span class="nx">h</span><span class="p">);</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-39-10"><a id="__codelineno-39-10" name="__codelineno-39-10" href="#__codelineno-39-10"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-39-11"><a id="__codelineno-39-11" name="__codelineno-39-11" href="#__codelineno-39-11"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-39-12"><a id="__codelineno-39-12" name="__codelineno-39-12" href="#__codelineno-39-12"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Calling ml.LinearTransformation for x shape(%v) and LlamaFeedForward.ffn_up weights shape(%v) -&gt; tensor ffnUpX&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">lff</span><span class="p">.</span><span class="nx">ffn_up</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-39-13"><a id="__codelineno-39-13" name="__codelineno-39-13" href="#__codelineno-39-13"></a><span class="w">    </span><span class="nx">ffnUpX</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">ml</span><span class="p">.</span><span class="nx">LinearTransformation</span><span class="p">(</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">lff</span><span class="p">.</span><span class="nx">ffn_up</span><span class="p">)</span>
</span><span id="__span-39-14"><a id="__codelineno-39-14" name="__codelineno-39-14" href="#__codelineno-39-14"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-39-15"><a id="__codelineno-39-15" name="__codelineno-39-15" href="#__codelineno-39-15"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-39-16"><a id="__codelineno-39-16" name="__codelineno-39-16" href="#__codelineno-39-16"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-39-17"><a id="__codelineno-39-17" name="__codelineno-39-17" href="#__codelineno-39-17"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Calling ml.MultiplyElementwise for h shape(%v) and ffnUpX weights shape(%v) -&gt; tensor ffnUpX&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">h</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">ffnUpX</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-39-18"><a id="__codelineno-39-18" name="__codelineno-39-18" href="#__codelineno-39-18"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">h</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">ml</span><span class="p">.</span><span class="nx">MultiplyElementwise</span><span class="p">(</span><span class="nx">h</span><span class="p">,</span><span class="w"> </span><span class="nx">ffnUpX</span><span class="p">);</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-39-19"><a id="__codelineno-39-19" name="__codelineno-39-19" href="#__codelineno-39-19"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-39-20"><a id="__codelineno-39-20" name="__codelineno-39-20" href="#__codelineno-39-20"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-39-21"><a id="__codelineno-39-21" name="__codelineno-39-21" href="#__codelineno-39-21"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Calling ml.LinearTransformation for h shape(%v) and LlamaFeedForward.ffn_down weights shape(%v) -&gt; tensor output&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">h</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">lff</span><span class="p">.</span><span class="nx">ffn_down</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-39-22"><a id="__codelineno-39-22" name="__codelineno-39-22" href="#__codelineno-39-22"></a><span class="w">    </span><span class="nx">output</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">ml</span><span class="p">.</span><span class="nx">LinearTransformation</span><span class="p">(</span><span class="nx">h</span><span class="p">,</span><span class="w"> </span><span class="nx">lff</span><span class="p">.</span><span class="nx">ffn_down</span><span class="p">)</span>
</span><span id="__span-39-23"><a id="__codelineno-39-23" name="__codelineno-39-23" href="#__codelineno-39-23"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-39-24"><a id="__codelineno-39-24" name="__codelineno-39-24" href="#__codelineno-39-24"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-39-25"><a id="__codelineno-39-25" name="__codelineno-39-25" href="#__codelineno-39-25"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-39-26"><a id="__codelineno-39-26" name="__codelineno-39-26" href="#__codelineno-39-26"></a><span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nx">output</span><span class="p">,</span><span class="w"> </span><span class="kc">nil</span>
</span><span id="__span-39-27"><a id="__codelineno-39-27" name="__codelineno-39-27" href="#__codelineno-39-27"></a><span class="p">}</span>
</span></code></pre></div>
<p>We can see output lines in the "debug.log" file if debugging is enabled, as follows:</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-40-1"><a id="__codelineno-40-1" name="__codelineno-40-1" href="#__codelineno-40-1"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>Calling<span class="w"> </span>ml.LinearTransformation<span class="w"> </span><span class="k">for</span><span class="w"> </span>x<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>and<span class="w"> </span>LlamaFeedForward.ffn_gate<span class="w"> </span>weights<span class="w"> </span>shape<span class="o">([</span><span class="m">14336</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>h<span class="w"> </span>...
</span><span id="__span-40-2"><a id="__codelineno-40-2" name="__codelineno-40-2" href="#__codelineno-40-2"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>Calling<span class="w"> </span>ml.LinearTransformation<span class="w"> </span><span class="k">for</span><span class="w"> </span>x<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>and<span class="w"> </span>LlamaFeedForward.ffn_up<span class="w"> </span>weights<span class="w"> </span>shape<span class="o">([</span><span class="m">14336</span><span class="w"> </span><span class="m">4096</span><span class="o">])</span><span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>ffnUpX<span class="w"> </span>...
</span><span id="__span-40-3"><a id="__codelineno-40-3" name="__codelineno-40-3" href="#__codelineno-40-3"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>Calling<span class="w"> </span>ml.MultiplyElementwise<span class="w"> </span><span class="k">for</span><span class="w"> </span>h<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">14336</span><span class="o">])</span><span class="w"> </span>and<span class="w"> </span>ffnUpX<span class="w"> </span>weights<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">14336</span><span class="o">])</span><span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>ffnUpX<span class="w"> </span>...
</span><span id="__span-40-4"><a id="__codelineno-40-4" name="__codelineno-40-4" href="#__codelineno-40-4"></a><span class="o">[</span>DEBUG<span class="o">]</span><span class="w"> </span>...<span class="w"> </span>Calling<span class="w"> </span>ml.LinearTransformation<span class="w"> </span><span class="k">for</span><span class="w"> </span>h<span class="w"> </span>shape<span class="o">([</span><span class="m">22</span><span class="w"> </span><span class="m">14336</span><span class="o">])</span><span class="w"> </span>and<span class="w"> </span>LlamaFeedForward.ffn_down<span class="w"> </span>weights<span class="w"> </span>shape<span class="o">([</span><span class="m">4096</span><span class="w"> </span><span class="m">14336</span><span class="o">])</span><span class="w"> </span>-&gt;<span class="w"> </span>tensor<span class="w"> </span>output<span class="w"> </span>...
</span></code></pre></div>
<h2 id="158-adding-the-feed-forward-network-module-output-to-current-tensor"><strong>15.8. Adding the feed-forward network module output to current tensor</strong><a class="headerlink" href="#158-adding-the-feed-forward-network-module-output-to-current-tensor" title="Permanent link">&para;</a></h2>
<p><img alt="STAGE 18: Add Feed-Forward module output and current tensor Diagram" src="../images/DIAG01-STAGE18-add-ffn-output-and-current-tensor.drawio.svg" />
<sup><em>Diagram: </em><em>Add Feed-Forward module output and current tensor</em><em>. For the complete diagram, <a href="../20-DIAGRAMS/#complete-model-diagram">click here</a>.</em></sup></p>
<p>Now, we returned to our latest position in the <code>LlamaTransformerBlock.Forward(...)</code> method.</p>
<ul>
<li>We have the <code>ffnOutput</code> tensor with the shape of <code>{22, 4096}</code>, which is the output of our feed-forward neural network module <code>LlamaFeedForward</code>.</li>
<li>Also, we had the <code>h</code> tensor as current tensor with the shape of <code>{22, 4096}</code>, which is the output of our attention module <code>LlamaAttention</code>.</li>
<li>We add <code>h</code> and <code>ffnOutput</code> tensors via <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/ml/operations_impl.go" target="_blank">ml.Add(...)</a> function and assign the result with shape of <code>{22, 4096}</code> into <code>output</code> tensor,</li>
<li>Return it as output of <code>LlamaTransformerBlock</code>.</li>
</ul>
<p><sup>from <a href="https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/src/model/llamatransformer.go" target="_blank">src/model/llamatransformer.go</a></sup></p>
<div class="language-go highlight"><pre><span></span><code><span id="__span-41-1"><a id="__codelineno-41-1" name="__codelineno-41-1" href="#__codelineno-41-1"></a><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">ltb</span><span class="w"> </span><span class="o">*</span><span class="nx">LlamaTransformerBlock</span><span class="p">)</span><span class="w"> </span><span class="nx">Forward</span><span class="p">(</span><span class="nx">infContext</span><span class="w"> </span><span class="o">*</span><span class="nx">InferenceContext</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">startPos</span><span class="w"> </span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="nx">freqsCis</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nx">mask</span><span class="w"> </span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="nx">ml</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">,</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-41-2"><a id="__codelineno-41-2" name="__codelineno-41-2" href="#__codelineno-41-2"></a><span class="w">    </span><span class="o">...</span>
</span><span id="__span-41-3"><a id="__codelineno-41-3" name="__codelineno-41-3" href="#__codelineno-41-3"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Calling ml.Add to calculate h shape(%v) + ffnOutput shape(%v) -&gt; tensor output&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">h</span><span class="p">.</span><span class="nx">Size</span><span class="p">,</span><span class="w"> </span><span class="nx">ffnOutput</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-41-4"><a id="__codelineno-41-4" name="__codelineno-41-4" href="#__codelineno-41-4"></a><span class="w">    </span><span class="nx">output</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">ml</span><span class="p">.</span><span class="nx">Add</span><span class="p">(</span><span class="nx">h</span><span class="p">,</span><span class="w"> </span><span class="nx">ffnOutput</span><span class="p">)</span>
</span><span id="__span-41-5"><a id="__codelineno-41-5" name="__codelineno-41-5" href="#__codelineno-41-5"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-41-6"><a id="__codelineno-41-6" name="__codelineno-41-6" href="#__codelineno-41-6"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span>
</span><span id="__span-41-7"><a id="__codelineno-41-7" name="__codelineno-41-7" href="#__codelineno-41-7"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-41-8"><a id="__codelineno-41-8" name="__codelineno-41-8" href="#__codelineno-41-8"></a><span class="w">    </span><span class="nx">common</span><span class="p">.</span><span class="nx">GLogger</span><span class="p">.</span><span class="nx">DebugPrintf</span><span class="p">(</span><span class="s">&quot;Returning tensor output: shape(%v)&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">output</span><span class="p">.</span><span class="nx">Size</span><span class="p">)</span>
</span><span id="__span-41-9"><a id="__codelineno-41-9" name="__codelineno-41-9" href="#__codelineno-41-9"></a><span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nx">output</span><span class="p">,</span><span class="w"> </span><span class="kc">nil</span>
</span><span id="__span-41-10"><a id="__codelineno-41-10" name="__codelineno-41-10" href="#__codelineno-41-10"></a><span class="p">}</span>
</span></code></pre></div>
<p>We can see output lines in the "debug.log" file if debugging is enabled, as follows:</p>
<p>```sh
[DEBUG] ... Calling ml.Add to calculate h shape([22 4096]) + ffnOutput shape([22 4096]) -&gt; tensor output ...
[DEBUG] ... Returning tensor output: shape([22 4096]) ...
````</p>
<p>The flow will continue with next <code>LlamaTransformerBlock</code> layer.</p>







  
  



  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../14-MAKING-PREDICTION-WITH-LLAMA-MODEL-1/" class="md-footer__link md-footer__link--prev" aria-label="Previous: MAKING PREDICTION with LLAMA MODEL - 1">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                MAKING PREDICTION with LLAMA MODEL - 1
              </div>
            </div>
          </a>
        
        
          
          <a href="../16-MAKING-PREDICTION-WITH-LLAMA-MODEL-3/" class="md-footer__link md-footer__link--next" aria-label="Next: MAKING PREDICTION with LLAMA MODEL - 3">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                MAKING PREDICTION with LLAMA MODEL - 3
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2024 - present, Adil Alper DALKIRAN. All rights reserved.
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="https://github.com/adalkiran" target="_blank" rel="noopener" title="adalkiran" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://www.linkedin.com/in/alper-dalkiran/" target="_blank" rel="noopener" title="@aadalkiran" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9L389.2 48zm-24.8 373.8h39.1L151.1 88h-42l255.3 333.8z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://www.linkedin.com/in/alper-dalkiran/" target="_blank" rel="noopener" title="in/alper-dalkiran" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["toc.follow", "toc.integrate", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "navigation.footer", "content.code.copy"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.fe8b6f2b.min.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>
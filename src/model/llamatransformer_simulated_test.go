package model

import (
	"fmt"
	"math"
	"os"
	"reflect"
	"testing"

	"github.com/adalkiran/llama-nuts-and-bolts/src/common"
	"github.com/adalkiran/llama-nuts-and-bolts/src/dtype"
	"github.com/adalkiran/llama-nuts-and-bolts/src/ml"
)

/*
	This simulation was added to ensure all steps are done correctly
	(until only first attention layer).
*/

func testTransformer_Prepare(t *testing.T, skipCompareTestTensor bool, transformer *LlamaTransformer, inputTokens *ml.Tensor, startPos int) (actualInputTensor *ml.Tensor, actualFreqsCis *ml.Tensor, actualMask *ml.Tensor) {
	expectedInputTensorSize := []int{15, 4096}
	// Shortened form as corresponding indices [0, 1, 2, 4093, 4094, 4095]
	expectedInputTensorShortened := [][]float32{
		{-0.0001, 0.0003, -0.0002 /*...,*/, -0.0003, -0.0002, 0.0005},
		{0.0006, -0.0002, 0.0001 /*...,*/, -0.0001, 0.0004, 0.0010},
		{0.0040, 0.0064, 0.0001 /*...,*/, 0.0006, -0.0085, 0.0008},
		/*...,*/
		{-0.0092, -0.0022, 0.0044 /*...,*/, -0.0001, -0.0015, 0.0082},
		{-0.0000, -0.0007, -0.0001 /*...,*/, -0.0002, -0.0006, 0.0010},
		{-0.0026, 0.0038, -0.0005 /*...,*/, 0.0014, -0.0006, 0.0013},
	}

	expectedFreqsCisSize := []int{15, 64}

	expectedMaskSize := []int{15, 15}
	negInf := float32(math.Inf(-1))
	expectedMask := [][]float32{
		{0, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf},
		{0, 0, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf},
		{0, 0, 0, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf},
		{0, 0, 0, 0, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf},
		{0, 0, 0, 0, 0, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf},
		{0, 0, 0, 0, 0, 0, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf},
		{0, 0, 0, 0, 0, 0, 0, negInf, negInf, negInf, negInf, negInf, negInf, negInf, negInf},
		{0, 0, 0, 0, 0, 0, 0, 0, negInf, negInf, negInf, negInf, negInf, negInf, negInf},
		{0, 0, 0, 0, 0, 0, 0, 0, 0, negInf, negInf, negInf, negInf, negInf, negInf},
		{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, negInf, negInf, negInf, negInf, negInf},
		{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, negInf, negInf, negInf, negInf},
		{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, negInf, negInf, negInf},
		{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, negInf, negInf},
		{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, negInf},
		{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
	}
	var err error
	if actualInputTensor, actualFreqsCis, actualMask, err = transformer.prepare(inputTokens, startPos); err != nil {
		t.Fatal(err)
	}

	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedInputTensorShortened, expectedInputTensorSize, actualInputTensor, common.THRESHOLD_F32, true); err != nil {
		t.Fatal(err)
	}

	if !skipCompareTestTensor && !reflect.DeepEqual(expectedFreqsCisSize, actualFreqsCis.Size) {
		t.Fatalf("expected size %v, but got %v", expectedFreqsCisSize, actualFreqsCis.Size)
	}

	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedMask, expectedMaskSize, actualMask, common.THRESHOLD_F32, false); err != nil {
		t.Fatal(err)
	}
	return
}

func testTransformerBlock_AttnNorm_Forward(t *testing.T, skipCompareTestTensor bool, transformerBlock *LlamaTransformerBlock, x *ml.Tensor) *ml.Tensor {
	/*
		normalizedX, err := ltb.attn_norm.Forward(infContext, x)
	*/
	expectedAttnNormPartSize := []int{15, 4096}
	expectedAttnNormPart := [][]float32{
		{-0.0107, 0.0332, -0.0317 /*...,*/, -0.0417, -0.0278, 0.0608},
		{0.1846, -0.0713, 0.0403 /*...,*/, -0.0425, 0.1367, 0.3086},
		{0.4766, 0.7656, 0.0164 /*...,*/, 0.0737, -1.0234, 0.0933},
		/*...,*/
		{-0.8867, -0.2070, 0.4199 /*...,*/, -0.0063, -0.1475, 0.7852},
		{-0.0057, -0.2129, -0.0176 /*...,*/, -0.0518, -0.1855, 0.3047},
		{-0.3887, 0.5703, -0.0698 /*...,*/, 0.2168, -0.0967, 0.1904},
	}

	expectedAttnNormalizedXSize := []int{15, 4096}
	expectedAttnNormalizedX := [][]float32{
		{-0.0006, 0.0069, -0.0142 /*...,*/, -0.0036, -0.0012, 0.0018},
		{0.0099, -0.0148, 0.0181 /*...,*/, -0.0036, 0.0060, 0.0089},
		{0.0255, 0.1592, 0.0073 /*...,*/, 0.0063, -0.0444, 0.0027},
		/*...,*/
		{-0.0474, -0.0430, 0.1875 /*...,*/, -0.0005, -0.0064, 0.0227},
		{-0.0003, -0.0442, -0.0079 /*...,*/, -0.0044, -0.0081, 0.0088},
		{-0.0208, 0.1187, -0.0312 /*...,*/, 0.0186, -0.0042, 0.0055},
	}

	actualAttnNormPart, err := transformerBlock.attn_norm.doNormalization(x)
	if err != nil {
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedAttnNormPart, expectedAttnNormPartSize, actualAttnNormPart, 2*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	actualAttnNormalizedX, err := ml.MultiplyElementwise(actualAttnNormPart, transformerBlock.attn_norm.weights)
	if err != nil {
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedAttnNormalizedX, expectedAttnNormalizedXSize, actualAttnNormalizedX, common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}
	return actualAttnNormalizedX
}

func testTransformerBlock_Attention_Forward(t *testing.T, skipCompareTestTensor bool, infContext *InferenceContext, attention *LlamaAttention, x *ml.Tensor, startPos int, freqsCis *ml.Tensor, mask *ml.Tensor) *ml.Tensor {
	expectedXqSize := []int{15, 4096}
	expectedXq := [][]float32{
		{-0.0537, -0.1094, 0.3438 /*...,*/, 0.4941, -0.4922, 0.2441},
		{0.2100, -0.0306, 0.6016 /*...,*/, 1.0391, -1.7188, 0.9023},
		{0.1133, -0.1660, 0.3906 /*...,*/, 1.9453, -3.1250, 1.5938},
		/*...,*/
		{0.4082, -0.1182, 0.7578 /*...,*/, 1.8203, -2.8594, 1.6172},
		{0.1777, -0.0398, 0.5156 /*...,*/, 0.9141, -1.5156, 0.6484},
		{0.4844, -0.1206, 1.4844 /*...,*/, 2.4375, -2.7812, 1.2969},
	}

	expectedXkSize := []int{15, 1024}
	expectedXk := [][]float32{
		{0.6289, -0.3066, 1.0391 /*...,*/, 1.4141, -1.7578, 1.1719},
		{4.1562, 1.5469, 1.1406 /*...,*/, -0.9023, 0.6953, -0.2168},
		{7.9062, 3.4375, 0.9727 /*...,*/, -1.6016, 1.4922, -1.0312},
		/*...,*/
		{7.8438, 3.4688, 2.0000 /*...,*/, -1.5234, 1.5859, -0.6641},
		{3.7031, 1.5391, 1.0156 /*...,*/, -0.8164, 0.5898, -0.1943},
		{2.7031, -0.6953, 2.8906 /*...,*/, 0.2695, -0.6680, -2.4062},
	}

	expectedXvSize := []int{15, 1024}
	expectedXv := [][]float32{
		{0.0153, -0.0002, 0.0292 /*...,*/, 0.0008, 0.0005, -0.0001},
		{0.0066, -0.0037, -0.0161 /*...,*/, 0.0085, 0.0020, -0.0015},
		{0.0854, 0.0012, 0.0623 /*...,*/, 0.0508, 0.0562, -0.0718},
		/*...,*/
		{0.0137, -0.0398, 0.0466 /*...,*/, 0.0049, -0.0120, 0.0359},
		{0.0048, -0.0032, -0.0129 /*...,*/, 0.0036, -0.0002, -0.0014},
		{-0.0068, -0.0004, 0.0173 /*...,*/, 0.0177, -0.0025, 0.0003},
	}

	sequenceLength := x.Size[0]

	// lat.attn_wq: [out_features, in_features] -> shape: [4096 4096] -> [N_Heads * HeadDim, Dim]
	actualXq, err := ml.LinearTransformation(x, attention.attn_wq)
	if err != nil {
		t.Fatal(err)
	}

	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedXq, expectedXqSize, actualXq, 4*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	// lat.attn_wk: [out_features, in_features] -> shape: [4096 4096] -> [N_KVHeads * HeadDim, Dim]
	actualXk, err := ml.LinearTransformation(x, attention.attn_wk)
	if err != nil {
		t.Fatal(err)
	}

	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedXk, expectedXkSize, actualXk, 7*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	// lat.attn_wv: [out_features, in_features] -> shape: [4096 4096] -> [N_KVHeads * HeadDim, Dim]
	actualXv, err := ml.LinearTransformation(x, attention.attn_wv)
	if err != nil {
		t.Fatal(err)
	}

	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedXv, expectedXvSize, actualXv, 4*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	/*
		Do reshapings
	*/

	expectedXqRsSize := []int{15, 32, 128}
	expectedXqRs := [][][]float32{
		{
			{-0.0537, -0.1094, 0.3438 /*...,*/, 0.5508, 0.1953, 0.0859},
			{0.1484, 0.0172, 0.3711 /*...,*/, 0.5078, 0.2266, 0.1611},
			{0.7227, -0.2793, 0.8672 /*...,*/, 0.7109, 0.3926, 0.1523},
			/*...,*/
			{0.1826, -0.7695, 1.3750 /*...,*/, 0.6641, -0.0562, 1.3906},
			{-0.1006, -0.4512, -0.1289 /*...,*/, 0.3828, -0.4551, -0.0884},
			{-0.4688, 0.3301, 0.1719 /*...,*/, 0.4941, -0.4922, 0.2441}},
		{
			{0.2100, -0.0306, 0.6016 /*...,*/, 0.5195, 0.3848, 0.2832},
			{0.4199, -0.1226, 0.2598 /*...,*/, 0.6758, 0.7227, 0.7539},
			{1.6172, -0.6016, 1.5391 /*...,*/, 0.6445, 0.9180, 0.7461},
			/*...,*/
			{-0.9922, -1.5312, 0.0198 /*...,*/, 0.5195, -1.0859, -0.2793},
			{1.6406, -0.2852, -0.0491 /*...,*/, 0.4922, -0.5703, 0.1396},
			{-0.0693, -0.3398, 0.0718 /*...,*/, 1.0391, -1.7188, 0.9023},
		},
		{
			{0.1133, -0.1660, 0.3906 /*...,*/, 1.1250, 0.3555, 0.7344},
			{0.0850, 0.0786, 0.0432 /*...,*/, 1.2109, 1.3359, 1.1484},
			{1.6484, -0.6797, 2.5625 /*...,*/, 0.6719, 1.0781, 1.2266},
			/*...,*/
			{-0.3555, -1.7812, 0.4297 /*...,*/, 1.0547, -1.7266, -0.1367},
			{1.9062, 0.4102, 0.0923 /*...,*/, 0.9570, -1.1250, 0.4023},
			{-0.8867, -1.1641, 0.2656 /*...,*/, 1.9453, -3.1250, 1.5938},
		},
		/*...,*/
		{
			{0.4082, -0.1182, 0.7578 /*...,*/, 1.2422, 0.6406, 0.6836},
			{-0.2422, 0.1426, -0.2520 /*...,*/, 1.2891, 1.2812, 1.3906},
			{2.9844, -1.7500, 3.2031 /*...,*/, 1.0078, 1.5000, 1.5703},
			/*...,*/
			{-1.0938, -1.7734, 0.4219 /*...,*/, 0.8711, -1.8672, -0.2910},
			{2.1250, -0.0036, -0.1953 /*...,*/, 0.9492, -1.0078, 0.3379},
			{-0.4082, -0.8438, 0.0527 /*...,*/, 1.8203, -2.8594, 1.6172},
		},
		{
			{0.1777, -0.0398, 0.5156 /*...,*/, 0.4902, 0.3477, 0.2812},
			{0.4883, -0.1279, 0.3281 /*...,*/, 0.6172, 0.6406, 0.6367},
			{1.3828, -0.5547, 1.4297 /*...,*/, 0.6016, 0.8477, 0.6797},
			/*...,*/
			{-0.8047, -1.4062, -0.0312 /*...,*/, 0.5547, -0.9883, -0.2373},
			{1.4375, -0.2988, -0.0864 /*...,*/, 0.4902, -0.5273, 0.1533},
			{-0.0452, -0.3457, 0.0698 /*...,*/, 0.9141, -1.5156, 0.6484},
		},
		{
			{0.4844, -0.1206, 1.4844 /*...,*/, 1.6328, 0.4492, -0.1123},
			{0.1748, -0.0469, 0.1562 /*...,*/, 1.8203, 0.8477, 0.6914},
			{2.8594, -1.1562, 2.7188 /*...,*/, 2.1719, 1.4688, 0.7383},
			/*...,*/
			{0.0178, -3.4062, 3.6250 /*...,*/, 3.1250, -0.8750, 0.7188},
			{3.2656, -1.7656, -1.1797 /*...,*/, 1.1250, -1.1719, 0.3086},
			{0.3867, -0.2832, 0.0767 /*...,*/, 2.4375, -2.7812, 1.2969},
		},
	}

	expectedXkRsSize := []int{15, 8, 128}
	expectedXkRs := [][][]float32{
		{
			{0.6289, -0.3066, 1.0391 /*...,*/, -0.2559, 0.0244, 0.1924},
			{-0.0016, -0.0527, -0.0713 /*...,*/, 0.2949, -0.7422, -0.2393},
			{-0.0447, 0.2266, 0.1416 /*...,*/, 1.4766, -0.7969, -0.5469},
			/*...,*/
			{-0.1807, 0.1177, 0.0457 /*...,*/, -0.6055, 1.0547, 0.3379},
			{-0.1514, -0.0024, -0.2236 /*...,*/, -0.1006, -0.1030, -0.0574},
			{-0.0264, 0.0164, -0.2227 /*...,*/, 1.4141, -1.7578, 1.1719},
		},
		{
			{4.1562, 1.5469, 1.1406 /*...,*/, -0.5391, -0.7148, -0.7383},
			{-0.0771, -1.2578, -1.0312 /*...,*/, -0.8281, 0.2832, -0.0226},
			{-0.0052, 0.7148, -0.1196 /*...,*/, -0.4727, -0.0029, 0.2344},
			/*...,*/
			{-1.4922, -0.3906, 0.8477 /*...,*/, 0.7734, -0.7578, -0.6250},
			{-1.7734, -0.2275, -1.9062 /*...,*/, 0.6641, -1.0938, -0.6836},
			{0.4238, -2.5781, -1.3594 /*...,*/, -0.9023, 0.6953, -0.2168},
		},
		{
			{7.9062, 3.4375, 0.9727 /*...,*/, -0.9258, -1.1953, -0.8242},
			{-0.5078, -3.0625, -2.2969 /*...,*/, -1.0781, 0.6680, -1.1016},
			{0.6289, 0.2246, -0.8125 /*...,*/, -0.6836, 0.1992, 0.5898},
			/*...,*/
			{-3.1562, -1.0078, 1.4844 /*...,*/, 1.2812, -1.3203, -0.2080},
			{-4.1250, 0.8672, -3.7656 /*...,*/, 2.9531, -1.6641, -1.4531},
			{0.6797, -5.5312, -3.5156 /*...,*/, -1.6016, 1.4922, -1.0312},
		},
		/*...,*/
		{
			{7.8438, 3.4688, 2.0000 /*...,*/, -0.9805, -1.1641, -1.2812},
			{-0.4180, -3.2500, -1.7812 /*...,*/, -1.6328, 1.1172, -0.8359},
			{0.0265, 1.2266, -0.2598 /*...,*/, -1.8672, 1.7891, -0.0713},
			/*...,*/
			{-2.8750, -0.1758, 1.0938 /*...,*/, 1.3281, -1.5703, -0.6445},
			{-4.3750, -0.5703, -3.9062 /*...,*/, 1.9297, -3.9219, -1.5781},
			{0.9922, -4.9062, -3.2188 /*...,*/, -1.5234, 1.5859, -0.6641},
		},
		{
			{3.7031, 1.5391, 1.0156 /*...,*/, -0.5195, -0.6641, -0.6367},
			{-0.0518, -1.0781, -1.0078 /*...,*/, -0.7930, 0.4395, 0.0146},
			{-0.0231, 0.5820, -0.0225 /*...,*/, -0.2773, -0.0688, 0.1445},
			/*...,*/
			{-1.4453, -0.4980, 0.8633 /*...,*/, 0.7266, -0.8398, -0.4922},
			{-1.5469, -0.0105, -1.7969 /*...,*/, 0.6445, -1.1250, -0.6641},
			{0.3242, -2.1719, -1.0547 /*...,*/, -0.8164, 0.5898, -0.1943},
		},
		{
			{2.7031, -0.6953, 2.8906 /*...,*/, -0.7734, -0.4609, -0.3809},
			{-0.1406, -0.1396, 0.0233 /*...,*/, -0.6914, 0.1768, -0.1895},
			{-2.3750, 5.9688, 3.5000 /*...,*/, 0.3867, -0.4375, 1.5781},
			/*...,*/
			{-1.5781, 1.1953, 2.2344 /*...,*/, 2.3750, 0.4863, -4.1562},
			{-0.4746, 0.2734, -1.1094 /*...,*/, 0.7109, -1.0547, -0.7031},
			{-0.0237, -0.1592, -1.8984 /*...,*/, 0.2695, -0.6680, -2.4062},
		},
	}

	expectedXvRsSize := []int{15, 8, 128}
	expectedXvRs := [][][]float32{
		{
			{0.0153, -0.0002, 0.0292 /*...,*/, -0.0245, -0.0035, 0.0708},
			{-0.0013, -0.0035, 0.0013 /*...,*/, -0.0015, 0.0011, 0.0006},
			{0.0029, -0.0048, -0.0025 /*...,*/, 0.0006, 0.0028, -0.0040},
			/*...,*/
			{-0.0006, 0.0001, 0.0019 /*...,*/, -0.0008, -0.0001, 0.0011},
			{0.0028, -0.0005, 0.0005 /*...,*/, 0.0015, 0.0053, -0.0017},
			{-0.0008, 0.0004, 0.0008 /*...,*/, 0.0008, 0.0005, -0.0001},
		},
		{
			{0.0066, -0.0037, -0.0161 /*...,*/, 0.0044, 0.0095, -0.0176},
			{-0.0023, 0.0077, -0.0053 /*...,*/, 0.0011, 0.0043, -0.0024},
			{-0.0006, -0.0040, -0.0069 /*...,*/, -0.0136, -0.0160, -0.0089},
			/*...,*/
			{0.0024, 0.0050, 0.0117 /*...,*/, 0.0021, -0.0034, -0.0052},
			{-0.0079, -0.0070, -0.0023 /*...,*/, -0.0092, -0.0074, -0.0124},
			{0.0060, 0.0107, 0.0107 /*...,*/, 0.0085, 0.0020, -0.0015},
		},
		{
			{0.0854, 0.0012, 0.0623 /*...,*/, -0.0160, 0.0019, -0.0933},
			{0.0542, -0.0149, 0.0018 /*...,*/, -0.0265, -0.0544, -0.0742},
			{0.0588, 0.0234, -0.0007 /*...,*/, -0.0371, 0.0044, -0.0293},
			/*...,*/
			{-0.0474, -0.0187, 0.0239 /*...,*/, -0.0320, -0.0104, 0.0249},
			{-0.0125, -0.0024, -0.0210 /*...,*/, -0.0209, -0.0204, -0.0009},
			{-0.0070, -0.0347, 0.0144 /*...,*/, 0.0508, 0.0562, -0.0718}},
		/*...,*/
		{
			{0.0137, -0.0398, 0.0466 /*...,*/, 0.0161, 0.0452, -0.0693},
			{0.0752, -0.0457, 0.0106 /*...,*/, 0.1035, -0.0918, -0.0128},
			{0.0442, 0.0030, -0.0160 /*...,*/, 0.0178, 0.0065, -0.0549},
			/*...,*/
			{-0.0148, -0.0162, -0.0223 /*...,*/, 0.0248, 0.0035, -0.0137},
			{0.0449, -0.0398, -0.0074 /*...,*/, 0.0981, -0.0028, 0.0223},
			{-0.0146, 0.0151, -0.0048 /*...,*/, 0.0049, -0.0120, 0.0359},
		},
		{
			{0.0048, -0.0032, -0.0129 /*...,*/, 0.0085, 0.0017, -0.0273},
			{-0.0040, 0.0041, 0.0033 /*...,*/, 0.0008, 0.0129, -0.0007},
			{-0.0032, -0.0021, -0.0046 /*...,*/, -0.0214, -0.0052, -0.0090},
			/*...,*/
			{0.0082, 0.0018, 0.0076 /*...,*/, 0.0011, -0.0027, -0.0032},
			{-0.0114, 0.0001, -0.0131 /*...,*/, -0.0095, -0.0096, -0.0095},
			{-0.0008, 0.0064, 0.0342 /*...,*/, 0.0036, -0.0002, -0.0014},
		},
		{
			{-0.0068, -0.0004, 0.0173 /*...,*/, -0.0031, 0.0060, 0.1211},
			{0.0069, 0.0089, -0.0106 /*...,*/, 0.0011, -0.0069, 0.0054},
			{0.0194, -0.0184, 0.0029 /*...,*/, 0.0177, 0.0272, 0.0056},
			/*...,*/
			{-0.0004, 0.0127, -0.0079 /*...,*/, -0.0014, 0.0176, 0.0311},
			{-0.0055, -0.0038, -0.0083 /*...,*/, 0.0038, 0.0092, 0.0005},
			{-0.0161, -0.0079, 0.0398 /*...,*/, 0.0177, -0.0025, 0.0003},
		},
	}

	if actualXq, err = actualXq.Reshape([]int{sequenceLength, attention.N_Heads, attention.HeadDim}); err != nil {
		t.Fatal(err)
	}

	if actualXk, err = actualXk.Reshape([]int{sequenceLength, attention.N_KVHeads, attention.HeadDim}); err != nil {
		t.Fatal(err)
	}

	if actualXv, err = actualXv.Reshape([]int{sequenceLength, attention.N_KVHeads, attention.HeadDim}); err != nil {
		t.Fatal(err)
	}

	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedXqRs, expectedXqRsSize, actualXq, 4*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedXkRs, expectedXkRsSize, actualXk, 7*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedXvRs, expectedXvRsSize, actualXv, 4*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	/*
		Apply rotary embeddings
	*/

	expectedXqRotarySize := []int{15, 32, 128}
	expectedXqRotary := [][][]float32{
		{
			{-0.0537, -0.1094, 0.3438 /*...,*/, 0.5508, 0.1953, 0.0859},
			{0.1484, 0.0172, 0.3711 /*...,*/, 0.5078, 0.2266, 0.1611},
			{0.7227, -0.2793, 0.8672 /*...,*/, 0.7109, 0.3926, 0.1523},
			/*...,*/
			{0.1826, -0.7695, 1.3750 /*...,*/, 0.6641, -0.0562, 1.3906},
			{-0.1006, -0.4512, -0.1289 /*...,*/, 0.3828, -0.4551, -0.0884},
			{-0.4688, 0.3301, 0.1719 /*...,*/, 0.4941, -0.4922, 0.2441},
		},
		{
			{0.1396, 0.1602, 0.6328 /*...,*/, 0.5195, 0.3848, 0.2832},
			{0.3301, 0.2871, 0.1406 /*...,*/, 0.6758, 0.7227, 0.7539},
			{1.3828, 1.0391, 1.2891 /*...,*/, 0.6445, 0.9180, 0.7461},
			/*...,*/
			{0.7539, -1.6641, -0.1221 /*...,*/, 0.5195, -1.0859, -0.2793},
			{1.1250, 1.2266, 0.2148 /*...,*/, 0.4922, -0.5703, 0.1396},
			{0.2480, -0.2422, -0.0058 /*...,*/, 1.0391, -1.7188, 0.9023},
		},
		{
			{0.1040, 0.1719, 0.3613 /*...,*/, 1.1250, 0.3555, 0.7344},
			{-0.1069, 0.0444, -0.1221 /*...,*/, 1.2109, 1.3359, 1.1484},
			{-0.0679, 1.7812, 0.6328 /*...,*/, 0.6719, 1.0781, 1.2266},
			/*...,*/
			{1.7656, 0.4180, -0.3438 /*...,*/, 1.0547, -1.7266, -0.1367},
			{-1.1641, 1.5625, 0.2070 /*...,*/, 0.9570, -1.1250, 0.4023},
			{1.4297, -0.3223, -0.3633 /*...,*/, 1.9453, -3.1250, 1.5938},
		},
		/*...,*/
		{
			{0.2812, -0.3184, -0.8789 /*...,*/, 1.2422, 0.6406, 0.6836},
			{-0.1279, 0.2500, 0.2432 /*...,*/, 1.2891, 1.2812, 1.3906},
			{1.5781, -3.0781, -3.2500 /*...,*/, 1.0078, 1.5000, 1.5703},
			/*...,*/
			{-1.8750, -0.9102, -0.3066 /*...,*/, 0.8711, -1.8672, -0.2910},
			{1.7891, -1.1406, 0.0923 /*...,*/, 0.9492, -1.0078, 0.3379},
			{-0.7969, -0.4922, -0.0026 /*...,*/, 1.8203, -2.8594, 1.6172},
		},
		{
			{0.1777, 0.0386, -0.4414 /*...,*/, 0.4902, 0.3477, 0.2812},
			{0.4961, 0.0889, -0.0703 /*...,*/, 0.6172, 0.6406, 0.6367},
			{1.4844, 0.0776, -0.9062 /*...,*/, 0.6016, 0.8477, 0.6797},
			/*...,*/
			{-0.1396, -1.6172, 0.1426 /*...,*/, 0.5547, -0.9883, -0.2373},
			{1.4297, 0.3320, -0.2852 /*...,*/, 0.4902, -0.5273, 0.1533},
			{0.1045, -0.3320, 0.0383 /*...,*/, 0.9141, -1.5156, 0.6484},
		},
		{
			{0.1855, 0.4629, -0.1729 /*...,*/, 1.6328, 0.4492, -0.1123},
			{0.0703, 0.1670, 0.1562 /*...,*/, 1.8203, 0.8477, 0.6914},
			{1.5391, 2.6719, 0.5039 /*0.5859*/ /*...,*/, 2.1719, 1.4688, 0.7383},
			/*...,*/
			{3.3750, -0.4473, 3.0468 /*3.1562*/ /*...,*/, 3.1250, -0.8750, 0.7188},
			{2.2031, 3.0000, -0.3164 /*...,*/, 1.1250, -1.1719, 0.3086},
			{0.3340, 0.3438, 0.6953 /*...,*/, 2.4375, -2.7812, 1.2969},
		},
	}

	expectedXkRotarySize := []int{15, 8, 128}
	expectedXkRotary := [][][]float32{
		{
			{0.6289, -0.3066, 1.0391 /*...,*/, -0.2559, 0.0244, 0.1924},
			{-0.0016, -0.0527, -0.0713 /*...,*/, 0.2949, -0.7422, -0.2393},
			{-0.0447, 0.2266, 0.1416 /*...,*/, 1.4766, -0.7969, -0.5469},
			/*...,*/
			{-0.1807, 0.1177, 0.0457 /*...,*/, -0.6055, 1.0547, 0.3379},
			{-0.1514, -0.0024, -0.2236 /*...,*/, -0.1006, -0.1030, -0.0574},
			{-0.0264, 0.0164, -0.2227 /*...,*/, 1.4141, -1.7578, 1.1719},
		},
		{
			{0.9453, 4.3438, -0.5586 /*...,*/, -0.5391, -0.7148, -0.7383},
			{1.0156, -0.7461, -1.7812 /*...,*/, -0.8281, 0.2832, -0.0226},
			{-0.6055, 0.3809, -0.2451 /*...,*/, -0.4727, -0.0029, 0.2344},
			/*...,*/
			{-0.4766, -1.4688, 0.8789 /*...,*/, 0.7734, -0.7578, -0.6250},
			{-0.7656, -1.6172, 0.0447 /*...,*/, 0.6641, -1.0938, -0.6836},
			{2.3906, -1.0391, -1.0000 /*...,*/, -0.9023, 0.6953, -0.2168}},
		{
			{-6.4062, 5.7500, -3.5938 /*...,*/, -0.9258, -1.1953, -0.8242},
			{3.0000, 0.8125, -3.3594 /*...,*/, -1.0781, 0.6680, -1.1016},
			{-0.4668, 0.4785, -0.3359 /*...,*/, -0.6836, 0.1992, 0.5898},
			/*...,*/
			{2.2344, -2.4531, 0.9570 /*...,*/, 1.2812, -1.3203, -0.2080},
			{0.9297, -4.1250, 3.6875 /*...,*/, 2.9531, -1.6641, -1.4531},
			{4.7500, 2.9219, -0.1221 /*...,*/, -1.6016, 1.4922, -1.0312},
		},
		/*...,*/
		{
			{8.375 /*8.5000*/, -1.2812, -0.9141 /*...,*/, -0.9805, -1.1641, -1.2812},
			{-2.0938, -2.5156, 2.7656 /*...,*/, -1.6328, 1.1172, -0.8359},
			{0.6797, 1.0234, 0.2578 /*...,*/, -1.8672, 1.7891, -0.0713},
			/*...,*/
			{-2.5156, 1.3906, -1.4609 /*...,*/, 1.3281, -1.5703, -0.6445},
			{-4.0000, 1.8672, 2.4844 /*...,*/, 1.9297, -3.9219, -1.5781},
			{-1.7969, -4.6875, 3.0781 /*...,*/, -1.5234, 1.5859, -0.6641}},

		{
			{2.7188, 2.9531, 1.1641 /*...,*/, -0.5195, -0.6641, -0.6367},
			{0.4062, -1.0000, 1.5469 /*...,*/, -0.7930, 0.4395, 0.0146},
			{-0.2656, 0.5195, 0.2461 /*...,*/, -0.2773, -0.0688, 0.1445},
			/*...,*/
			{-1.1016, -1.0625, -0.5938 /*...,*/, 0.7266, -0.8398, -0.4922},
			{-1.3984, -0.6602, -0.7266 /*...,*/, 0.6445, -1.1250, -0.6641},
			{1.2031, -1.8359, 0.4551 /*...,*/, -0.8164, 0.5898, -0.1943},
		},
		{
			{1.0547, 2.5781, 0.7852 /*...,*/, -0.7734, -0.4609, -0.3809},
			{0.1191, -0.1582, 0.5742 /*...,*/, -0.6914, 0.1768, -0.1895},
			{-6.2500, -1.5391, 1.9297 /*...,*/, 0.3867, -0.4375, 1.5781},
			/*...,*/
			{-1.3984, -1.3984, 0.2754 /*...,*/, 2.3750, 0.4863, -4.1562},
			{-0.3359, -0.4336, -0.0435 /*...,*/, 0.7109, -1.0547, -0.7031},
			{0.1543, -0.0452, 0.8359 /*...,*/, 0.2695, -0.6680, -2.4062},
		},
	}

	if actualXq, actualXk, err = applyRotaryEmbeddings(actualXq, actualXk, freqsCis); err != nil {
		return nil
	}

	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedXqRotary, expectedXqRotarySize, actualXq, 7*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedXkRotary, expectedXkRotarySize, actualXk, 12*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	/*
		Update KV cache
	*/

	infContext.CacheK[attention.LayerIndex].SetSlice([]int{startPos}, []int{startPos + sequenceLength}, actualXk)
	infContext.CacheV[attention.LayerIndex].SetSlice([]int{startPos}, []int{startPos + sequenceLength}, actualXv)

	/*
		Retrieve cached KV so far
	*/

	actualKeys, err := infContext.CacheK[attention.LayerIndex].Slice([]int{0}, []int{startPos + sequenceLength})
	if err != nil {
		t.Fatal(err)
	}
	actualValues, err := infContext.CacheV[attention.LayerIndex].Slice([]int{0}, []int{startPos + sequenceLength})
	if err != nil {
		t.Fatal(err)
	}

	/*
		Repeat k/v heads if N_KVHeads < N_Heads
	*/

	expectedKeysRepSize := []int{15, 32, 128}
	expectedKeysRep := [][][]float32{
		{
			{0.6289, -0.3066, 1.0391 /*...,*/, -0.2559, 0.0244, 0.1924},
			{0.6289, -0.3066, 1.0391 /*...,*/, -0.2559, 0.0244, 0.1924},
			{0.6289, -0.3066, 1.0391 /*...,*/, -0.2559, 0.0244, 0.1924},
			/*...,*/
			{-0.0264, 0.0164, -0.2227 /*...,*/, 1.4141, -1.7578, 1.1719},
			{-0.0264, 0.0164, -0.2227 /*...,*/, 1.4141, -1.7578, 1.1719},
			{-0.0264, 0.0164, -0.2227 /*...,*/, 1.4141, -1.7578, 1.1719},
		},
		{
			{0.9453, 4.3438, -0.5586 /*...,*/, -0.5391, -0.7148, -0.7383},
			{0.9453, 4.3438, -0.5586 /*...,*/, -0.5391, -0.7148, -0.7383},
			{0.9453, 4.3438, -0.5586 /*...,*/, -0.5391, -0.7148, -0.7383},
			/*...,*/
			{2.3906, -1.0391, -1.0000 /*...,*/, -0.9023, 0.6953, -0.2168},
			{2.3906, -1.0391, -1.0000 /*...,*/, -0.9023, 0.6953, -0.2168},
			{2.3906, -1.0391, -1.0000 /*...,*/, -0.9023, 0.6953, -0.2168},
		},
		{
			{-6.4062, 5.7500, -3.5938 /*...,*/, -0.9258, -1.1953, -0.8242},
			{-6.4062, 5.7500, -3.5938 /*...,*/, -0.9258, -1.1953, -0.8242},
			{-6.4062, 5.7500, -3.5938 /*...,*/, -0.9258, -1.1953, -0.8242},
			/*...,*/
			{4.7500, 2.9219, -0.1221 /*...,*/, -1.6016, 1.4922, -1.0312},
			{4.7500, 2.9219, -0.1221 /*...,*/, -1.6016, 1.4922, -1.0312},
			{4.7500, 2.9219, -0.1221 /*...,*/, -1.6016, 1.4922, -1.0312},
		},
		/*...,*/
		{
			{8.375 /*8.5000*/, -1.2812, -0.9141 /*...,*/, -0.9805, -1.1641, -1.2812},
			{8.375 /*8.5000*/, -1.2812, -0.9141 /*...,*/, -0.9805, -1.1641, -1.2812},
			{8.375 /*8.5000*/, -1.2812, -0.9141 /*...,*/, -0.9805, -1.1641, -1.2812},
			/*...,*/
			{-1.7969, -4.6875, 3.0781 /*...,*/, -1.5234, 1.5859, -0.6641},
			{-1.7969, -4.6875, 3.0781 /*...,*/, -1.5234, 1.5859, -0.6641},
			{-1.7969, -4.6875, 3.0781 /*...,*/, -1.5234, 1.5859, -0.6641},
		},
		{
			{2.7188, 2.9531, 1.1641 /*...,*/, -0.5195, -0.6641, -0.6367},
			{2.7188, 2.9531, 1.1641 /*...,*/, -0.5195, -0.6641, -0.6367},
			{2.7188, 2.9531, 1.1641 /*...,*/, -0.5195, -0.6641, -0.6367},
			/*...,*/
			{1.2031, -1.8359, 0.4551 /*...,*/, -0.8164, 0.5898, -0.1943},
			{1.2031, -1.8359, 0.4551 /*...,*/, -0.8164, 0.5898, -0.1943},
			{1.2031, -1.8359, 0.4551 /*...,*/, -0.8164, 0.5898, -0.1943},
		},
		{
			{1.0547, 2.5781, 0.7852 /*...,*/, -0.7734, -0.4609, -0.3809},
			{1.0547, 2.5781, 0.7852 /*...,*/, -0.7734, -0.4609, -0.3809},
			{1.0547, 2.5781, 0.7852 /*...,*/, -0.7734, -0.4609, -0.3809},
			/*...,*/
			{0.1543, -0.0452, 0.8359 /*...,*/, 0.2695, -0.6680, -2.4062},
			{0.1543, -0.0452, 0.8359 /*...,*/, 0.2695, -0.6680, -2.4062},
			{0.1543, -0.0452, 0.8359 /*...,*/, 0.2695, -0.6680, -2.4062},
		},
	}

	expectedValuesRepSize := []int{15, 32, 128}
	expectedValuesRep := [][][]float32{
		{
			{0.0153, -0.0002, 0.0292 /*...,*/, -0.0245, -0.0035, 0.0708},
			{0.0153, -0.0002, 0.0292 /*...,*/, -0.0245, -0.0035, 0.0708},
			{0.0153, -0.0002, 0.0292 /*...,*/, -0.0245, -0.0035, 0.0708},
			/*...,*/
			{-0.0008, 0.0004, 0.0008 /*...,*/, 0.0008, 0.0005, -0.0001},
			{-0.0008, 0.0004, 0.0008 /*...,*/, 0.0008, 0.0005, -0.0001},
			{-0.0008, 0.0004, 0.0008 /*...,*/, 0.0008, 0.0005, -0.0001},
		},
		{
			{0.0066, -0.0037, -0.0161 /*...,*/, 0.0044, 0.0095, -0.0176},
			{0.0066, -0.0037, -0.0161 /*...,*/, 0.0044, 0.0095, -0.0176},
			{0.0066, -0.0037, -0.0161 /*...,*/, 0.0044, 0.0095, -0.0176},
			/*...,*/
			{0.0060, 0.0107, 0.0107 /*...,*/, 0.0085, 0.0020, -0.0015},
			{0.0060, 0.0107, 0.0107 /*...,*/, 0.0085, 0.0020, -0.0015},
			{0.0060, 0.0107, 0.0107 /*...,*/, 0.0085, 0.0020, -0.0015},
		},
		{
			{0.0854, 0.0012, 0.0623 /*...,*/, -0.0160, 0.0019, -0.0933},
			{0.0854, 0.0012, 0.0623 /*...,*/, -0.0160, 0.0019, -0.0933},
			{0.0854, 0.0012, 0.0623 /*...,*/, -0.0160, 0.0019, -0.0933},
			/*...,*/
			{-0.0070, -0.0347, 0.0144 /*...,*/, 0.0508, 0.0562, -0.0718},
			{-0.0070, -0.0347, 0.0144 /*...,*/, 0.0508, 0.0562, -0.0718},
			{-0.0070, -0.0347, 0.0144 /*...,*/, 0.0508, 0.0562, -0.0718},
		},
		/*...,*/
		{
			{0.0137, -0.0398, 0.0466 /*...,*/, 0.0161, 0.0452, -0.0693},
			{0.0137, -0.0398, 0.0466 /*...,*/, 0.0161, 0.0452, -0.0693},
			{0.0137, -0.0398, 0.0466 /*...,*/, 0.0161, 0.0452, -0.0693},
			/*...,*/
			{-0.0146, 0.0151, -0.0048 /*...,*/, 0.0049, -0.0120, 0.0359},
			{-0.0146, 0.0151, -0.0048 /*...,*/, 0.0049, -0.0120, 0.0359},
			{-0.0146, 0.0151, -0.0048 /*...,*/, 0.0049, -0.0120, 0.0359},
		},
		{
			{0.0048, -0.0032, -0.0129 /*...,*/, 0.0085, 0.0017, -0.0273},
			{0.0048, -0.0032, -0.0129 /*...,*/, 0.0085, 0.0017, -0.0273},
			{0.0048, -0.0032, -0.0129 /*...,*/, 0.0085, 0.0017, -0.0273},
			/*...,*/
			{-0.0008, 0.0064, 0.0342 /*...,*/, 0.0036, -0.0002, -0.0014},
			{-0.0008, 0.0064, 0.0342 /*...,*/, 0.0036, -0.0002, -0.0014},
			{-0.0008, 0.0064, 0.0342 /*...,*/, 0.0036, -0.0002, -0.0014},
		},
		{
			{-0.0068, -0.0004, 0.0173 /*...,*/, -0.0031, 0.0060, 0.1211},
			{-0.0068, -0.0004, 0.0173 /*...,*/, -0.0031, 0.0060, 0.1211},
			{-0.0068, -0.0004, 0.0173 /*...,*/, -0.0031, 0.0060, 0.1211},
			/*...,*/
			{-0.0161, -0.0079, 0.0398 /*...,*/, 0.0177, -0.0025, 0.0003},
			{-0.0161, -0.0079, 0.0398 /*...,*/, 0.0177, -0.0025, 0.0003},
			{-0.0161, -0.0079, 0.0398 /*...,*/, 0.0177, -0.0025, 0.0003},
		},
	}

	N_Rep := attention.N_Rep

	if actualKeys, err = attentionRepeatKV(actualKeys, N_Rep); err != nil { // shape=[5, 32, 128] (cacheLen + sequenceLength, N_Heads, HeadDim)
		t.Fatal(err)
	}
	if actualValues, err = attentionRepeatKV(actualValues, N_Rep); err != nil { // shape=[5, 32, 128] (cacheLen + sequenceLength, N_Heads, HeadDim)
		t.Fatal(err)
	}

	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedKeysRep, expectedKeysRepSize, actualKeys, 12*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedValuesRep, expectedValuesRepSize, actualValues, 7*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	/*
		Do transposes
	*/

	expectedXqTransposeSize := []int{32, 15, 128}
	expectedXqTranspose := [][][]float32{
		{
			{-0.0537, -0.1094, 0.3438 /*...,*/, 0.5508, 0.1953, 0.0859},
			{0.1396, 0.1602, 0.6328 /*...,*/, 0.5195, 0.3848, 0.2832},
			{0.1040, 0.1719, 0.3613 /*...,*/, 1.1250, 0.3555, 0.7344},
			/*...,*/
			{0.2812, -0.3184, -0.8789 /*...,*/, 1.2422, 0.6406, 0.6836},
			{0.1777, 0.0386, -0.4414 /*...,*/, 0.4902, 0.3477, 0.2812},
			{0.1855, 0.4629, -0.1729 /*...,*/, 1.6328, 0.4492, -0.1123},
		},
		{
			{0.1484, 0.0172, 0.3711 /*...,*/, 0.5078, 0.2266, 0.1611},
			{0.3301, 0.2871, 0.1406 /*...,*/, 0.6758, 0.7227, 0.7539},
			{-0.1069, 0.0444, -0.1221 /*...,*/, 1.2109, 1.3359, 1.1484},
			/*...,*/
			{-0.1279, 0.2500, 0.2432 /*...,*/, 1.2891, 1.2812, 1.3906},
			{0.4961, 0.0889, -0.0703 /*...,*/, 0.6172, 0.6406, 0.6367},
			{0.0703, 0.1670, 0.1562 /*...,*/, 1.8203, 0.8477, 0.6914},
		},
		{
			{0.7227, -0.2793, 0.8672 /*...,*/, 0.7109, 0.3926, 0.1523},
			{1.3828, 1.0391, 1.2891 /*...,*/, 0.6445, 0.9180, 0.7461},
			{-0.0679, 1.7812, 0.6328 /*...,*/, 0.6719, 1.0781, 1.2266},
			/*...,*/
			{1.5781, -3.0781, -3.2500 /*...,*/, 1.0078, 1.5000, 1.5703},
			{1.4844, 0.0776, -0.9062 /*...,*/, 0.6016, 0.8477, 0.6797},
			{1.5391, 2.6719, 0.5039 /*0.5859*/ /*...,*/, 2.1719, 1.4688, 0.7383},
		},
		/*...,*/
		{
			{0.1826, -0.7695, 1.3750 /*...,*/, 0.6641, -0.0562, 1.3906},
			{0.7539, -1.6641, -0.1221 /*...,*/, 0.5195, -1.0859, -0.2793},
			{1.7656, 0.4180, -0.3438 /*...,*/, 1.0547, -1.7266, -0.1367},
			/*...,*/
			{-1.8750, -0.9102, -0.3066 /*...,*/, 0.8711, -1.8672, -0.2910},
			{-0.1396, -1.6172, 0.1426 /*...,*/, 0.5547, -0.9883, -0.2373},
			{3.3750, -0.4473, 3.0468 /*3.1562*/ /*...,*/, 3.1250, -0.8750, 0.7188},
		},
		{
			{-0.1006, -0.4512, -0.1289 /*...,*/, 0.3828, -0.4551, -0.0884},
			{1.1250, 1.2266, 0.2148 /*...,*/, 0.4922, -0.5703, 0.1396},
			{-1.1641, 1.5625, 0.2070 /*...,*/, 0.9570, -1.1250, 0.4023},
			/*...,*/
			{1.7891, -1.1406, 0.0923 /*...,*/, 0.9492, -1.0078, 0.3379},
			{1.4297, 0.3320, -0.2852 /*...,*/, 0.4902, -0.5273, 0.1533},
			{2.2031, 3.0000, -0.3164 /*...,*/, 1.1250, -1.1719, 0.3086},
		},
		{
			{-0.4688, 0.3301, 0.1719 /*...,*/, 0.4941, -0.4922, 0.2441},
			{0.2480, -0.2422, -0.0058 /*...,*/, 1.0391, -1.7188, 0.9023},
			{1.4297, -0.3223, -0.3633 /*...,*/, 1.9453, -3.1250, 1.5938},
			/*...,*/
			{-0.7969, -0.4922, -0.0026 /*...,*/, 1.8203, -2.8594, 1.6172},
			{0.1045, -0.3320, 0.0383 /*...,*/, 0.9141, -1.5156, 0.6484},
			{0.3340, 0.3438, 0.6953 /*...,*/, 2.4375, -2.7812, 1.2969},
		},
	}

	if actualXq, err = actualXq.Transpose(0, 1); err != nil { // from [15, 32, 128] -> shape=[32, 15, 128] (N_Heads, sequenceLength, HeadDim)
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedXqTranspose, expectedXqTransposeSize, actualXq, 7*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	expectedKeysTransposeDims0_1_Size := []int{32, 15, 128}
	expectedKeysTransposeDims0_1 := [][][]float32{
		{
			{0.6289, -0.3066, 1.0391 /*...,*/, -0.2559, 0.0244, 0.1924},
			{0.9453, 4.3438, -0.5586 /*...,*/, -0.5391, -0.7148, -0.7383},
			{-6.4062, 5.7500, -3.5938 /*...,*/, -0.9258, -1.1953, -0.8242},
			/*...,*/
			{8.375 /*8.5000*/, -1.2812, -0.9141 /*...,*/, -0.9805, -1.1641, -1.2812},
			{2.7188, 2.9531, 1.1641 /*...,*/, -0.5195, -0.6641, -0.6367},
			{1.0547, 2.5781, 0.7852 /*...,*/, -0.7734, -0.4609, -0.3809},
		},
		{
			{0.6289, -0.3066, 1.0391 /*...,*/, -0.2559, 0.0244, 0.1924},
			{0.9453, 4.3438, -0.5586 /*...,*/, -0.5391, -0.7148, -0.7383},
			{-6.4062, 5.7500, -3.5938 /*...,*/, -0.9258, -1.1953, -0.8242},
			/*...,*/
			{8.375 /*8.5000*/, -1.2812, -0.9141 /*...,*/, -0.9805, -1.1641, -1.2812},
			{2.7188, 2.9531, 1.1641 /*...,*/, -0.5195, -0.6641, -0.6367},
			{1.0547, 2.5781, 0.7852 /*...,*/, -0.7734, -0.4609, -0.3809},
		},
		{
			{0.6289, -0.3066, 1.0391 /*...,*/, -0.2559, 0.0244, 0.1924},
			{0.9453, 4.3438, -0.5586 /*...,*/, -0.5391, -0.7148, -0.7383},
			{-6.4062, 5.7500, -3.5938 /*...,*/, -0.9258, -1.1953, -0.8242},
			/*...,*/
			{8.375 /*8.5000*/, -1.2812, -0.9141 /*...,*/, -0.9805, -1.1641, -1.2812},
			{2.7188, 2.9531, 1.1641 /*...,*/, -0.5195, -0.6641, -0.6367},
			{1.0547, 2.5781, 0.7852 /*...,*/, -0.7734, -0.4609, -0.3809},
		},
		/*...,*/
		{
			{-0.0264, 0.0164, -0.2227 /*...,*/, 1.4141, -1.7578, 1.1719},
			{2.3906, -1.0391, -1.0000 /*...,*/, -0.9023, 0.6953, -0.2168},
			{4.7500, 2.9219, -0.1221 /*...,*/, -1.6016, 1.4922, -1.0312},
			/*...,*/
			{-1.7969, -4.6875, 3.0781 /*...,*/, -1.5234, 1.5859, -0.6641},
			{1.2031, -1.8359, 0.4551 /*...,*/, -0.8164, 0.5898, -0.1943},
			{0.1543, -0.0452, 0.8359 /*...,*/, 0.2695, -0.6680, -2.4062},
		},
		{
			{-0.0264, 0.0164, -0.2227 /*...,*/, 1.4141, -1.7578, 1.1719},
			{2.3906, -1.0391, -1.0000 /*...,*/, -0.9023, 0.6953, -0.2168},
			{4.7500, 2.9219, -0.1221 /*...,*/, -1.6016, 1.4922, -1.0312},
			/*...,*/
			{-1.7969, -4.6875, 3.0781 /*...,*/, -1.5234, 1.5859, -0.6641},
			{1.2031, -1.8359, 0.4551 /*...,*/, -0.8164, 0.5898, -0.1943},
			{0.1543, -0.0452, 0.8359 /*...,*/, 0.2695, -0.6680, -2.4062},
		},
		{
			{-0.0264, 0.0164, -0.2227 /*...,*/, 1.4141, -1.7578, 1.1719},
			{2.3906, -1.0391, -1.0000 /*...,*/, -0.9023, 0.6953, -0.2168},
			{4.7500, 2.9219, -0.1221 /*...,*/, -1.6016, 1.4922, -1.0312},
			/*...,*/
			{-1.7969, -4.6875, 3.0781 /*...,*/, -1.5234, 1.5859, -0.6641},
			{1.2031, -1.8359, 0.4551 /*...,*/, -0.8164, 0.5898, -0.1943},
			{0.1543, -0.0452, 0.8359 /*...,*/, 0.2695, -0.6680, -2.4062},
		},
	}

	if actualKeys, err = actualKeys.Transpose(0, 1); err != nil { // from [5, 32, 128] -> shape=[32, 5, 128] (N_Heads, sequenceLength, HeadDim)
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedKeysTransposeDims0_1, expectedKeysTransposeDims0_1_Size, actualKeys, 12*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	expectedValuesTransposeSize := []int{32, 15, 128}
	expectedValuesTranspose := [][][]float32{
		{
			{0.0153, -0.0002, 0.0292 /*...,*/, -0.0245, -0.0035, 0.0708},
			{0.0066, -0.0037, -0.0161 /*...,*/, 0.0044, 0.0095, -0.0176},
			{0.0854, 0.0012, 0.0623 /*...,*/, -0.0160, 0.0019, -0.0933},
			/*...,*/
			{0.0137, -0.0398, 0.0466 /*...,*/, 0.0161, 0.0452, -0.0693},
			{0.0048, -0.0032, -0.0129 /*...,*/, 0.0085, 0.0017, -0.0273},
			{-0.0068, -0.0004, 0.0173 /*...,*/, -0.0031, 0.0060, 0.1211},
		},
		{
			{0.0153, -0.0002, 0.0292 /*...,*/, -0.0245, -0.0035, 0.0708},
			{0.0066, -0.0037, -0.0161 /*...,*/, 0.0044, 0.0095, -0.0176},
			{0.0854, 0.0012, 0.0623 /*...,*/, -0.0160, 0.0019, -0.0933},
			/*...,*/
			{0.0137, -0.0398, 0.0466 /*...,*/, 0.0161, 0.0452, -0.0693},
			{0.0048, -0.0032, -0.0129 /*...,*/, 0.0085, 0.0017, -0.0273},
			{-0.0068, -0.0004, 0.0173 /*...,*/, -0.0031, 0.0060, 0.1211},
		},
		{
			{0.0153, -0.0002, 0.0292 /*...,*/, -0.0245, -0.0035, 0.0708},
			{0.0066, -0.0037, -0.0161 /*...,*/, 0.0044, 0.0095, -0.0176},
			{0.0854, 0.0012, 0.0623 /*...,*/, -0.0160, 0.0019, -0.0933},
			/*...,*/
			{0.0137, -0.0398, 0.0466 /*...,*/, 0.0161, 0.0452, -0.0693},
			{0.0048, -0.0032, -0.0129 /*...,*/, 0.0085, 0.0017, -0.0273},
			{-0.0068, -0.0004, 0.0173 /*...,*/, -0.0031, 0.0060, 0.1211},
		},
		/*...,*/
		{
			{-0.0008, 0.0004, 0.0008 /*...,*/, 0.0008, 0.0005, -0.0001},
			{0.0060, 0.0107, 0.0107 /*...,*/, 0.0085, 0.0020, -0.0015},
			{-0.0070, -0.0347, 0.0144 /*...,*/, 0.0508, 0.0562, -0.0718},
			/*...,*/
			{-0.0146, 0.0151, -0.0048 /*...,*/, 0.0049, -0.0120, 0.0359},
			{-0.0008, 0.0064, 0.0342 /*...,*/, 0.0036, -0.0002, -0.0014},
			{-0.0161, -0.0079, 0.0398 /*...,*/, 0.0177, -0.0025, 0.0003},
		},
		{
			{-0.0008, 0.0004, 0.0008 /*...,*/, 0.0008, 0.0005, -0.0001},
			{0.0060, 0.0107, 0.0107 /*...,*/, 0.0085, 0.0020, -0.0015},
			{-0.0070, -0.0347, 0.0144 /*...,*/, 0.0508, 0.0562, -0.0718},
			/*...,*/
			{-0.0146, 0.0151, -0.0048 /*...,*/, 0.0049, -0.0120, 0.0359},
			{-0.0008, 0.0064, 0.0342 /*...,*/, 0.0036, -0.0002, -0.0014},
			{-0.0161, -0.0079, 0.0398 /*...,*/, 0.0177, -0.0025, 0.0003},
		},
		{
			{-0.0008, 0.0004, 0.0008 /*...,*/, 0.0008, 0.0005, -0.0001},
			{0.0060, 0.0107, 0.0107 /*...,*/, 0.0085, 0.0020, -0.0015},
			{-0.0070, -0.0347, 0.0144 /*...,*/, 0.0508, 0.0562, -0.0718},
			/*...,*/
			{-0.0146, 0.0151, -0.0048 /*...,*/, 0.0049, -0.0120, 0.0359},
			{-0.0008, 0.0064, 0.0342 /*...,*/, 0.0036, -0.0002, -0.0014},
			{-0.0161, -0.0079, 0.0398 /*...,*/, 0.0177, -0.0025, 0.0003},
		},
	}

	if actualValues, err = actualValues.Transpose(0, 1); err != nil { // from [15, 32, 128] -> shape=[32, 15, 128] (N_Heads, sequenceLength, HeadDim)
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedValuesTranspose, expectedValuesTransposeSize, actualValues, common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	expectedKeysTransposeDims1_2_Size := []int{32, 128, 15}
	expectedKeysTransposeDims1_2 := [][][]float32{
		{
			{0.6289, 0.9453, -6.4062 /*...,*/, 8.375 /*8.5000*/, 2.7188, 1.0547},
			{-0.3066, 4.3438, 5.7500 /*...,*/, -1.2812, 2.9531, 2.5781},
			{1.0391, -0.5586, -3.5938 /*...,*/, -0.9141, 1.1641, 0.7852},
			/*...,*/
			{-0.2559, -0.5391, -0.9258 /*...,*/, -0.9805, -0.5195, -0.7734},
			{0.0244, -0.7148, -1.1953 /*...,*/, -1.1641, -0.6641, -0.4609},
			{0.1924, -0.7383, -0.8242 /*...,*/, -1.2812, -0.6367, -0.3809},
		},
		{
			{0.6289, 0.9453, -6.4062 /*...,*/, 8.375 /*8.5000*/, 2.7188, 1.0547},
			{-0.3066, 4.3438, 5.7500 /*...,*/, -1.2812, 2.9531, 2.5781},
			{1.0391, -0.5586, -3.5938 /*...,*/, -0.9141, 1.1641, 0.7852},
			/*...,*/
			{-0.2559, -0.5391, -0.9258 /*...,*/, -0.9805, -0.5195, -0.7734},
			{0.0244, -0.7148, -1.1953 /*...,*/, -1.1641, -0.6641, -0.4609},
			{0.1924, -0.7383, -0.8242 /*...,*/, -1.2812, -0.6367, -0.3809},
		},
		{
			{0.6289, 0.9453, -6.4062 /*...,*/, 8.375 /*8.5000*/, 2.7188, 1.0547},
			{-0.3066, 4.3438, 5.7500 /*...,*/, -1.2812, 2.9531, 2.5781},
			{1.0391, -0.5586, -3.5938 /*...,*/, -0.9141, 1.1641, 0.7852},
			/*...,*/
			{-0.2559, -0.5391, -0.9258 /*...,*/, -0.9805, -0.5195, -0.7734},
			{0.0244, -0.7148, -1.1953 /*...,*/, -1.1641, -0.6641, -0.4609},
			{0.1924, -0.7383, -0.8242 /*...,*/, -1.2812, -0.6367, -0.3809},
		},
		/*...,*/
		{
			{-0.0264, 2.3906, 4.7500 /*...,*/, -1.7969, 1.2031, 0.1543},
			{0.0164, -1.0391, 2.9219 /*...,*/, -4.6875, -1.8359, -0.0452},
			{-0.2227, -1.0000, -0.1221 /*...,*/, 3.0781, 0.4551, 0.8359},
			/*...,*/
			{1.4141, -0.9023, -1.6016 /*...,*/, -1.5234, -0.8164, 0.2695},
			{-1.7578, 0.6953, 1.4922 /*...,*/, 1.5859, 0.5898, -0.6680},
			{1.1719, -0.2168, -1.0312 /*...,*/, -0.6641, -0.1943, -2.4062},
		},
		{
			{-0.0264, 2.3906, 4.7500 /*...,*/, -1.7969, 1.2031, 0.1543},
			{0.0164, -1.0391, 2.9219 /*...,*/, -4.6875, -1.8359, -0.0452},
			{-0.2227, -1.0000, -0.1221 /*...,*/, 3.0781, 0.4551, 0.8359},
			/*...,*/
			{1.4141, -0.9023, -1.6016 /*...,*/, -1.5234, -0.8164, 0.2695},
			{-1.7578, 0.6953, 1.4922 /*...,*/, 1.5859, 0.5898, -0.6680},
			{1.1719, -0.2168, -1.0312 /*...,*/, -0.6641, -0.1943, -2.4062},
		},
		{
			{-0.0264, 2.3906, 4.7500 /*...,*/, -1.7969, 1.2031, 0.1543},
			{0.0164, -1.0391, 2.9219 /*...,*/, -4.6875, -1.8359, -0.0452},
			{-0.2227, -1.0000, -0.1221 /*...,*/, 3.0781, 0.4551, 0.8359},
			/*...,*/
			{1.4141, -0.9023, -1.6016 /*...,*/, -1.5234, -0.8164, 0.2695},
			{-1.7578, 0.6953, 1.4922 /*...,*/, 1.5859, 0.5898, -0.6680},
			{1.1719, -0.2168, -1.0312 /*...,*/, -0.6641, -0.1943, -2.4062},
		},
	}

	if actualKeys, err = actualKeys.Transpose(1, 2); err != nil { // from [32, 15, 128] -> shape=[32, 128, 15] (N_Heads, HeadDim, sequenceLength)
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedKeysTransposeDims1_2, expectedKeysTransposeDims1_2_Size, actualKeys, 12*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	/*
		Goal in Python manner:
		scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)
	*/

	expectedScoresSize := []int{32, 15, 15}
	expectedScores := [][][]float32{
		{
			{1.0781, -0.4004, -1.1406 /*...,*/, -0.9961, -0.4512, -0.0559},
			{1.5391, -0.0364, -0.9062 /*...,*/, -0.7422, -0.3652, -0.0337},
			{2.7969, 0.7969, 0.9766 /*...,*/, 0.4473, 0.0967, 0.9102},
			/*...,*/
			{1.2344, -0.6953, -1.0391 /*...,*/, 0.8828, -0.3516, 0.5117},
			{0.4727, -0.3809, -0.6680 /*...,*/, 0.5703, -0.0015, 0.5898},
			{-0.4727, -1.0078, -1.4844 /*...,*/, 0.9180, -0.1543, 1.1094},
		},
		{
			{0.8516, 0.2695, -0.2305 /*...,*/, -0.0593, -0.0549, 0.0376},
			{1.7891, -0.0255, -0.5117 /*...,*/, -0.3652, -0.2910, 0.6875},
			{2.6875, -0.7891, -1.2188 /*...,*/, -0.9375, -0.7734, 0.9336},
			/*...,*/
			{2.8438, -1.2656, -2.2500 /*...,*/, -0.8281, -0.4258, 1.2500},
			{1.0781, -0.7578, -1.5781 /*...,*/, 0.3418, -0.0176, 1.0469},
			{2.8438, -0.3945, -0.7969 /*...,*/, 0.2129, 0.2236, 0.8828},
		},
		{
			{0.6133, -0.4648, -1.8750 /*...,*/, -0.9336, -0.5430, -0.9102},
			{0.8164, -0.0444, -2.2500 /*...,*/, -1.8984, -1.0312, -1.6250},
			{0.5156, 0.6289, -0.1484 /*...,*/, -4.4062, -2.0156, -2.8750},
			/*...,*/
			{-1.7656, -5.0938, -8.1250 /*...,*/, 0.3105, -2.7344, -1.2109},
			{-0.8555, -1.9219, -3.7500 /*...,*/, 1.1484, -0.0962, 0.9883},
			{-1.7656, -3.3438, -6.3125 /*...,*/, -0.8789, 0.2109, 1.1484},
		},
		/*...,*/
		{
			{1.8359, -0.2178, -0.6367 /*...,*/, -0.8047, -0.6094, -1.8125},
			{4.0938, -0.9180, -2.5469 /*...,*/, -2.6562, -1.1797, 1.5781},
			{7.2188, -1.4375, -2.7031 /*...,*/, -5.5312, -2.1094, 2.0312},
			/*...,*/
			{7.5000, -3.0469, -6.9375 /*...,*/, -2.7188, -1.9531, 1.5938},
			{4.1250, -1.4922, -3.8281 /*...,*/, -1.0078, -0.7578, 1.1953},
			{7.9688, -3.7188, -4.9062 /*...,*/, -3.2969, -1.2422, -2.9375},
		},
		{
			{2.2969, -0.8789, -2.4375 /*...,*/, -1.4141, -0.6758, 0.4883},
			{2.3906, 0.1416, 1.3438 /*...,*/, -1.9531, -0.6836, -0.0781},
			{5.0938, -0.7852, -0.0264 /*...,*/, -3.4688, -1.8359, -0.1660},
			/*...,*/
			{5.2500, -1.7266, -4.1875 /*...,*/, 0.0674, 0.4414, 0.7773},
			{2.5938, -0.8398, -1.6875 /*...,*/, -1.0078, 0.0884, 0.5898},
			{6.1875, -2.7344, -4.1875 /*...,*/, -5.2812, -1.2188, 2.7031},
		},
		{
			{4.6562, -2.2500, -5.6875 /*...,*/, -3.5625, -1.5000, 1.3281},
			{9.0625, -2.4688, -9.1875 /*...,*/, -7.2188, -2.1250, 1.5703},
			{15.625 /*16.1250*/, -3.8906, -15.0625 /*-15.4375*/ /*...,*/, -12.875 /*-13.1875*/, -3.6719, 2.4375},
			/*...,*/
			{15.5625, -4.8750, -16.7500 /*...,*/, -11.5000, -3.4844, 2.2656},
			{8.0000, -2.4062, -8.4375 /*...,*/, -6.0625, -1.6797, 1.5547},
			{20.8750, -8.0000, -22.3750 /*...,*/, -17.8750, -6.1562, 7.2500},
		},
	}

	xqMatMulKeys, err := ml.MatMul(actualXq, actualKeys) // matmul([32,15,128], [32,128,15]) -> shape=[32,15,15] (N_Heads, sequenceLength, sequenceLength)
	if err != nil {
		t.Fatal(err)
	}
	actualScores, err := ml.DivToScalar(xqMatMulKeys, dtype.BFloat16fromFloat32(float32(math.Sqrt(float64(attention.HeadDim))))) // shape=[32,15,15]
	if err != nil {
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedScores, expectedScoresSize, actualScores, 70*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}
	negInf := float32(math.Inf(-1))

	if mask != nil {
		expectedScoresPlusMaskSize := []int{32, 15, 15}
		expectedScoresPlusMask := [][][]float32{
			{
				{1.0781, negInf, negInf /*...,*/, negInf, negInf, negInf},
				{1.5391, -0.0364, negInf /*...,*/, negInf, negInf, negInf},
				{2.7969, 0.7969, 0.9766 /*...,*/, negInf, negInf, negInf},
				/*...,*/
				{1.2344, -0.6953, -1.0391 /*...,*/, 0.8828, negInf, negInf},
				{0.4727, -0.3809, -0.6680 /*...,*/, 0.5703, -0.0015, negInf},
				{-0.4727, -1.0078, -1.4844 /*...,*/, 0.9180, -0.1543, 1.1094},
			},
			{
				{0.8516, negInf, negInf /*...,*/, negInf, negInf, negInf},
				{1.7891, -0.0255, negInf /*...,*/, negInf, negInf, negInf},
				{2.6875, -0.7891, -1.2188 /*...,*/, negInf, negInf, negInf},
				/*...,*/
				{2.8438, -1.2656, -2.2500 /*...,*/, -0.8281, negInf, negInf},
				{1.0781, -0.7578, -1.5781 /*...,*/, 0.3418, -0.0176, negInf},
				{2.8438, -0.3945, -0.7969 /*...,*/, 0.2129, 0.2236, 0.8828},
			},
			{
				{0.6133, negInf, negInf /*...,*/, negInf, negInf, negInf},
				{0.8164, -0.0444, negInf /*...,*/, negInf, negInf, negInf},
				{0.5156, 0.6289, -0.1484 /*...,*/, negInf, negInf, negInf},
				/*...,*/
				{-1.7656, -5.0938, -8.1250 /*...,*/, 0.3105, negInf, negInf},
				{-0.8555, -1.9219, -3.7500 /*...,*/, 1.1484, -0.0962, negInf},
				{-1.7656, -3.3438, -6.3125 /*...,*/, -0.8789, 0.2109, 1.1484},
			},
			/*...,*/
			{
				{1.8359, negInf, negInf /*...,*/, negInf, negInf, negInf},
				{4.0938, -0.9180, negInf /*...,*/, negInf, negInf, negInf},
				{7.2188, -1.4375, -2.7031 /*...,*/, negInf, negInf, negInf},
				/*...,*/
				{7.5000, -3.0469, -6.9375 /*...,*/, -2.7188, negInf, negInf},
				{4.1250, -1.4922, -3.8281 /*...,*/, -1.0078, -0.7578, negInf},
				{7.9688, -3.7188, -4.9062 /*...,*/, -3.2969, -1.2422, -2.9375},
			},
			{
				{2.2969, negInf, negInf /*...,*/, negInf, negInf, negInf},
				{2.3906, 0.1416, negInf /*...,*/, negInf, negInf, negInf},
				{5.0938, -0.7852, -0.0264 /*...,*/, negInf, negInf, negInf},
				/*...,*/
				{5.2500, -1.7266, -4.1875 /*...,*/, 0.0674, negInf, negInf},
				{2.5938, -0.8398, -1.6875 /*...,*/, -1.0078, 0.0884, negInf},
				{6.1875, -2.7344, -4.1875 /*...,*/, -5.2812, -1.2188, 2.7031},
			},
			{
				{4.6562, negInf, negInf /*...,*/, negInf, negInf, negInf},
				{9.0625, -2.4688, negInf /*...,*/, negInf, negInf, negInf},
				{16.1250, -3.8906, -15.4375 /*...,*/, negInf, negInf, negInf},
				/*...,*/
				{15.5625, -4.8750, -16.7500 /*...,*/, -11.5000, negInf, negInf},
				{8.0000, -2.4062, -8.4375 /*...,*/, -6.0625, -1.6797, negInf},
				{20.8750, -8.0000, -22.3750 /*...,*/, -17.8750, -6.1562, 7.2500},
			},
		}
		if actualScores, err = ml.Add(actualScores, mask); err != nil { // shape=[32,5,5]
			t.Fatal(err)
		}
		if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedScoresPlusMask, expectedScoresPlusMaskSize, actualScores, 70*common.THRESHOLD_BF16, true); err != nil {
			t.Fatal(err)
		}
	}

	/*
		Goal in Python manner:
		scores = F.softmax(scores.float(), dim=-1).type_as(xq)
	*/

	expectedScoresSoftmaxSize := []int{32, 15, 15}
	expectedScoresSoftmax := [][][]float32{
		{
			{1.0000, 0.0000, 0.0000 /*...,*/, 0.0000, 0.0000, 0.0000},
			{0.8281, 0.1719, 0.0000 /*...,*/, 0.0000, 0.0000, 0.0000},
			{0.7695, 0.1045, 0.1250 /*...,*/, 0.0000, 0.0000, 0.0000},
			/*...,*/
			{0.1523, 0.0221, 0.0156 /*...,*/, 0.1069, 0.0000, 0.0000},
			{0.1025, 0.0437, 0.0327 /*...,*/, 0.1133, 0.0640, 0.0000},
			{0.0369, 0.0216, 0.0134 /*...,*/, 0.1484, 0.0508, 0.1797},
		},
		{
			{1.0000, 0.0000, 0.0000 /*...,*/, 0.0000, 0.0000, 0.0000},
			{0.8594, 0.1396, 0.0000 /*...,*/, 0.0000, 0.0000, 0.0000},
			{0.9531, 0.0294, 0.0192 /*...,*/, 0.0000, 0.0000, 0.0000},
			/*...,*/
			{0.6836, 0.0112, 0.0042 /*...,*/, 0.0175, 0.0000, 0.0000},
			{0.2559, 0.0410, 0.0181 /*...,*/, 0.1230, 0.0859, 0.0000},
			{0.4570, 0.0179, 0.0120 /*...,*/, 0.0330, 0.0332, 0.0645},
		},
		{
			{1.0000, 0.0000, 0.0000 /*...,*/, 0.0000, 0.0000, 0.0000},
			{0.7031, 0.2969, 0.0000 /*...,*/, 0.0000, 0.0000, 0.0000},
			{0.3789, 0.4258, 0.1953 /*...,*/, 0.0000, 0.0000, 0.0000},
			/*...,*/
			{0.0298, 0.0011, 0.0001 /*...,*/, 0.2373, 0.0000, 0.0000},
			{0.0625, 0.0215, 0.0034 /*...,*/, 0.4629, 0.1338, 0.0000},
			{0.0332, 0.0069, 0.0004 /*...,*/, 0.0806, 0.2402, 0.6133},
		},
		/*...,*/
		{
			{1.0000, 0.0000, 0.0000 /*...,*/, 0.0000, 0.0000, 0.0000},
			{0.9922, 0.0066, 0.0000 /*...,*/, 0.0000, 0.0000, 0.0000},
			{1.0000, 0.0002, 0.0000 /*...,*/, 0.0000, 0.0000, 0.0000},
			/*...,*/
			{0.9805, 0.0000, 0.0000 /*...,*/, 0.0000, 0.0000, 0.0000},
			{0.8164, 0.0030, 0.0003 /*...,*/, 0.0048, 0.0062, 0.0000},
			{0.9922, 0.0000, 0.0000 /*...,*/, 0.0000, 0.0001, 0.0000},
		},
		{
			{1.0000, 0.0000, 0.0000 /*...,*/, 0.0000, 0.0000, 0.0000},
			{0.9062, 0.0952, 0.0000 /*...,*/, 0.0000, 0.0000, 0.0000},
			{0.9922, 0.0028, 0.0059 /*...,*/, 0.0000, 0.0000, 0.0000},
			/*...,*/
			{0.9531, 0.0009, 0.0001 /*...,*/, 0.0053, 0.0000, 0.0000},
			{0.5820, 0.0188, 0.0081 /*...,*/, 0.0159, 0.0476, 0.0000},
			{0.9570, 0.0001, 0.0000 /*...,*/, 0.0000, 0.0006, 0.0294},
		},
		{
			{1.0000, 0.0000, 0.0000 /*...,*/, 0.0000, 0.0000, 0.0000},
			{1.0000, 0.0000, 0.0000 /*...,*/, 0.0000, 0.0000, 0.0000},
			{1.0000, 0.0000, 0.0000 /*...,*/, 0.0000, 0.0000, 0.0000},
			/*...,*/
			{1.0000, 0.0000, 0.0000 /*...,*/, 0.0000, 0.0000, 0.0000},
			{0.9961, 0.0000, 0.0000 /*...,*/, 0.0000, 0.0001, 0.0000},
			{1.0000, 0.0000, 0.0000 /*...,*/, 0.0000, 0.0000, 0.0000},
		},
	}

	actualScores32, err := actualScores.ToFloat32() // shape=[32,15,15] dtype=DT_F32
	if err != nil {
		t.Fatal(err)
	}
	if actualScores32, err = ml.Softmax(actualScores32, len(actualScores32.Size)-1); err != nil { // shape=[32,15,15] dtype=DT_F32
		t.Fatal(err)
	}
	if actualScores, err = actualScores32.ToBFloat16(); err != nil { // shape=[32,15,15] (N_Heads, sequenceLength, sequenceLength) dtype=DT_BF16
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedScoresSoftmax, expectedScoresSoftmaxSize, actualScores, 2*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	/*
		Goal in Python manner:
		output = torch.matmul(scores, values)
		output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)
	*/

	expectedOutputBeforeWeightsSize := []int{15, 4096}
	expectedOutputBeforeWeights := [][]float32{
		{0.0153, -0.0002, 0.0292 /*...,*/, 0.0008, 0.0005, -0.0001},
		{0.0138, -0.0008, 0.0214 /*...,*/, 0.0008, 0.0005, -0.0001},
		{0.0231, -0.0004, 0.0286 /*...,*/, 0.0008, 0.0005, -0.0001},
		/*...,*/
		{0.0078, -0.0038, 0.0181 /*...,*/, 0.0008, 0.0005, -0.0001},
		{0.0131, -0.0040, 0.0118 /*...,*/, 0.0008, 0.0005, -0.0001},
		{0.0075, -0.0059, 0.0142 /*...,*/, 0.0008, 0.0005, -0.0001},
	}

	actualOutput, err := ml.MatMul(actualScores, actualValues)
	if err != nil {
		t.Fatal(err)
	}
	if actualOutput, err = actualOutput.Transpose(0, 1); err != nil {
		t.Fatal(err)
	}
	outputTrailingSize := actualOutput.GetElementCount() / sequenceLength
	if actualOutput, err = actualOutput.Reshape([]int{sequenceLength, outputTrailingSize}); err != nil {
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedOutputBeforeWeights, expectedOutputBeforeWeightsSize, actualOutput, common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	expectedOutputAfterWeightsSize := []int{15, 4096}
	expectedOutputAfterWeights := [][]float32{
		{0.0009, -0.0026, 0.0015 /*...,*/, 0.0029, -0.0007, 0.0001},
		{0.0010, -0.0016, 0.0006 /*...,*/, 0.0020, 0.0000, -0.0004},
		{0.0002, 0.0014, 0.0003 /*...,*/, 0.0031, -0.0000, -0.0020},
		/*...,*/
		{0.0029, 0.0016, -0.0002 /*...,*/, -0.0014, 0.0002, -0.0010},
		{0.0025, 0.0021, 0.0057 /*...,*/, 0.0001, 0.0009, -0.0014},
		{0.0040, -0.0024, -0.0022 /*...,*/, -0.0089, -0.0002, -0.0022},
	}
	/*
		Apply lat.attn_wo weights to output
	*/

	// lat.attn_wo: [out_features, in_features] -> shape: [4096 4096] -> [N_Heads * HeadDim, Dim]
	if actualOutput, err = ml.LinearTransformation(actualOutput, attention.attn_wo); err != nil {
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedOutputAfterWeights, expectedOutputAfterWeightsSize, actualOutput, common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	return actualOutput
}

func testTransformerBlock_FeedForward_Forward(t *testing.T, feedForward *LlamaFeedForward, x *ml.Tensor) *ml.Tensor {
	/*
		Goal in Python manner:
		self.w2(F.silu(self.w1(x)) * self.w3(x))
		-->
		self.ffn_down(F.silu(self.ffn_gate(x)) * self.ffn_up(x))
	*/
	h, err := ml.LinearTransformation(x, feedForward.ffn_gate)
	if err != nil {
		t.Fatal(err)
	}
	if h, err = ml.Silu(h); err != nil {
		t.Fatal(err)
	}
	ffnUpX, err := ml.LinearTransformation(x, feedForward.ffn_up)
	if err != nil {
		t.Fatal(err)
	}
	if h, err = ml.MultiplyElementwise(h, ffnUpX); err != nil {
		t.Fatal(err)
	}
	actualOutput, err := ml.LinearTransformation(h, feedForward.ffn_down)
	if err != nil {
		t.Fatal(err)
	}
	return actualOutput
}

func testTransformerBlock_Forward(t *testing.T, skipCompareTestTensor bool, infContext *InferenceContext, transformerBlock *LlamaTransformerBlock, x *ml.Tensor, startPos int, freqsCis *ml.Tensor, mask *ml.Tensor) *ml.Tensor {
	/*
		h, err := ltb.attention.Forward(infContext, normalizedX, startPos, freqsCis, mask)
	*/
	normalizedX := testTransformerBlock_AttnNorm_Forward(t, skipCompareTestTensor, transformerBlock, x)
	h := testTransformerBlock_Attention_Forward(t, skipCompareTestTensor, infContext, transformerBlock.attention, normalizedX, startPos, freqsCis, mask)

	expectedHBeforeFeedForwardSize := []int{15, 4096}
	expectedHBeforeFeedForward := [][]float32{
		{0.0008, -0.0024, 0.0012 /*...,*/, 0.0025, -0.0009, 0.0005},
		{0.0015, -0.0018, 0.0008 /*...,*/, 0.0019, 0.0004, 0.0006},
		{0.0042, 0.0078, 0.0005 /*...,*/, 0.0038, -0.0085, -0.0012},
		/*...,*/
		{-0.0063, -0.0005, 0.0042 /*...,*/, -0.0015, -0.0013, 0.0072},
		{0.0025, 0.0014, 0.0057 /*...,*/, -0.0001, 0.0003, -0.0004},
		{0.0014, 0.0014, -0.0026 /*...,*/, -0.0075, -0.0008, -0.0009},
	}

	var err error
	if h, err = ml.Add(x, h); err != nil {
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedHBeforeFeedForward, expectedHBeforeFeedForwardSize, h, common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	/*
		h, err = ltb.ffn_norm.Forward(context, h)
	*/
	expectedOutputSize := []int{15, 4096}
	expectedOutput := [][]float32{
		{0.0014, 0.0040, -0.0050 /*...,*/, 0.0093, -0.0007, 0.0005},
		{0.0065, 0.0145, 0.0078 /*...,*/, -0.0156, -0.0422, -0.0074},
		{0.0062, 0.0115, 0.0087 /*...,*/, 0.0042, -0.0017, -0.0092},
		/*...,*/
		{-0.0315, -0.0170, 0.0009 /*...,*/, -0.0102, 0.0139, -0.0105},
		{0.0060, 0.0084, -0.0115 /*...,*/, -0.0215, -0.0025, 0.0041},
		{0.0025, 0.0054, 0.0052 /*...,*/, -0.0135, 0.0092, -0.0029},
	}
	normalizedH, err := transformerBlock.ffn_norm.Forward(infContext, h)
	if err != nil {
		t.Fatal(err)
	}
	ffnOutput := testTransformerBlock_FeedForward_Forward(t, transformerBlock.feedForward, normalizedH)
	actualOutput, err := ml.Add(h, ffnOutput)
	if err != nil {
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedOutput, expectedOutputSize, actualOutput, common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}
	return actualOutput
}

func testTransformer_Forward(t *testing.T, onlyFirstLayer bool, infContext *InferenceContext, transformer *LlamaTransformer, inputTokens *ml.Tensor, startPos int) *ml.Tensor {
	skipCompareTestTensor := !onlyFirstLayer || startPos > 0
	var err error
	actualInputTensor, actualFreqsCis, actualMask := testTransformer_Prepare(t, skipCompareTestTensor, transformer, inputTokens, startPos)

	currentTensor := actualInputTensor
	if onlyFirstLayer {
		firstLayer := transformer.Layers[0]
		currentTensor = testTransformerBlock_Forward(t, skipCompareTestTensor, infContext, firstLayer, currentTensor, startPos, actualFreqsCis, actualMask)
	} else {
		for layerIdx, layer := range transformer.Layers {
			infContext.Logf("Running transformer block layer: %d / %d\n", layerIdx+1, len(transformer.Layers))
			currentTensor = testTransformerBlock_Forward(t, true, infContext, layer, currentTensor, startPos, actualFreqsCis, actualMask)
		}
	}
	if currentTensor, err = transformer.output_norm.Forward(infContext, currentTensor); err != nil {
		t.Fatal(err)
	}
	output, err := ml.LinearTransformation(currentTensor, transformer.output)
	if err != nil {
		t.Fatal(err)
	}
	if output, err = output.ToFloat32(); err != nil {
		t.Fatal(err)
	}
	return output
}

func testSimulatedLog(format string, v ...any) {
	fmt.Printf(format, v...)
}

func testSimulatedInternal(t *testing.T, onlyFirstLayer bool) {
	modelDir := "../../models-original/Meta-Llama-3-8B-Instruct"
	if _, err := os.Stat(modelDir); err != nil {
		t.Skipf("Model directory \"%s\" is not found, passing this test: %s", modelDir, "TestSimulated")
		return
	}
	var err error
	common.GLogger, err = common.NewLogger(os.Stdout, nil)
	if err != nil {
		panic(err)
	}
	defer common.GLogger.Close()

	llamaModel, err := LoadModel(modelDir)
	if err != nil {
		t.Fatal(err)
	}
	defer llamaModel.Free()

	/*
	   promptTokens: " <|begin_of_text|><|start_header_id|>user<|end_header_id|>

	   What is your name?<|eot_id|><|start_header_id|>assistant<|end_header_id|>

	   "
	*/
	promptTokens := []TokenId{128000, 128006, 882, 128007, 271, 3923, 374, 701, 836, 30, 128009, 128006, 78191, 128007, 271}

	inferenceArgs := common.NewInferenceArgs()
	inferenceArgs.SequenceLength = 20
	infContext := NewInferenceContext(llamaModel, inferenceArgs, testSimulatedLog)

	tokens, err := ml.Full([]int{infContext.SequenceLength}, ml.DT_INT32, int32(llamaModel.Vocabulary.PadId))
	if err != nil {
		t.Fatal(err)
	}
	for i, token := range promptTokens {
		if err := tokens.SetItem([]int{i}, int32(token)); err != nil {
			t.Fatal(err)
		}
	}

	prevPos := 0
	minPromptLength := len(promptTokens)
	isFirstIteration := true
	for curPos := minPromptLength; curPos < infContext.SequenceLength; curPos++ {
		inputTokensSlice, err := tokens.Slice([]int{prevPos}, []int{curPos})
		if err != nil {
			t.Fatal(err)
		}

		actualLogits := testTransformer_Forward(t, onlyFirstLayer, infContext, llamaModel.Transformer, inputTokensSlice, prevPos)
		if onlyFirstLayer && isFirstIteration {
			// Although it is a valid and significant output as a tensor, it isn't a meaningful outcome,
			// because while producing this output, only first attention layer was run, not completely 32 of them.
			// But it is valuable to validate the whole model flow mathematically,
			// even if we had some precision differences between ours and original LLaMA Python code.

			expectedLogitsOnlyFirstLayerSize := []int{15, 128256}
			expectedLogitsOnlyFirstLayer := [][]float32{
				{-0.7695, -2.9219, -1.4219 /*...,*/, 1.2031, 1.2031, 1.2031},
				{1.5391, -0.5469, -0.6992 /*...,*/, -0.6680, -0.6680, -0.6680},
				{-0.3438, 0.3613, 0.8633 /*...,*/, 0.0850, 0.0850, 0.0850},
				/*...,*/
				{2.2031, 1.6406, 1.9531 /*...,*/, -0.3828, -0.3828, -0.3828},
				{-0.8711, -2.2969, -1.7656 /*...,*/, -0.3652, -0.3652, -0.3652},
				{0.3789, 0.6797, 0.3359 /*...,*/, -0.0659, -0.0659, -0.0659},
			}
			if err := ml.CompareTestTensorSkippable(!onlyFirstLayer, expectedLogitsOnlyFirstLayer, expectedLogitsOnlyFirstLayerSize, actualLogits, 30*common.THRESHOLD_BF16, true); err != nil {
				t.Fatal(err)
			}
		}
		if actualLogits, err = actualLogits.Slice([]int{actualLogits.Size[0] - 1}, []int{actualLogits.Size[0]}); err != nil {
			t.Fatal(err)
		}
		if onlyFirstLayer && isFirstIteration {
			expectedLogitsLastRowOnlyFirstLayerSize := []int{1, 128256}
			expectedLogitsLastRowOnlyFirstLayer := [][]float32{
				{0.3789, 0.6797, 0.3359 /*...,*/, -0.0659, -0.0659, -0.0659},
			}
			if err := ml.CompareTestTensorSkippable(!onlyFirstLayer, expectedLogitsLastRowOnlyFirstLayer, expectedLogitsLastRowOnlyFirstLayerSize, actualLogits, 30*common.THRESHOLD_BF16, true); err != nil {
				t.Fatal(err)
			}
		}
		actualNextToken, err := ml.Argmax(actualLogits, len(actualLogits.Size)-1) // shape=[1,1] dtype=DT_INT32
		if err != nil {
			t.Fatal(err)
		}
		if onlyFirstLayer && isFirstIteration {
			expectedNextTokenOnlyFirstLayerSize := []int{1}
			expectedNextTokenOnlyFirstLayer := []float32{
				63075,
			}
			if err := ml.CompareTestTensorSkippable(!onlyFirstLayer, expectedNextTokenOnlyFirstLayer, expectedNextTokenOnlyFirstLayerSize, actualNextToken, common.THRESHOLD_EXACT, true); err != nil {
				t.Fatal(err)
			}
		}
		actualNextTokenId := TokenId(actualNextToken.Item().(int32))
		// Comment in original Python code: only replace token if prompt has already been generated
		existingToken, err := tokens.GetItem([]int{curPos})
		if err != nil {
			t.Fatal(err)
		}
		existingTokenId := TokenId(existingToken.(int32))
		if existingTokenId != llamaModel.Vocabulary.PadId {
			actualNextTokenId = existingTokenId
		}
		if err = tokens.SetItem([]int{curPos}, int32(actualNextTokenId)); err != nil {
			t.Fatal(err)
		}
		_, actualEosReached := llamaModel.Vocabulary.StopTokenIds[actualNextTokenId]

		if onlyFirstLayer && isFirstIteration {
			expectedEosReached := false
			if actualEosReached != expectedEosReached {
				t.Fatalf("expected eosReached %v, but got %v", expectedEosReached, actualEosReached)
			}
		}
		prevPos = curPos
		isFirstIteration = false
	}

	if onlyFirstLayer {
		expectedOutputTokenIds := []TokenId{63075, 41042, 51214, 78759, 43456}
		actualOutputTokenIds := make([]TokenId, 0)
		for i := minPromptLength; i < infContext.SequenceLength; i++ {
			tokenItem, err := tokens.GetItem([]int{i})
			if err != nil {
				t.Fatal(err)
			}
			actualOutputTokenIds = append(actualOutputTokenIds, TokenId(tokenItem.(int32)))
		}
		if !reflect.DeepEqual(expectedOutputTokenIds, actualOutputTokenIds) {
			t.Fatalf("expected actualOutputTokenIds %v, but got %v", expectedOutputTokenIds, actualOutputTokenIds)
		}
	}
}

func TestSimulatedOnlyFirstLayer(t *testing.T) {
	t.Setenv("test.timeout", "10m")
	testSimulatedInternal(t, true)
}

const runTestSimulatedFull = false

func TestSimulatedFull(t *testing.T) {
	if !runTestSimulatedFull {
		t.Skip("Skipping TestSimulatedFull because runTestSimulatedFull is set to false")
	}
	testSimulatedInternal(t, false)
}

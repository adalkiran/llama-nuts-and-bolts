package model

import (
	"fmt"
	"math"
	"os"
	"reflect"
	"testing"

	"github.com/adalkiran/llama-nuts-and-bolts/src/common"
	"github.com/adalkiran/llama-nuts-and-bolts/src/dtype"
	"github.com/adalkiran/llama-nuts-and-bolts/src/ml"
)

/*
	This simulation was added to ensure all steps are done correctly
	(until only first attention layer).
*/

func testTransformer_Prepare(t *testing.T, skipCompareTestTensor bool, transformer *LlamaTransformer, inputTokens *ml.Tensor, startPos int) (actualInputTensor *ml.Tensor, actualFreqsCis *ml.Tensor, actualMask *ml.Tensor) {
	expectedInputTensorSize := []int{5, 4096}
	// Shortened form as corresponding indices [0, 1, 2, 4093, 4094, 4095]
	expectedInputTensorShortened := [][]float32{
		{0.0018, -0.0038, 0.0010 /*...,*/, -0.0090, 0.0027, -0.0038},
		{0.0374, -0.0038, 0.0062 /*...,*/, -0.0181, 0.0026, 0.0159},
		{-0.0130, -0.0147, 0.0109 /*...,*/, 0.0071, -0.0283, 0.0051},
		{-0.0013, -0.0098, -0.0095 /*...,*/, -0.0035, -0.0051, -0.0104},
		{-0.0051, 0.0017, 0.0095 /*...,*/, 0.0098, -0.0047, -0.0086},
	}

	expectedFreqsCisSize := []int{5, 64}

	expectedMaskSize := []int{5, 5}
	negInf := float32(math.Inf(-1))
	expectedMask := [][]float32{
		{0, negInf, negInf, negInf, negInf},
		{0, 0, negInf, negInf, negInf},
		{0, 0, 0, negInf, negInf},
		{0, 0, 0, 0, negInf},
		{0, 0, 0, 0, 0},
	}
	var err error
	if actualInputTensor, actualFreqsCis, actualMask, err = transformer.prepare(inputTokens, startPos); err != nil {
		t.Fatal(err)
	}

	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedInputTensorShortened, expectedInputTensorSize, actualInputTensor, common.THRESHOLD_F32, true); err != nil {
		t.Fatal(err)
	}

	if !skipCompareTestTensor && !reflect.DeepEqual(expectedFreqsCisSize, actualFreqsCis.Size) {
		t.Fatalf("expected size %v, but got %v", expectedFreqsCisSize, actualFreqsCis.Size)
	}

	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedMask, expectedMaskSize, actualMask, common.THRESHOLD_F32, false); err != nil {
		t.Fatal(err)
	}
	return
}

func testTransformerBlock_AttnNorm_Forward(t *testing.T, skipCompareTestTensor bool, transformerBlock *LlamaTransformerBlock, x *ml.Tensor) *ml.Tensor {
	/*
		normalizedX, err := ltb.attn_norm.Forward(infContext, x)
	*/
	expectedAttnNormPartSize := []int{5, 4096}
	expectedAttnNormPart := [][]float32{
		{0.2354, -0.4883, 0.1230 /*...,*/, -1.1562, 0.3398, -0.4805},
		{2.2812, -0.2334, 0.3809 /*...,*/, -1.1016, 0.1592, 0.9688},
		{-1.0781, -1.2266, 0.9102 /*...,*/, 0.5898, -2.3594, 0.4219},
		{-0.0933, -0.7031, -0.6758 /*...,*/, -0.2539, -0.3633, -0.7461},
		{-0.5312, 0.1787, 0.9844 /*...,*/, 1.0234, -0.4844, -0.8945},
	}

	expectedAttnNormalizedXSize := []int{5, 4096}
	expectedAttnNormalizedX := [][]float32{
		{0.0070, -0.0069, 0.0004 /*...,*/, -0.0115, 0.0039, -0.0035},
		{0.0679, -0.0033, 0.0012 /*...,*/, -0.0110, 0.0018, 0.0070},
		{-0.0322, -0.0172, 0.0028 /*...,*/, 0.0059, -0.0267, 0.0031},
		{-0.0028, -0.0099, -0.0021 /*...,*/, -0.0025, -0.0041, -0.0054},
		{-0.0159, 0.0025, 0.0031 /*...,*/, 0.0102, -0.0055, -0.0065},
	}

	actualAttnNormPart, err := transformerBlock.attn_norm.doNormalization(x)
	if err != nil {
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedAttnNormPart, expectedAttnNormPartSize, actualAttnNormPart, 2*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	actualAttnNormalizedX, err := ml.MultiplyElementwise(actualAttnNormPart, transformerBlock.attn_norm.weights)
	if err != nil {
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedAttnNormalizedX, expectedAttnNormalizedXSize, actualAttnNormalizedX, common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}
	return actualAttnNormalizedX
}

func testTransformerBlock_Attention_Forward(t *testing.T, skipCompareTestTensor bool, infContext *InferenceContext, attention *LlamaAttention, x *ml.Tensor, startPos int, freqsCis *ml.Tensor, mask *ml.Tensor) *ml.Tensor {
	expectedXqSize := []int{5, 4096}
	expectedXq := [][]float32{
		{0.1172, -0.4473, -0.4082 /*...,*/, 0.6250, -0.1553, 0.1797},
		{0.2559, -1.6641, -1.1328 /*...,*/, 0.3613, -0.1455, -0.4844},
		{0.0752, -1.6641, -0.9609 /*...,*/, 0.3633, 0.0295, -0.1885},
		{0.2021, -1.8672, -1.2422 /*...,*/, 0.1641, -0.0151, -0.2617},
		{0.0036, -1.3359, -0.5977 /*...,*/, 0.1846, 0.1670, 0.0815},
	}

	expectedXkSize := []int{5, 4096}
	expectedXk := [][]float32{
		{-0.4492, -0.2100, 0.0050 /*...,*/, 0.1777, -0.3008, 0.2852},
		{-0.4375, 0.6602, 0.5234 /*...,*/, -0.5820, 0.6641, 0.3320},
		{-0.5039, 0.3105, 0.2598 /*...,*/, 0.8359, -1.0625, -0.1118},
		{0.2061, -0.1279, -0.3730 /*...,*/, -0.5508, 0.6562, 0.1699},
		{-0.1436, -0.3789, -0.3535 /*...,*/, 0.9727, -0.9453, -0.2344},
	}

	expectedXvSize := []int{5, 4096}
	expectedXv := [][]float32{
		{-0.0008, -0.0205, -0.0021 /*...,*/, 0.0026, 0.0074, 0.0125},
		{-0.0002, -0.0085, -0.0034 /*...,*/, -0.0013, -0.0111, 0.0065},
		{0.0005, -0.0012, -0.0010 /*...,*/, -0.0084, 0.0192, -0.0050},
		{0.0019, -0.0090, 0.0136 /*...,*/, -0.0012, -0.0039, -0.0014},
		{-0.0085, 0.0068, 0.0004 /*...,*/, -0.0049, 0.0164, 0.0090},
	}

	sequenceLength := x.Size[0]

	// lat.attn_wq: [out_features, in_features] -> shape: [4096 4096] -> [N_Heads * HeadDim, Dim]
	actualXq, err := ml.LinearTransformation(x, attention.attn_wq)
	if err != nil {
		t.Fatal(err)
	}

	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedXq, expectedXqSize, actualXq, 2*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	// lat.attn_wk: [out_features, in_features] -> shape: [4096 4096] -> [N_KVHeads * HeadDim, Dim]
	actualXk, err := ml.LinearTransformation(x, attention.attn_wk)
	if err != nil {
		t.Fatal(err)
	}

	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedXk, expectedXkSize, actualXk, 2*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	// lat.attn_wv: [out_features, in_features] -> shape: [4096 4096] -> [N_KVHeads * HeadDim, Dim]
	actualXv, err := ml.LinearTransformation(x, attention.attn_wv)
	if err != nil {
		t.Fatal(err)
	}

	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedXv, expectedXvSize, actualXv, 2*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	/*
		Do reshapings
	*/

	expectedXqRsSize := []int{5, 32, 128}
	expectedXqRs := [][][]float32{
		{
			{0.1172, -0.4473, -0.4082 /*...,*/, 1.7109, 0.5977, -0.3340},
			{-0.1572, -1.0312, 0.2676 /*...,*/, -0.6914, 0.1084, 0.5586},
			{0.0342, 0.0559, 0.3867 /*...,*/, 0.3457, -0.3340, 0.3379},
			/*...,*/
			{0.6797, 1.3672, -1.6016 /*...,*/, 0.4902, -0.1162, 1.2734},
			{0.7461, 1.9219, 1.0156 /*...,*/, 0.5195, -0.3789, 0.4844},
			{-1.3906, 0.7305, 1.3828 /*...,*/, 0.6250, -0.1553, 0.1797},
		},
		{
			{0.2559, -1.6641, -1.1328 /*...,*/, 0.5078, -1.0625, -0.7539},
			{0.0942, 0.0364, 0.1338 /*...,*/, -0.8828, 0.1357, 0.5391},
			{0.5312, 0.5469, 0.0084 /*...,*/, 0.3008, -0.2578, 0.3301},
			/*...,*/
			{0.5234, 2.7500, -2.3438 /*...,*/, 0.8945, -1.3828, 2.0156},
			{0.7930, 2.3594, 1.9531 /*...,*/, 0.5312, -0.3848, 0.5469},
			{-0.9297, 0.4434, -0.2793 /*...,*/, 0.3613, -0.1455, -0.4844},
		},
		{
			{0.0752, -1.6641, -0.9609 /*...,*/, 0.3008, -0.8516, -0.7656},
			{0.3301, -0.1079, 0.1904 /*...,*/, -0.1514, -0.6758, -0.1416},
			{-0.4902, 0.8711, -0.7734 /*...,*/, -0.1162, 0.0991, -0.0903},
			/*...,*/
			{0.4668, 1.3594, -1.1328 /*...,*/, 0.7852, -0.3730, 1.2266},
			{0.6133, 2.6875, 2.1875 /*...,*/, 0.6367, -0.4707, 0.6523},
			{-1.8359, 0.4238, -1.1641 /*...,*/, 0.3633, 0.0295, -0.1885},
		},
		{
			{0.2021, -1.8672, -1.2422 /*...,*/, 0.7617, -1.0312, -1.0469},
			{0.0996, -0.0698, 0.0579 /*...,*/, -0.5664, -0.0483, 0.2305},
			{0.4688, 0.1299, 0.2344 /*...,*/, 0.0815, -0.0654, 0.1074},
			/*...,*/
			{0.5703, 1.9531, -2.1094 /*...,*/, 0.7930, -0.7148, 1.6797},
			{0.8672, 2.6875, 2.0469 /*...,*/, 0.6172, -0.4531, 0.6406},
			{-1.3281, 0.4844, -0.5234 /*...,*/, 0.1641, -0.0151, -0.2617},
		},
		{
			{0.0036, -1.3359, -0.5977 /*...,*/, 0.3672, -0.9414, -0.7969},
			{0.4766, -0.3184, 0.1592 /*...,*/, 0.1729, -0.9531, -0.3750},
			{-0.5898, 0.6445, -0.7617 /*...,*/, -0.2754, 0.2227, -0.2461},
			/*...,*/
			{0.3730, 0.7266, -0.6055 /*...,*/, 0.5352, -0.0801, 0.7344},
			{0.5977, 2.7656, 2.0938 /*...,*/, 0.6523, -0.4863, 0.6719},
			{-2.0625, 0.4180, -1.3203 /*...,*/, 0.1846, 0.1670, 0.0815},
		},
	}

	expectedXkRsSize := []int{5, 32, 128}
	expectedXkRs := [][][]float32{
		{
			{-4.4922e-01, -2.0996e-01, 4.9744e-03 /*...,*/, -7.8613e-02, -5.6641e-01, -5.9570e-02},
			{1.3672e+00, -6.3281e-01, 1.0859e+00 /*...,*/, -2.7734e-01, 1.6699e-01, 5.5859e-01},
			{-6.9580e-03, 3.5547e-01, -2.7148e-01 /*...,*/, 3.4766e-01, -6.6797e-01, 6.1328e-01},
			/*...,*/
			{-1.4526e-02, -6.3477e-02, 6.0730e-03 /*...,*/, 9.4922e-01, -4.6680e-01, -4.3164e-01},
			{2.3633e-01, -6.7188e-01, -5.7031e-01 /*...,*/, 6.5918e-02, -9.5703e-02, 7.0312e-02},
			{-4.4531e-01, 1.8164e-01, 1.2969e+00 /*...,*/, 1.7773e-01, -3.0078e-01, 2.8516e-01},
		},
		{
			{-4.3750e-01, 6.6016e-01, 5.2344e-01 /*...,*/, -1.2354e-01, 2.7930e-01, 5.8594e-01},
			{8.7109e-01, -2.6953e-01, 5.6250e-01 /*...,*/, 3.0078e-01, 2.5781e-01, -1.8359e-01},
			{-1.5039e-01, 2.8516e-01, -4.6094e-01 /*...,*/, -4.8828e-01, 5.5469e-01, -4.3945e-01},
			/*...,*/
			{-1.1475e-01, -1.5625e-01, 7.9102e-02 /*...,*/, 5.7031e-01, -1.9434e-01, -1.1816e-01},
			{2.4512e-01, 2.4805e-01, -2.1191e-01 /*...,*/, -1.0840e-01, 2.0703e-01, -1.0449e-01},
			{-9.9609e-01, -2.0020e-01, -7.6172e-01 /*...,*/, -5.8203e-01, 6.6406e-01, 3.3203e-01},
		},
		{
			{-5.0391e-01, 3.1055e-01, 2.5977e-01 /*...,*/, -2.5586e-01, 1.8848e-01, 3.7695e-01},
			{6.1328e-01, 5.2734e-01, 8.0469e-01 /*...,*/, 2.5977e-01, -1.9434e-01, -6.9922e-01},
			{-3.8867e-01, -4.6680e-01, 5.6152e-02 /*...,*/, 1.8359e+00, -1.7812e+00, 1.6953e+00},
			/*...,*/
			{-1.2422e+00, 3.3008e-01, -2.4219e-01 /*...,*/, -7.1875e-01, 1.3984e+00, -1.0859e+00},
			{-4.6094e-01, -1.7285e-01, -3.0469e-01 /*...,*/, 1.4355e-01, -3.5156e-01, 1.4551e-01},
			{1.1670e-01, -8.9355e-02, -5.7129e-02 /*...,*/, 8.3594e-01, -1.0625e+00, -1.1182e-01},
		},
		{
			{2.0605e-01, -1.2793e-01, -3.7305e-01 /*...,*/, 2.0752e-02, -1.0596e-01, 5.8105e-02},
			{9.6484e-01, 4.9591e-04, 7.4609e-01 /*...,*/, -4.4189e-02, 3.6523e-01, 4.3750e-01},
			{5.7129e-02, 3.8672e-01, -2.5781e-01 /*...,*/, -8.2031e-01, 8.2422e-01, -7.1094e-01},
			/*...,*/
			{-3.5547e-01, 5.4932e-02, -1.0010e-02 /*...,*/, 1.0986e-01, 3.0273e-01, -5.4688e-01},
			{2.4805e-01, -1.1536e-02, -4.4922e-01 /*...,*/, -2.5781e-01, 3.5938e-01, -2.5586e-01},
			{-6.6797e-01, -3.9258e-01, -5.2344e-01 /*...,*/, -5.5078e-01, 6.5625e-01, 1.6992e-01},
		},
		{
			{-1.4355e-01, -3.7891e-01, -3.5352e-01 /*...,*/, 2.4121e-01, -1.1914e-01, -1.6895e-01},
			{5.5078e-01, 8.6328e-01, 9.8438e-01 /*...,*/, 6.7871e-02, -3.0859e-01, -3.6523e-01},
			{-2.9492e-01, -4.0625e-01, 1.7480e-01 /*...,*/, 1.8281e+00, -1.7188e+00, 1.6641e+00},
			/*...,*/
			{-1.7734e+00, 5.0781e-01, -3.7305e-01 /*...,*/, -1.0547e+00, 1.9766e+00, -1.3750e+00},
			{-3.7695e-01, -4.5898e-01, -6.2891e-01 /*...,*/, 6.6895e-02, -3.1641e-01, 6.6895e-02},
			{1.8945e-01, -9.5703e-02, -1.1780e-02 /*...,*/, 9.7266e-01, -9.4531e-01, -2.3438e-01},
		},
	}

	expectedXvRsSize := []int{5, 32, 128}
	expectedXvRs := [][][]float32{
		{
			{-8.2397e-04, -2.0508e-02, -2.0752e-03 /*...,*/, -1.5137e-02, 4.6997e-03, -7.6294e-03},
			{-3.2959e-03, 1.6689e-04, -7.4768e-03 /*...,*/, 7.1716e-04, 1.0071e-02, -1.1475e-02},
			{1.6327e-03, -1.1841e-02, 8.3008e-03 /*...,*/, 6.5918e-03, 8.2779e-04, -7.2937e-03},
			/*...,*/
			{-7.0801e-03, -8.4839e-03, 5.1880e-03 /*...,*/, 2.6001e-02, 2.8809e-02, -3.4027e-03},
			{1.5991e-02, -1.4404e-02, -8.3008e-03 /*...,*/, 1.6251e-03, -2.8931e-02, -7.7515e-03},
			{-3.3112e-03, -5.2490e-03, 3.9673e-03 /*...,*/, 2.5635e-03, 7.4463e-03, 1.2512e-02},
		},
		{
			{-2.1744e-04, -8.4839e-03, -3.4332e-03 /*...,*/, 4.3945e-03, 6.7139e-03, -6.0425e-03},
			{-3.9673e-03, -1.0834e-03, 4.6082e-03 /*...,*/, 1.0620e-02, 1.0223e-03, -6.5613e-03},
			{1.0498e-02, -6.1340e-03, -2.7466e-03 /*...,*/, 8.4229e-03, 5.3101e-03, -5.5237e-03},
			/*...,*/
			{-2.0996e-02, 2.9785e-02, -7.2754e-02 /*...,*/, -1.0254e-01, -3.0396e-02, -1.9165e-02},
			{-7.0496e-03, 1.9409e-02, -1.1475e-02 /*...,*/, -2.8992e-03, 2.0447e-03, -5.8289e-03},
			{1.4954e-03, 9.8877e-03, -1.1719e-02 /*...,*/, -1.3123e-03, -1.1108e-02, 6.4697e-03},
		},
		{
			{5.1117e-04, -1.1673e-03, -9.9182e-04 /*...,*/, 7.4463e-03, -4.7913e-03, -1.7242e-03},
			{-2.2888e-03, 2.9602e-03, 6.8665e-03 /*...,*/, -9.9487e-03, -2.1057e-03, -6.6528e-03},
			{-9.9487e-03, 3.3569e-03, 6.9275e-03 /*...,*/, -3.7193e-04, 3.7079e-03, 6.4697e-03},
			/*...,*/
			{-8.4305e-04, -2.8687e-02, 3.2471e-02 /*...,*/, -2.2583e-03, -8.1787e-03, 2.2095e-02},
			{1.3184e-02, -1.3428e-03, 1.4114e-04 /*...,*/, -2.7771e-03, -1.0620e-02, 3.1433e-03},
			{-9.2316e-04, -1.9775e-02, -3.2654e-03 /*...,*/, -8.4229e-03, 1.9165e-02, -5.0049e-03},
		},
		{
			{1.9226e-03, -8.9722e-03, 1.3611e-02 /*...,*/, -5.9509e-03, 8.7891e-03, -1.0376e-03},
			{-6.7139e-03, -1.4496e-03, 1.6556e-03 /*...,*/, 3.8757e-03, 7.0496e-03, 5.5542e-03},
			{3.3417e-03, -9.5825e-03, 3.1738e-03 /*...,*/, -8.5449e-03, -5.1575e-03, -7.8125e-03},
			/*...,*/
			{-1.4465e-02, -6.2256e-02, 4.9438e-03 /*...,*/, -5.6885e-02, -1.2329e-02, 1.8921e-02},
			{2.2888e-03, -8.3160e-04, -1.7643e-04 /*...,*/, -4.9438e-03, -1.0376e-02, -1.0910e-03},
			{-2.5330e-03, 1.7548e-03, -1.0742e-02 /*...,*/, -1.2436e-03, -3.8757e-03, -1.4420e-03},
		},
		{
			{-8.4839e-03, 6.8054e-03, 4.4250e-04 /*...,*/, 2.2583e-03, -4.6692e-03, 4.3640e-03},
			{-6.3477e-03, 4.9133e-03, -6.2866e-03 /*...,*/, -5.1270e-03, -6.9885e-03, -3.3875e-03},
			{-2.1935e-05, -5.0049e-03, -2.8687e-03 /*...,*/, 3.9978e-03, -5.3711e-03, 6.1798e-04},
			/*...,*/
			{-8.3008e-03, -2.6001e-02, -2.7954e-02 /*...,*/, 2.5269e-02, -1.2207e-02, -2.6489e-02},
			{-4.4861e-03, 6.2561e-03, 4.6387e-03 /*...,*/, -1.7047e-05, 5.3711e-03, 3.9864e-04},
			{-1.7929e-03, 5.0354e-03, -1.9684e-03 /*...,*/, -4.8523e-03, 1.6357e-02, 8.9722e-03},
		},
	}

	if actualXq, err = actualXq.Reshape([]int{sequenceLength, attention.N_Heads, attention.HeadDim}); err != nil {
		t.Fatal(err)
	}

	if actualXk, err = actualXk.Reshape([]int{sequenceLength, attention.N_KVHeads, attention.HeadDim}); err != nil {
		t.Fatal(err)
	}

	if actualXv, err = actualXv.Reshape([]int{sequenceLength, attention.N_KVHeads, attention.HeadDim}); err != nil {
		t.Fatal(err)
	}

	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedXqRs, expectedXqRsSize, actualXq, 4*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedXkRs, expectedXkRsSize, actualXk, 4*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedXvRs, expectedXvRsSize, actualXv, 4*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	/*
		Apply rotary embeddings
	*/

	expectedXqRotarySize := []int{5, 32, 128}
	expectedXqRotary := [][][]float32{
		{
			{0.1172, -0.4473, -0.4082 /*...,*/, 1.7109, 0.5977, -0.3340},
			{-0.1572, -1.0312, 0.2676 /*...,*/, -0.6914, 0.1084, 0.5586},
			{0.0342, 0.0559, 0.3867 /*...,*/, 0.3457, -0.3340, 0.3379},
			/*...,*/
			{0.6797, 1.3672, -1.6016 /*...,*/, 0.4902, -0.1162, 1.2734},
			{0.7461, 1.9219, 1.0156 /*...,*/, 0.5195, -0.3789, 0.4844},
			{-1.3906, 0.7305, 1.3828 /*...,*/, 0.6250, -0.1553, 0.1797},
		},
		{
			{1.5391, -0.6836, -1.9766 /*...,*/, 0.5078, -1.0625, -0.7539},
			{0.0203, 0.0991, -0.0674 /*...,*/, -0.8828, 0.1357, 0.5391},
			{-0.1729, 0.7422, -0.4883 /*...,*/, 0.3008, -0.2578, 0.3301},
			/*...,*/
			{-2.0312, 1.9297, -2.3125 /*...,*/, 0.8945, -1.3828, 2.0156},
			{-1.5547, 1.9453, 2.3594 /*...,*/, 0.5312, -0.3848, 0.5469},
			{-0.8750, -0.5430, -0.5273 /*...,*/, 0.3613, -0.1455, -0.4844},
		},
		{
			{1.4844, 0.7617, -1.3594 /*...,*/, 0.3008, -0.8516, -0.7656},
			{-0.0393, 0.3457, -0.0564 /*...,*/, -0.1514, -0.6758, -0.1416},
			{-0.5898, -0.8086, -0.4316 /*...,*/, -0.1162, 0.0991, -0.0903},
			/*...,*/
			{-1.4297, -0.1416, -0.1826 /*...,*/, 0.7852, -0.3730, 1.2266},
			{-2.7031, -0.5625, 1.0938 /*...,*/, 0.6367, -0.4707, 0.6523},
			{0.3789, -1.8438, -0.3574 /*...,*/, 0.3633, 0.0295, -0.1885},
		},
		{
			{0.0635, 1.8750 /*0.0771*/, 0.0336 /*...,*/, 0.7617, -1.0312, -1.0469},
			{-0.0889, 0.0830, -0.0938 /*...,*/, -0.5664, -0.0483, 0.2305},
			{-0.4824, -0.0625, -0.4395 /*...,*/, 0.0815, -0.0654, 0.1074},
			/*...,*/
			{-0.8398, -1.8516 /*1.3828*/, 1.3359 /*...,*/, 0.7930, -0.7148, 1.6797},
			{-1.2344, -2.5312 /*-0.9336*/, -0.8789 /*...,*/, 0.6172, -0.4531, 0.6406},
			{1.2500, -0.6680, 0.2041 /*...,*/, 0.1641, -0.0151, -0.2617},
		},
		{
			{-1.0156, 0.8711, 0.9961 /*...,*/, 0.3672, -0.9414, -0.7969},
			{-0.5508, -0.1523, -0.2168 /*...,*/, 0.1729, -0.9531, -0.3750},
			{0.8750, 0.0251, 0.9258 /*...,*/, -0.2754, 0.2227, -0.2461},
			/*...,*/
			{0.3066, -0.7578, 0.6094 /*...,*/, 0.5352, -0.0806, 0.7344},
			{1.7031 /*-2.2656*/, -2.2187, -2.4219 /*...,*/, 0.6523, -0.4863, 0.6719},
			{1.6641, 1.2891, 1.4219 /*...,*/, 0.1846, 0.1670, 0.0815},
		},
	}

	expectedXkRotarySize := []int{5, 32, 128}
	expectedXkRotary := [][][]float32{
		{
			{-0.4492, -0.2100, 0.0050 /*...,*/, -0.0786, -0.5664, -0.0596},
			{1.3672, -0.6328, 1.0859 /*...,*/, -0.2773, 0.1670, 0.5586},
			{-0.0070, 0.3555, -0.2715 /*...,*/, 0.3477, -0.6680, 0.6133},
			/*...,*/
			{-0.0145, -0.0635, 0.0061 /*...,*/, 0.9492, -0.4668, -0.4316},
			{0.2363, -0.6719, -0.5703 /*...,*/, 0.0659, -0.0957, 0.0703},
			{-0.4453, 0.1816, 1.2969 /*...,*/, 0.1777, -0.3008, 0.2852},
		},
		{
			{-0.7930, -0.0115, 0.4707 /*...,*/, -0.1235, 0.2793, 0.5859},
			{0.6992, 0.5859, 0.6680 /*...,*/, 0.3008, 0.2578, -0.1836},
			{-0.3203, 0.0275, -0.1035 /*...,*/, -0.4883, 0.5547, -0.4395},
			/*...,*/
			{0.0693, -0.1807, 0.1270 /*...,*/, 0.5703, -0.1943, -0.1182},
			{-0.0762, 0.3398, 0.4043 /*...,*/, -0.1084, 0.2070, -0.1045},
			{-0.3691, -0.9453, -0.8477 /*...,*/, -0.5820, 0.6641, 0.3320},
		},
		{
			{-0.0728, -0.5859, 0.0253 /*...,*/, -0.2559, 0.1885, 0.3770},
			{-0.7344, 0.3379, -0.4355 /*...,*/, 0.2598, -0.1943, -0.6992},
			{0.5859, -0.1592, 0.3105 /*...,*/, 1.8359, -1.7812, 1.6953},
			/*...,*/
			{0.2168, -1.2656, 0.9102 /*...,*/, -0.7188, 1.3984, -1.0859},
			{0.3496, -0.3477, 0.8125 /*...,*/, 0.1436, -0.3516, 0.1455},
			{0.0327, 0.1436, -0.0967 /*...,*/, 0.8359, -1.0625, -0.1123},
		},
		{
			{-0.1855, 0.1553, 0.2432 /*...,*/, 0.0208, -0.1060, 0.0581},
			{-0.9570, 0.1357, -0.4805 /*...,*/, -0.0442, 0.3652, 0.4375},
			{-0.1113, -0.3750, 0.3281 /*...,*/, -0.8203, 0.8242, -0.7109},
			/*...,*/
			{0.3438, -0.1045, 0.1309 /*...,*/, 0.1104, 0.3027, -0.5469},
			{-0.2441, 0.0464, 0.6211 /*...,*/, -0.2578, 0.3594, -0.2559},
			{0.7148, 0.2949, 0.2891 /*...,*/, -0.5508, 0.6562, 0.1699},
		},
		{
			{-0.1934, 0.3555, 0.4238 /*...,*/, 0.2412, -0.1191, -0.1689},
			{0.2930, -0.9805, -0.7461 /*...,*/, 0.0679, -0.3086, -0.3652},
			{-0.1147, 0.4883, -0.2500 /*...,*/, 1.8281, -1.7188, 1.6641},
			/*...,*/
			{1.5469, 1.0078, -0.0767 /*...,*/, -1.0547, 1.9766, -1.3750},
			{-0.1011, 0.5859, 0.4043 /*...,*/, 0.0669, -0.3164, 0.0669},
			{-0.1963, -0.0811, 0.0260 /*...,*/, 0.9727, -0.9453, -0.2344},
		},
	}

	if actualXq, actualXk, err = applyRotaryEmbeddings(actualXq, actualXk, freqsCis); err != nil {
		return nil
	}

	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedXqRotary, expectedXqRotarySize, actualXq, 4*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedXkRotary, expectedXkRotarySize, actualXk, 4*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	/*
		Update KV cache
	*/

	infContext.CacheK[attention.LayerIndex].SetSlice([]int{startPos}, []int{startPos + sequenceLength}, actualXk)
	infContext.CacheV[attention.LayerIndex].SetSlice([]int{startPos}, []int{startPos + sequenceLength}, actualXv)

	/*
		Retrieve cached KV so far
	*/

	actualKeys, err := infContext.CacheK[attention.LayerIndex].Slice([]int{0}, []int{startPos + sequenceLength})
	if err != nil {
		t.Fatal(err)
	}
	actualValues, err := infContext.CacheV[attention.LayerIndex].Slice([]int{0}, []int{startPos + sequenceLength})
	if err != nil {
		t.Fatal(err)
	}

	/*
		Repeat k/v heads if N_KVHeads < N_Heads
	*/

	N_Rep := 1 // no repetition is needed for 7B model

	if actualKeys, err = attentionRepeatKV(actualKeys, N_Rep); err != nil { // shape=[5, 32, 128] (cacheLen + sequenceLength, N_Heads, HeadDim)
		t.Fatal(err)
	}
	if actualValues, err = attentionRepeatKV(actualValues, N_Rep); err != nil { // shape=[5, 32, 128] (cacheLen + sequenceLength, N_Heads, HeadDim)
		t.Fatal(err)
	}

	if startPos == 0 {
		// For startPos=0 and N_Rep=1 case, retrieved "keys" and "values" variables from KV Cache are same with "actualXk" and "actualXV" values
		if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedXkRotary, expectedXkRotarySize, actualXk, 4*common.THRESHOLD_BF16, true); err != nil {
			t.Fatal(err)
		}

		if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedXvRs, expectedXvRsSize, actualXv, 4*common.THRESHOLD_BF16, true); err != nil {
			t.Fatal(err)
		}
	}

	/*
		Do transposes
	*/

	expectedXqTransposeSize := []int{32, 5, 128}
	expectedXqTranspose := [][][]float32{
		{
			{0.1172, -0.4473, -0.4082 /*...,*/, 1.7109, 0.5977, -0.3340},
			{1.5391, -0.6836, -1.9766 /*...,*/, 0.5078, -1.0625, -0.7539},
			{1.4844, 0.7617, -1.3594 /*...,*/, 0.3008, -0.8516, -0.7656},
			{0.0635, 1.8750 /*0.0771*/, 0.0336 /*...,*/, 0.7617, -1.0312, -1.0469},
			{-1.0156, 0.8711, 0.9961 /*...,*/, 0.3672, -0.9414, -0.7969},
		},
		{
			{-0.1572, -1.0312, 0.2676 /*...,*/, -0.6914, 0.1084, 0.5586},
			{0.0203, 0.0991, -0.0674 /*...,*/, -0.8828, 0.1357, 0.5391},
			{-0.0393, 0.3457, -0.0564 /*...,*/, -0.1514, -0.6758, -0.1416},
			{-0.0889, 0.0830, -0.0938 /*...,*/, -0.5664, -0.0483, 0.2305},
			{-0.5508, -0.1523, -0.2168 /*...,*/, 0.1729, -0.9531, -0.3750},
		},
		{
			{0.0342, 0.0559, 0.3867 /*...,*/, 0.3457, -0.3340, 0.3379},
			{-0.1729, 0.7422, -0.4883 /*...,*/, 0.3008, -0.2578, 0.3301},
			{-0.5898, -0.8086, -0.4316 /*...,*/, -0.1162, 0.0991, -0.0903},
			{-0.4824, -0.0625, -0.4395 /*...,*/, 0.0815, -0.0654, 0.1074},
			{0.8750, 0.0251, 0.9258 /*...,*/, -0.2754, 0.2227, -0.2461},
		},
		/*...,*/
		{
			{0.6797, 1.3672, -1.6016 /*...,*/, 0.4902, -0.1162, 1.2734},
			{-2.0312, 1.9297, -2.3125 /*...,*/, 0.8945, -1.3828, 2.0156},
			{-1.4297, -0.1416, -0.1826 /*...,*/, 0.7852, -0.3730, 1.2266},
			{-0.8398, -1.8516 /*1.3828*/, 1.3359 /*...,*/, 0.7930, -0.7148, 1.6797},
			{0.3066, -0.7578, 0.6094 /*...,*/, 0.5352, -0.0806, 0.7344},
		},
		{
			{0.7461, 1.9219, 1.0156 /*...,*/, 0.5195, -0.3789, 0.4844},
			{-1.5547, 1.9453, 2.3594 /*...,*/, 0.5312, -0.3848, 0.5469},
			{-2.7031, -0.5625, 1.0938 /*...,*/, 0.6367, -0.4707, 0.6523},
			{-1.2344, -2.5312 /*-0.9336*/, -0.8789 /*...,*/, 0.6172, -0.4531, 0.6406},
			{1.7031 /*-2.2656*/, -2.21875, -2.4219 /*...,*/, 0.6523, -0.4863, 0.6719},
		},
		{
			{-1.3906, 0.7305, 1.3828 /*...,*/, 0.6250, -0.1553, 0.1797},
			{-0.8750, -0.5430, -0.5273 /*...,*/, 0.3613, -0.1455, -0.4844},
			{0.3789, -1.8438, -0.3574 /*...,*/, 0.3633, 0.0295, -0.1885},
			{1.2500, -0.6680, 0.2041 /*...,*/, 0.1641, -0.0151, -0.2617},
			{1.6641, 1.2891, 1.4219 /*...,*/, 0.1846, 0.1670, 0.0815},
		},
	}

	if actualXq, err = actualXq.Transpose(0, 1); err != nil { // from [5, 32, 128] -> shape=[32, 5, 128] (N_Heads, sequenceLength, HeadDim)
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedXqTranspose, expectedXqTransposeSize, actualXq, 4*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	expectedKeysTransposeDims0_1_Size := []int{32, 5, 128}
	expectedKeysTransposeDims0_1 := [][][]float32{
		{
			{-0.4492, -0.2100, 0.0050 /*...,*/, -0.0786, -0.5664, -0.0596},
			{-0.7930, -0.0115, 0.4707 /*...,*/, -0.1235, 0.2793, 0.5859},
			{-0.0728, -0.5859, 0.0253 /*...,*/, -0.2559, 0.1885, 0.3770},
			{-0.1855, 0.1553, 0.2432 /*...,*/, 0.0208, -0.1060, 0.0581},
			{-0.1934, 0.3555, 0.4238 /*...,*/, 0.2412, -0.1191, -0.1689},
		},
		{
			{1.3672, -0.6328, 1.0859 /*...,*/, -0.2773, 0.1670, 0.5586},
			{0.6992, 0.5859, 0.6680 /*...,*/, 0.3008, 0.2578, -0.1836},
			{-0.7344, 0.3379, -0.4355 /*...,*/, 0.2598, -0.1943, -0.6992},
			{-0.9570, 0.1357, -0.4805 /*...,*/, -0.0442, 0.3652, 0.4375},
			{0.2930, -0.9805, -0.7461 /*...,*/, 0.0679, -0.3086, -0.3652},
		},
		{
			{-0.0070, 0.3555, -0.2715 /*...,*/, 0.3477, -0.6680, 0.6133},
			{-0.3203, 0.0275, -0.1035 /*...,*/, -0.4883, 0.5547, -0.4395},
			{0.5859, -0.1592, 0.3105 /*...,*/, 1.8359, -1.7812, 1.6953},
			{-0.1113, -0.3750, 0.3281 /*...,*/, -0.8203, 0.8242, -0.7109},
			{-0.1147, 0.4883, -0.2500 /*...,*/, 1.8281, -1.7188, 1.6641},
		},
		/*...,*/
		{
			{-0.0145, -0.0635, 0.0061 /*...,*/, 0.9492, -0.4668, -0.4316},
			{0.0693, -0.1807, 0.1270 /*...,*/, 0.5703, -0.1943, -0.1182},
			{0.2168, -1.2656, 0.9102 /*...,*/, -0.7188, 1.3984, -1.0859},
			{0.3438, -0.1045, 0.1309 /*...,*/, 0.1104, 0.3027, -0.5469},
			{1.5469, 1.0078, -0.0767 /*...,*/, -1.0547, 1.9766, -1.3750}},
		{
			{0.2363, -0.6719, -0.5703 /*...,*/, 0.0659, -0.0957, 0.0703},
			{-0.0762, 0.3398, 0.4043 /*...,*/, -0.1084, 0.2070, -0.1045},
			{0.3496, -0.3477, 0.8125 /*...,*/, 0.1436, -0.3516, 0.1455},
			{-0.2441, 0.0464, 0.6211 /*...,*/, -0.2578, 0.3594, -0.2559},
			{-0.1011, 0.5859, 0.4043 /*...,*/, 0.0669, -0.3164, 0.0669},
		},
		{
			{-0.4453, 0.1816, 1.2969 /*...,*/, 0.1777, -0.3008, 0.2852},
			{-0.3691, -0.9453, -0.8477 /*...,*/, -0.5820, 0.6641, 0.3320},
			{0.0327, 0.1436, -0.0967 /*...,*/, 0.8359, -1.0625, -0.1123},
			{0.7148, 0.2949, 0.2891 /*...,*/, -0.5508, 0.6562, 0.1699},
			{-0.1963, -0.0811, 0.0260 /*...,*/, 0.9727, -0.9453, -0.2344},
		},
	}

	if actualKeys, err = actualKeys.Transpose(0, 1); err != nil { // from [5, 32, 128] -> shape=[32, 5, 128] (N_Heads, sequenceLength, HeadDim)
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedKeysTransposeDims0_1, expectedKeysTransposeDims0_1_Size, actualKeys, 4*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	expectedValuesTransposeSize := []int{32, 5, 128}
	expectedValuesTranspose := [][][]float32{
		{
			{-8.2397e-04, -2.0508e-02, -2.0752e-03 /*...,*/, -1.5137e-02, 4.6997e-03, -7.6294e-03},
			{-2.1744e-04, -8.4839e-03, -3.4332e-03 /*...,*/, 4.3945e-03, 6.7139e-03, -6.0425e-03},
			{5.1117e-04, -1.1673e-03, -9.9182e-04 /*...,*/, 7.4463e-03, -4.7913e-03, -1.7242e-03},
			{1.9226e-03, -8.9722e-03, 1.3611e-02 /*...,*/, -5.9509e-03, 8.7891e-03, -1.0376e-03},
			{-8.4839e-03, 6.8054e-03, 4.4250e-04 /*...,*/, 2.2583e-03, -4.6692e-03, 4.3640e-03},
		},
		{
			{-3.2959e-03, 1.6689e-04, -7.4768e-03 /*...,*/, 7.1716e-04, 1.0071e-02, -1.1475e-02},
			{-3.9673e-03, -1.0834e-03, 4.6082e-03 /*...,*/, 1.0620e-02, 1.0223e-03, -6.5613e-03},
			{-2.2888e-03, 2.9602e-03, 6.8665e-03 /*...,*/, -9.9487e-03, -2.1057e-03, -6.6528e-03},
			{-6.7139e-03, -1.4496e-03, 1.6556e-03 /*...,*/, 3.8757e-03, 7.0496e-03, 5.5542e-03},
			{-6.3477e-03, 4.9133e-03, -6.2866e-03 /*...,*/, -5.1270e-03, -6.9885e-03, -3.3875e-03},
		},
		{
			{1.6327e-03, -1.1841e-02, 8.3008e-03 /*...,*/, 6.5918e-03, 8.2779e-04, -7.2937e-03},
			{1.0498e-02, -6.1340e-03, -2.7466e-03 /*...,*/, 8.4229e-03, 5.3101e-03, -5.5237e-03},
			{-9.9487e-03, 3.3569e-03, 6.9275e-03 /*...,*/, -3.7193e-04, 3.7079e-03, 6.4697e-03},
			{3.3417e-03, -9.5825e-03, 3.1738e-03 /*...,*/, -8.5449e-03, -5.1575e-03, -7.8125e-03},
			{-2.1935e-05, -5.0049e-03, -2.8687e-03 /*...,*/, 3.9978e-03, -5.3711e-03, 6.1798e-04},
		},
		/*...,*/
		{
			{-7.0801e-03, -8.4839e-03, 5.1880e-03 /*...,*/, 2.6001e-02, 2.8809e-02, -3.4027e-03},
			{-2.0996e-02, 2.9785e-02, -7.2754e-02 /*...,*/, -1.0254e-01, -3.0396e-02, -1.9165e-02},
			{-8.4305e-04, -2.8687e-02, 3.2471e-02 /*...,*/, -2.2583e-03, -8.1787e-03, 2.2095e-02},
			{-1.4465e-02, -6.2256e-02, 4.9438e-03 /*...,*/, -5.6885e-02, -1.2329e-02, 1.8921e-02},
			{-8.3008e-03, -2.6001e-02, -2.7954e-02 /*...,*/, 2.5269e-02, -1.2207e-02, -2.6489e-02},
		},
		{
			{1.5991e-02, -1.4404e-02, -8.3008e-03 /*...,*/, 1.6251e-03, -2.8931e-02, -7.7515e-03},
			{-7.0496e-03, 1.9409e-02, -1.1475e-02 /*...,*/, -2.8992e-03, 2.0447e-03, -5.8289e-03},
			{1.3184e-02, -1.3428e-03, 1.4114e-04 /*...,*/, -2.7771e-03, -1.0620e-02, 3.1433e-03},
			{2.2888e-03, -8.3160e-04, -1.7643e-04 /*...,*/, -4.9438e-03, -1.0376e-02, -1.0910e-03},
			{-4.4861e-03, 6.2561e-03, 4.6387e-03 /*...,*/, -1.7047e-05, 5.3711e-03, 3.9864e-04},
		},
		{
			{-3.3112e-03, -5.2490e-03, 3.9673e-03 /*...,*/, 2.5635e-03, 7.4463e-03, 1.2512e-02},
			{1.4954e-03, 9.8877e-03, -1.1719e-02 /*...,*/, -1.3123e-03, -1.1108e-02, 6.4697e-03},
			{-9.2316e-04, -1.9775e-02, -3.2654e-03 /*...,*/, -8.4229e-03, 1.9165e-02, -5.0049e-03},
			{-2.5330e-03, 1.7548e-03, -1.0742e-02 /*...,*/, -1.2436e-03, -3.8757e-03, -1.4420e-03},
			{-1.7929e-03, 5.0354e-03, -1.9684e-03 /*...,*/, -4.8523e-03, 1.6357e-02, 8.9722e-03},
		},
	}

	if actualValues, err = actualValues.Transpose(0, 1); err != nil { // from [5, 32, 128] -> shape=[32, 5, 128] (N_Heads, sequenceLength, HeadDim)
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedValuesTranspose, expectedValuesTransposeSize, actualValues, common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	expectedKeysTransposeDims1_2_Size := []int{32, 128, 5}
	expectedKeysTransposeDims1_2 := [][][]float32{
		{
			{-0.4492, -0.7930, -0.0728, -0.1855, -0.1934},
			{-0.2100, -0.0115, -0.5859, 0.1553, 0.3555},
			{0.0050, 0.4707, 0.0253, 0.2432, 0.4238},
			/*...,*/
			{-0.0786, -0.1235, -0.2559, 0.0208, 0.2412},
			{-0.5664, 0.2793, 0.1885, -0.1060, -0.1191},
			{-0.0596, 0.5859, 0.3770, 0.0581, -0.1689},
		},
		{
			{1.3672, 0.6992, -0.7344, -0.9570, 0.2930},
			{-0.6328, 0.5859, 0.3379, 0.1357, -0.9805},
			{1.0859, 0.6680, -0.4355, -0.4805, -0.7461},
			/*...,*/
			{-0.2773, 0.3008, 0.2598, -0.0442, 0.0679},
			{0.1670, 0.2578, -0.1943, 0.3652, -0.3086},
			{0.5586, -0.1836, -0.6992, 0.4375, -0.3652},
		},
		{
			{-0.0070, -0.3203, 0.5859, -0.1113, -0.1147},
			{0.3555, 0.0275, -0.1592, -0.3750, 0.4883},
			{-0.2715, -0.1035, 0.3105, 0.3281, -0.2500},
			/*...,*/
			{0.3477, -0.4883, 1.8359, -0.8203, 1.8281},
			{-0.6680, 0.5547, -1.7812, 0.8242, -1.7188},
			{0.6133, -0.4395, 1.6953, -0.7109, 1.6641},
		},
		/*...,*/
		{
			{-0.0145, 0.0693, 0.2168, 0.3438, 1.5469},
			{-0.0635, -0.1807, -1.2656, -0.1045, 1.0078},
			{0.0061, 0.1270, 0.9102, 0.1309, -0.0767},
			/*...,*/
			{0.9492, 0.5703, -0.7188, 0.1104, -1.0547},
			{-0.4668, -0.1943, 1.3984, 0.3027, 1.9766},
			{-0.4316, -0.1182, -1.0859, -0.5469, -1.3750},
		},
		{
			{0.2363, -0.0762, 0.3496, -0.2441, -0.1011},
			{-0.6719, 0.3398, -0.3477, 0.0464, 0.5859},
			{-0.5703, 0.4043, 0.8125, 0.6211, 0.4043},
			/*...,*/
			{0.0659, -0.1084, 0.1436, -0.2578, 0.0669},
			{-0.0957, 0.2070, -0.3516, 0.3594, -0.3164},
			{0.0703, -0.1045, 0.1455, -0.2559, 0.0669},
		},
		{
			{-0.4453, -0.3691, 0.0327, 0.7148, -0.1963},
			{0.1816, -0.9453, 0.1436, 0.2949, -0.0811},
			{1.2969, -0.8477, -0.0967, 0.2891, 0.0260},
			/*...,*/
			{0.1777, -0.5820, 0.8359, -0.5508, 0.9727},
			{-0.3008, 0.6641, -1.0625, 0.6562, -0.9453},
			{0.2852, 0.3320, -0.1123, 0.1699, -0.2344},
		},
	}

	if actualKeys, err = actualKeys.Transpose(1, 2); err != nil { // from [32, 5, 128] -> shape=[32, 128, 5] (N_Heads, HeadDim, sequenceLength)
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedKeysTransposeDims1_2, expectedKeysTransposeDims1_2_Size, actualKeys, 4*common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	/*
		Goal in Python manner:
		scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)
	*/

	expectedScoresSize := []int{32, 5, 5}
	expectedScores := [][][]float32{
		{
			{2.9219e+00, 2.8281e+00, 1.2891e+00, 1.2344e+00, 2.2812e+00},
			{3.2500e+00, -5.0781e-01, 7.0703e-01, 2.1719e+00, 2.8281e+00},
			{3.5312e+00, -3.2422e-01, 6.6797e-01, 2.6250e+00, 3.2188e+00},
			{1.9219e+00, -9.2188e-01, -3.3789e-01, 2.3281e+00, 2.9219e+00},
			{4.3438e+00, 7.7344e-01, 1.2656e+00, 3.2656e+00, 4.0000e+00},
		},
		{
			{5.0625e+00, 1.0469e+00, 6.1719e-01, 1.0312e+00, 3.8867e-01},
			{1.0078e+00, 3.1836e-01, 2.4902e-01, 5.1953e-01, 3.7500e-01},
			{8.2422e-01, 6.3672e-01, 3.2422e-01, 4.6094e-01, 1.3477e-01},
			{7.1875e-01, 6.2109e-01, 4.3555e-01, 6.4062e-01, 3.6133e-01},
			{6.9141e-01, 8.9062e-01, 7.8906e-01, 1.0078e+00, 3.8867e-01},
		},
		{
			{5.9375e-01, 1.5625e-01, 7.9688e-01, 7.3730e-02, 9.0234e-01},
			{1.1953e+00, 4.8633e-01, 4.0039e-01, 3.5938e-01, 7.1094e-01},
			{8.6328e-01, 7.7344e-01, -3.2422e-01, 6.3672e-01, -3.5547e-01},
			{1.0078e+00, 5.6250e-01, 1.6309e-01, 4.5312e-01, 2.5391e-01},
			{8.1641e-01, 8.0469e-01, -2.7930e-01, 7.9688e-01, -6.6797e-01},
		},
		{
			{3.4531e+00, 2.1094e+00, 2.3906e+00, 2.2344e+00, 2.2969e+00},
			{3.9531e+00, 2.7344e+00, 3.2969e+00, 2.6562e+00, 3.1562e+00},
			{4.1562e+00, 3.0938e+00, 3.6250e+00, 2.9375e+00, 3.5156e+00},
			{4.2188e+00, 3.1094e+00, 3.5781e+00, 3.0938e+00, 3.5781e+00},
			{4.1875e+00, 3.0938e+00, 3.5625e+00, 3.3125e+00, 3.6875e+00},
		},
		{
			{7.7188e+00, 5.2734e-01, 7.9688e-01, 1.6328e+00, 1.0859e+00},
			{-2.1875e+00, 6.6406e-01, 5.0391e-01, 2.5000e+00, 3.2617e-01},
			{-2.7656e+00, 9.2969e-01, 6.8359e-01, 2.5938e+00, 2.4414e-01},
			{-2.2969e+00, 6.9141e-01, 6.3672e-01, 2.5469e+00, 2.9883e-01},
			{-2.5156e+00, 8.7109e-01, 7.3047e-01, 2.8750e+00, 5.0000e-01},
		},
		{
			{2.0625e+00, 8.1250e-01, 1.3828e+00, 7.0312e-01, 1.5391e+00},
			{-1.3672e+00, 1.1279e-01, 4.5117e-01, 6.9824e-02, 5.8203e-01},
			{-3.7305e-01, 5.7031e-01, 7.1094e-01, 5.4297e-01, 8.6719e-01},
			{-1.0781e+00, -4.7363e-02, 1.1377e-01, 2.1553e-04, 3.1250e-01},
			{8.7891e-02, 8.7891e-01, 1.0781e+00, 8.7891e-01, 1.0703e+00},
		},
		{
			{2.8438e+00, 3.6250e+00, 2.1250e+00, 2.5781e+00, 1.6875e+00},
			{1.0938e+00, 3.6250e+00, 2.0781e+00, 2.9219e+00, 1.1562e+00},
			{5.9766e-01, 1.6875e+00, 1.8906e+00, 1.7812e+00, 1.4062e+00},
			{7.1094e-01, 4.1797e-01, 6.7578e-01, 2.3906e+00, 7.7344e-01},
			{8.2812e-01, 1.0156e+00, 1.4141e+00, 1.6328e+00, 1.3828e+00},
		},
		{
			{5.8984e-01, 1.9238e-01, 6.1719e-01, -5.8594e-01, 8.2422e-01},
			{8.7109e-01, -3.0762e-02, 1.1865e-01, -1.2500e+00, 3.5156e-01},
			{9.4531e-01, 2.4121e-01, 2.5000e-01, -1.5000e+00, 6.2256e-02},
			{9.4922e-01, 5.4297e-01, 6.8359e-01, -1.5000e+00, -8.8867e-02},
			{1.0312e+00, 6.2109e-01, 1.1172e+00, -1.2891e+00, 5.4199e-02},
		},
		{
			{1.9238e-01, 2.4414e-01, 1.4355e-01, -2.1094e-01, -2.1851e-02},
			{-3.1641e-01, 3.6719e-01, 2.1289e-01, 6.7188e-01, 9.3359e-01},
			{4.6875e-01, 1.4375e+00, 9.9609e-01, 1.6094e+00, 1.9141e+00},
			{-9.0234e-01, -2.8516e-01, -1.0303e-01, 7.2266e-01, 8.5547e-01},
			{2.1562e+00, 3.1406e+00, 2.2344e+00, 2.8906e+00, 2.9219e+00},
		},
		{
			{1.6016e+00, 7.7734e-01, 1.5918e-01, -4.4727e-01, 9.7656e-01},
			{7.8516e-01, 1.4688e+00, 3.4375e-01, -1.2344e+00, 1.3516e+00},
			{5.9766e-01, 1.5234e+00, 6.5625e-01, -1.0938e+00, 1.1875e+00},
			{7.0312e-01, 1.4141e+00, 8.1641e-01, -1.3906e+00, 1.6094e+00},
			{7.4219e-01, 9.6094e-01, 1.1484e+00, -1.0703e+00, 2.0469e+00},
		},
		{
			{4.0625e+00, 2.5312e+00, 2.1094e+00, 2.0156e+00, 2.1875e+00},
			{1.2969e+00, 1.1641e+00, 6.3281e-01, 1.2344e+00, 1.0781e+00},
			{1.7891e+00, 1.8438e+00, 7.3047e-01, 1.9688e+00, 1.1016e+00},
			{5.2734e-01, 8.1250e-01, 5.1953e-01, 1.3125e+00, 8.5156e-01},
			{1.8047e+00, 1.8359e+00, 1.1719e+00, 2.3594e+00, 1.1016e+00},
		},
		{
			{-1.4062e+00, 4.6680e-01, 9.2188e-01, -2.6123e-02, 1.9922e+00},
			{-2.4375e+00, -2.2656e-01, 3.0664e-01, -4.9414e-01, 1.2188e+00},
			{-2.4219e+00, -2.0312e-01, 4.5898e-01, -6.6797e-01, 1.2891e+00},
			{-2.0938e+00, -8.6426e-02, 6.0938e-01, -6.6016e-01, 1.5469e+00},
			{-1.8984e+00, -1.7773e-01, 6.6797e-01, -6.5625e-01, 1.9141e+00},
		},
		{
			{3.0469e+00, 1.4453e+00, 5.0000e-01, 1.6562e+00, 1.1094e+00},
			{3.7812e+00, 4.0000e+00, 3.1875e+00, 3.2188e+00, 2.9844e+00},
			{3.9375e+00, 3.2656e+00, 4.2500e+00, 3.1875e+00, 4.0312e+00},
			{2.8438e+00, 3.2656e+00, 2.9844e+00, 3.9531e+00, 3.2188e+00},
			{3.4219e+00, 2.8906e+00, 3.7500e+00, 3.0469e+00, 3.7344e+00},
		},
		{
			{1.2031e+00, 3.5938e-01, -3.8086e-01, 2.9492e-01, -2.3340e-01},
			{3.1494e-02, 4.1406e-01, 1.2512e-02, -2.3047e-01, -4.3213e-02},
			{1.6504e-01, 6.2891e-01, -3.4668e-02, -1.9922e-01, -2.4414e-01},
			{2.6953e-01, 7.2266e-01, 1.2012e-01, -1.6406e-01, -3.0859e-01},
			{4.0820e-01, 6.7969e-01, 2.3340e-01, 3.7842e-02, -2.8516e-01},
		},
		{
			{1.3281e+00, 9.4531e-01, 1.1719e+00, 7.2266e-01, 1.3828e+00},
			{5.0391e-01, 3.4375e-01, -1.0840e-01, 9.0332e-02, 2.8516e-01},
			{5.7422e-01, 8.3984e-01, 3.6523e-01, 3.7500e-01, 3.8086e-01},
			{5.2344e-01, 9.0234e-01, 4.3945e-01, 3.7305e-01, 2.6562e-01},
			{7.1094e-01, 1.0078e+00, 1.0469e+00, 7.5000e-01, 4.7852e-01},
		},
		{
			{8.5156e-01, 4.7461e-01, 3.9258e-01, 4.7461e-01, 3.7109e-01},
			{1.5469e+00, 1.3203e+00, 6.1719e-01, 1.1797e+00, 4.5898e-01},
			{1.5703e+00, 1.4062e+00, 7.1094e-01, 1.3125e+00, 4.3164e-01},
			{1.5000e+00, 1.3359e+00, 8.6328e-01, 1.3828e+00, 5.6250e-01},
			{1.3906e+00, 1.2188e+00, 8.2422e-01, 1.3672e+00, 5.9375e-01},
		},
		{
			{8.5156e-01, 1.2656e+00, 2.0781e+00, 1.4219e+00, 1.9219e+00},
			{1.2031e+00, 1.4922e+00, 6.0547e-01, 7.2266e-01, 4.6875e-01},
			{5.6250e-01, 2.2344e+00, 1.4609e+00, 1.7656e+00, 1.2031e+00},
			{1.4219e+00, 2.2969e+00, 1.1562e+00, 1.4844e+00, 7.8516e-01},
			{2.9492e-01, 2.2656e+00, 1.7109e+00, 2.5156e+00, 1.1719e+00},
		},
		{
			{2.2500e+00, 3.6719e+00, 8.4375e-01, 2.6094e+00, 1.2578e+00},
			{4.4141e-01, 1.0000e+00, 1.1719e+00, 1.0391e+00, 1.4062e+00},
			{1.1963e-01, 8.7109e-01, 9.3750e-01, 9.0234e-01, 9.3359e-01},
			{2.9297e-01, 8.8281e-01, 1.2266e+00, 1.1797e+00, 1.0547e+00},
			{-9.0332e-03, 7.0312e-01, 1.0312e+00, 1.0781e+00, 9.3750e-01},
		},
		{
			{2.3750e+00, 3.7812e+00, 3.4688e+00, 3.5312e+00, 4.3438e+00},
			{1.8984e+00, 3.1562e+00, 3.2031e+00, 2.9219e+00, 3.4531e+00},
			{3.0312e+00, 3.6875e+00, 3.0469e+00, 3.4688e+00, 3.4062e+00},
			{7.8516e-01, 1.5391e+00, 2.3594e+00, 1.6328e+00, 1.9141e+00},
			{3.0469e+00, 3.3750e+00, 3.2188e+00, 4.0312e+00, 3.2969e+00},
		},
		{
			{3.3125e+00, 3.7109e-01, 3.4531e+00, 1.4922e+00, 3.4062e+00},
			{4.3750e+00, 4.1406e-01, 4.6250e+00, 1.8203e+00, 4.5312e+00},
			{4.8750e+00, 5.8594e-01, 5.0938e+00, 2.0938e+00, 5.0625e+00},
			{5.0312e+00, 6.7578e-01, 5.1875e+00, 2.3281e+00, 5.2500e+00},
			{5.0625e+00, 8.6719e-01, 5.2188e+00, 2.5000e+00, 5.4062e+00},
		},
		{
			{3.8906e+00, 2.2031e+00, 2.4531e+00, 1.4844e+00, 2.3594e+00},
			{1.1641e+00, 1.1328e+00, 1.1328e+00, 9.2969e-01, 1.3203e+00},
			{2.1289e-01, 8.6328e-01, 7.0703e-01, 7.2266e-01, 1.1406e+00},
			{1.6016e+00, 1.3516e+00, 1.5234e+00, 1.1562e+00, 1.7344e+00},
			{-3.0469e-01, 4.0625e-01, 5.1172e-01, 5.8984e-01, 1.0000e+00},
		},
		{
			{-2.2031e+00, 7.5000e-01, 4.4531e-01, 4.4531e-01, 4.6680e-01},
			{2.7539e-01, 7.9688e-01, -4.0039e-01, 1.7285e-01, -2.5391e-01},
			{5.3516e-01, 7.7344e-01, -4.9414e-01, 1.3281e-01, -5.9375e-01},
			{7.6172e-01, 8.5156e-01, -3.3594e-01, 2.2070e-01, -7.1875e-01},
			{7.1094e-01, 7.1875e-01, -2.0264e-02, 4.1406e-01, -6.7969e-01},
		},
		{
			{2.5469e+00, 1.8906e+00, 2.1719e+00, 1.6719e+00, 1.3906e+00},
			{-1.6602e-01, 1.4688e+00, 2.1250e+00, 3.0000e+00, 1.9922e+00},
			{-5.9766e-01, -2.8906e-01, 2.0938e+00, 2.6094e+00, 3.2500e+00},
			{-4.5508e-01, -1.5391e+00, 4.5312e-01, 1.9688e+00, 2.5000e+00},
			{-9.7168e-02, -2.3125e+00, 1.4941e-01, 4.6094e-01, 2.6250e+00},
		},
		{
			{2.8125e+00, 1.7266e+00, 1.8652e-01, 1.5547e+00, -5.3125e-01},
			{8.3594e-01, 1.3984e+00, 4.0625e-01, 1.9297e+00, -3.0469e-01},
			{9.3359e-01, 6.6797e-01, 2.3594e+00, 2.0625e+00, 2.5625e+00},
			{1.2188e+00, 7.7734e-01, -2.5977e-01, 1.7578e+00, 2.8320e-01},
			{1.2109e+00, 1.4453e-01, -1.5723e-01, 1.4648e-01, 2.4531e+00},
		},
		{
			{2.4688e+00, 1.4062e+00, -2.6953e-01, 1.8438e+00, 1.4258e-01},
			{1.3281e+00, 3.0859e-01, -1.7422e+00, 4.0820e-01, -1.6562e+00},
			{1.6250e+00, 7.4219e-01, -1.6719e+00, 1.9238e-01, -1.9531e+00},
			{1.5156e+00, 1.0781e+00, -1.3984e+00, 2.8320e-01, -1.9766e+00},
			{1.1953e+00, 1.2109e+00, -1.1094e+00, 8.3984e-01, -1.7656e+00},
		},
		{
			{6.9922e-01, 1.8750e-01, -2.2656e-01, 6.2256e-02, -1.6309e-01},
			{1.5781e+00, 1.6797e+00, 1.1797e+00, 1.2578e+00, 1.1328e+00},
			{1.5703e+00, 1.7812e+00, 1.2734e+00, 1.3594e+00, 1.1172e+00},
			{1.6094e+00, 1.8047e+00, 1.3750e+00, 1.4688e+00, 1.2188e+00},
			{1.4844e+00, 1.6250e+00, 1.1953e+00, 1.4219e+00, 1.1953e+00},
		},
		{
			{-1.3828e+00, 9.3359e-01, 1.8262e-01, 1.3770e-01, -8.2520e-02},
			{-1.4922e+00, 8.8281e-01, 9.5703e-01, 8.0469e-01, 1.1953e+00},
			{-7.0703e-01, 1.7656e+00, 1.9531e+00, 2.0156e+00, 2.3125e+00},
			{-7.3438e-01, 1.3672e+00, 1.7578e+00, 1.7578e+00, 2.3438e+00},
			{-1.7773e-01, 1.6797e+00, 2.2031e+00, 2.7188e+00, 3.4062e+00},
		},
		{
			{2.0312e-01, 4.1406e-01, 1.5391e+00, 3.4570e-01, 1.0156e+00},
			{9.0234e-01, 6.8359e-01, 1.7266e+00, 2.0605e-01, 1.5938e+00},
			{1.4062e+00, 8.4375e-01, 1.6328e+00, -1.6016e-01, 1.8594e+00},
			{2.0000e+00, 1.0625e+00, 1.0938e+00, -1.7773e-01, 1.5938e+00},
			{2.0000e+00, 1.1484e+00, 8.1641e-01, 6.9336e-02, 1.4062e+00},
		},
		{
			{4.4062e+00, 1.2656e+00, 1.2891e+00, 1.4453e+00, 1.6484e+00},
			{1.1016e+00, 1.5234e-01, 1.0938e+00, 1.1406e+00, 1.5391e+00},
			{2.0625e+00, 8.5547e-01, 1.6094e+00, 1.7891e+00, 2.1094e+00},
			{1.4453e+00, 7.8516e-01, 1.6797e+00, 1.6875e+00, 2.0625e+00},
			{2.4062e+00, 1.1875e+00, 1.9219e+00, 2.1094e+00, 2.3906e+00},
		},
		{
			{1.8438e+00, 7.6953e-01, -1.5156e+00, -3.4180e-01, -1.9297e+00},
			{5.4688e+00, 1.9141e+00, -6.8750e-01, 6.2891e-01, -2.5312e+00},
			{5.8750e+00, 2.6094e+00, 1.6016e+00, 1.8438e+00, 7.8125e-02},
			{5.4688e+00, 2.5312e+00, 1.6562e+00, 1.6719e+00, -1.0312e+00},
			{5.6250e+00, 2.4688e+00, 2.4375e+00, 1.8984e+00, 2.1250e+00},
		},
		{
			{-2.5781e+00, -3.9258e-01, 1.1172e+00, -1.4688e+00, 9.4727e-02},
			{-2.1250e+00, -6.1328e-01, 8.0078e-01, -2.2812e+00, -4.8828e-01},
			{-1.9688e+00, -5.0000e-01, 1.1875e+00, -2.6250e+00, -3.9453e-01},
			{-1.2578e+00, -6.0938e-01, 1.0625e+00, -2.7656e+00, -5.2344e-01},
			{-8.3984e-01, -5.9766e-01, 1.0859e+00, -2.5312e+00, -4.6680e-01},
		},
		{
			{1.0812e+01, 2.4062e+00, 1.5234e+00, 1.7812e+00, 2.0312e+00},
			{1.2891e+00, 1.0469e+00, 4.0039e-01, 7.0312e-01, 7.7344e-01},
			{9.7656e-01, 1.8203e+00, 4.1211e-01, 1.5938e+00, 7.8906e-01},
			{4.3555e-01, 6.8359e-01, 5.6641e-01, 1.0469e+00, 8.2812e-01},
			{6.8359e-01, 5.3516e-01, 5.2344e-01, 1.8750e+00, 9.2969e-01},
		},
	}

	xqMatMulKeys, err := ml.MatMul(actualXq, actualKeys) // matmul([32,5,128], [32,128,5]) -> shape=[32,5,5] (N_Heads, sequenceLength, sequenceLength)
	if err != nil {
		t.Fatal(err)
	}
	actualScores, err := ml.DivToScalar(xqMatMulKeys, dtype.BFloat16fromFloat32(float32(math.Sqrt(float64(attention.HeadDim))))) // shape=[32,5,5]
	if err != nil {
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedScores, expectedScoresSize, actualScores, 30*common.THRESHOLD_BF16, false); err != nil {
		t.Fatal(err)
	}
	negInf := float32(math.Inf(-1))

	if mask != nil {
		expectedScoresPlusMaskSize := []int{32, 5, 5}
		expectedScoresPlusMask := [][][]float32{
			{
				{2.9219e+00, negInf, negInf, negInf, negInf},
				{3.2500e+00, -5.0781e-01, negInf, negInf, negInf},
				{3.5312e+00, -3.2422e-01, 6.6797e-01, negInf, negInf},
				{1.9219e+00, -9.2188e-01, -3.3789e-01, 2.3281e+00, negInf},
				{4.3438e+00, 7.7344e-01, 1.2656e+00, 3.2656e+00, 4.0000e+00},
			},
			{
				{5.0625e+00, negInf, negInf, negInf, negInf},
				{1.0078e+00, 3.1836e-01, negInf, negInf, negInf},
				{8.2422e-01, 6.3672e-01, 3.2422e-01, negInf, negInf},
				{7.1875e-01, 6.2109e-01, 4.3555e-01, 6.4062e-01, negInf},
				{6.9141e-01, 8.9062e-01, 7.8906e-01, 1.0078e+00, 3.8867e-01},
			},
			{
				{5.9375e-01, negInf, negInf, negInf, negInf},
				{1.1953e+00, 4.8633e-01, negInf, negInf, negInf},
				{8.6328e-01, 7.7344e-01, -3.2422e-01, negInf, negInf},
				{1.0078e+00, 5.6250e-01, 1.6309e-01, 4.5312e-01, negInf},
				{8.1641e-01, 8.0469e-01, -2.7930e-01, 7.9688e-01, -6.6797e-01},
			},
			{
				{3.4531e+00, negInf, negInf, negInf, negInf},
				{3.9531e+00, 2.7344e+00, negInf, negInf, negInf},
				{4.1562e+00, 3.0938e+00, 3.6250e+00, negInf, negInf},
				{4.2188e+00, 3.1094e+00, 3.5781e+00, 3.0938e+00, negInf},
				{4.1875e+00, 3.0938e+00, 3.5625e+00, 3.3125e+00, 3.6875e+00},
			},
			{
				{7.7188e+00, negInf, negInf, negInf, negInf},
				{-2.1875e+00, 6.6406e-01, negInf, negInf, negInf},
				{-2.7656e+00, 9.2969e-01, 6.8359e-01, negInf, negInf},
				{-2.2969e+00, 6.9141e-01, 6.3672e-01, 2.5469e+00, negInf},
				{-2.5156e+00, 8.7109e-01, 7.3047e-01, 2.8750e+00, 5.0000e-01},
			},
			{
				{2.0625e+00, negInf, negInf, negInf, negInf},
				{-1.3672e+00, 1.1279e-01, negInf, negInf, negInf},
				{-3.7305e-01, 5.7031e-01, 7.1094e-01, negInf, negInf},
				{-1.0781e+00, -4.7363e-02, 1.1377e-01, 2.1553e-04, negInf},
				{8.7891e-02, 8.7891e-01, 1.0781e+00, 8.7891e-01, 1.0703e+00},
			},
			{
				{2.8438e+00, negInf, negInf, negInf, negInf},
				{1.0938e+00, 3.6250e+00, negInf, negInf, negInf},
				{5.9766e-01, 1.6875e+00, 1.8906e+00, negInf, negInf},
				{7.1094e-01, 4.1797e-01, 6.7578e-01, 2.3906e+00, negInf},
				{8.2812e-01, 1.0156e+00, 1.4141e+00, 1.6328e+00, 1.3828e+00},
			},
			{
				{5.8984e-01, negInf, negInf, negInf, negInf},
				{8.7109e-01, -3.0762e-02, negInf, negInf, negInf},
				{9.4531e-01, 2.4121e-01, 2.5000e-01, negInf, negInf},
				{9.4922e-01, 5.4297e-01, 6.8359e-01, -1.5000e+00, negInf},
				{1.0312e+00, 6.2109e-01, 1.1172e+00, -1.2891e+00, 5.4199e-02},
			},
			{
				{1.9238e-01, negInf, negInf, negInf, negInf},
				{-3.1641e-01, 3.6719e-01, negInf, negInf, negInf},
				{4.6875e-01, 1.4375e+00, 9.9609e-01, negInf, negInf},
				{-9.0234e-01, -2.8516e-01, -1.0303e-01, 7.2266e-01, negInf},
				{2.1562e+00, 3.1406e+00, 2.2344e+00, 2.8906e+00, 2.9219e+00},
			},
			{
				{1.6016e+00, negInf, negInf, negInf, negInf},
				{7.8516e-01, 1.4688e+00, negInf, negInf, negInf},
				{5.9766e-01, 1.5234e+00, 6.5625e-01, negInf, negInf},
				{7.0312e-01, 1.4141e+00, 8.1641e-01, -1.3906e+00, negInf},
				{7.4219e-01, 9.6094e-01, 1.1484e+00, -1.0703e+00, 2.0469e+00},
			},
			{
				{4.0625e+00, negInf, negInf, negInf, negInf},
				{1.2969e+00, 1.1641e+00, negInf, negInf, negInf},
				{1.7891e+00, 1.8438e+00, 7.3047e-01, negInf, negInf},
				{5.2734e-01, 8.1250e-01, 5.1953e-01, 1.3125e+00, negInf},
				{1.8047e+00, 1.8359e+00, 1.1719e+00, 2.3594e+00, 1.1016e+00},
			},
			{
				{-1.4062e+00, negInf, negInf, negInf, negInf},
				{-2.4375e+00, -2.2656e-01, negInf, negInf, negInf},
				{-2.4219e+00, -2.0312e-01, 4.5898e-01, negInf, negInf},
				{-2.0938e+00, -8.6426e-02, 6.0938e-01, -6.6016e-01, negInf},
				{-1.8984e+00, -1.7773e-01, 6.6797e-01, -6.5625e-01, 1.9141e+00},
			},
			{
				{3.0469e+00, negInf, negInf, negInf, negInf},
				{3.7812e+00, 4.0000e+00, negInf, negInf, negInf},
				{3.9375e+00, 3.2656e+00, 4.2500e+00, negInf, negInf},
				{2.8438e+00, 3.2656e+00, 2.9844e+00, 3.9531e+00, negInf},
				{3.4219e+00, 2.8906e+00, 3.7500e+00, 3.0469e+00, 3.7344e+00},
			},
			{
				{1.2031e+00, negInf, negInf, negInf, negInf},
				{3.1494e-02, 4.1406e-01, negInf, negInf, negInf},
				{1.6504e-01, 6.2891e-01, -3.4668e-02, negInf, negInf},
				{2.6953e-01, 7.2266e-01, 1.2012e-01, -1.6406e-01, negInf},
				{4.0820e-01, 6.7969e-01, 2.3340e-01, 3.7842e-02, -2.8516e-01},
			},
			{
				{1.3281e+00, negInf, negInf, negInf, negInf},
				{5.0391e-01, 3.4375e-01, negInf, negInf, negInf},
				{5.7422e-01, 8.3984e-01, 3.6523e-01, negInf, negInf},
				{5.2344e-01, 9.0234e-01, 4.3945e-01, 3.7305e-01, negInf},
				{7.1094e-01, 1.0078e+00, 1.0469e+00, 7.5000e-01, 4.7852e-01},
			},
			{
				{8.5156e-01, negInf, negInf, negInf, negInf},
				{1.5469e+00, 1.3203e+00, negInf, negInf, negInf},
				{1.5703e+00, 1.4062e+00, 7.1094e-01, negInf, negInf},
				{1.5000e+00, 1.3359e+00, 8.6328e-01, 1.3828e+00, negInf},
				{1.3906e+00, 1.2188e+00, 8.2422e-01, 1.3672e+00, 5.9375e-01},
			},
			{
				{8.5156e-01, negInf, negInf, negInf, negInf},
				{1.2031e+00, 1.4922e+00, negInf, negInf, negInf},
				{5.6250e-01, 2.2344e+00, 1.4609e+00, negInf, negInf},
				{1.4219e+00, 2.2969e+00, 1.1562e+00, 1.4844e+00, negInf},
				{2.9492e-01, 2.2656e+00, 1.7109e+00, 2.5156e+00, 1.1719e+00},
			},
			{
				{2.2500e+00, negInf, negInf, negInf, negInf},
				{4.4141e-01, 1.0000e+00, negInf, negInf, negInf},
				{1.1963e-01, 8.7109e-01, 9.3750e-01, negInf, negInf},
				{2.9297e-01, 8.8281e-01, 1.2266e+00, 1.1797e+00, negInf},
				{-9.0332e-03, 7.0312e-01, 1.0312e+00, 1.0781e+00, 9.3750e-01},
			},
			{
				{2.3750e+00, negInf, negInf, negInf, negInf},
				{1.8984e+00, 3.1562e+00, negInf, negInf, negInf},
				{3.0312e+00, 3.6875e+00, 3.0469e+00, negInf, negInf},
				{7.8516e-01, 1.5391e+00, 2.3594e+00, 1.6328e+00, negInf},
				{3.0469e+00, 3.3750e+00, 3.2188e+00, 4.0312e+00, 3.2969e+00},
			},
			{
				{3.3125e+00, negInf, negInf, negInf, negInf},
				{4.3750e+00, 4.1406e-01, negInf, negInf, negInf},
				{4.8750e+00, 5.8594e-01, 5.0938e+00, negInf, negInf},
				{5.0312e+00, 6.7578e-01, 5.1875e+00, 2.3281e+00, negInf},
				{5.0625e+00, 8.6719e-01, 5.2188e+00, 2.5000e+00, 5.4062e+00},
			},

			{{3.8906e+00, negInf, negInf, negInf, negInf},
				{1.1641e+00, 1.1328e+00, negInf, negInf, negInf},
				{2.1289e-01, 8.6328e-01, 7.0703e-01, negInf, negInf},
				{1.6016e+00, 1.3516e+00, 1.5234e+00, 1.1562e+00, negInf},
				{-3.0469e-01, 4.0625e-01, 5.1172e-01, 5.8984e-01, 1.0000e+00},
			},
			{
				{-2.2031e+00, negInf, negInf, negInf, negInf},
				{2.7539e-01, 7.9688e-01, negInf, negInf, negInf},
				{5.3516e-01, 7.7344e-01, -4.9414e-01, negInf, negInf},
				{7.6172e-01, 8.5156e-01, -3.3594e-01, 2.2070e-01, negInf},
				{7.1094e-01, 7.1875e-01, -2.0264e-02, 4.1406e-01, -6.7969e-01},
			},
			{
				{2.5469e+00, negInf, negInf, negInf, negInf},
				{-1.6602e-01, 1.4688e+00, negInf, negInf, negInf},
				{-5.9766e-01, -2.8906e-01, 2.0938e+00, negInf, negInf},
				{-4.5508e-01, -1.5391e+00, 4.5312e-01, 1.9688e+00, negInf},
				{-9.7168e-02, -2.3125e+00, 1.4941e-01, 4.6094e-01, 2.6250e+00},
			},
			{
				{2.8125e+00, negInf, negInf, negInf, negInf},
				{8.3594e-01, 1.3984e+00, negInf, negInf, negInf},
				{9.3359e-01, 6.6797e-01, 2.3594e+00, negInf, negInf},
				{1.2188e+00, 7.7734e-01, -2.5977e-01, 1.7578e+00, negInf},
				{1.2109e+00, 1.4453e-01, -1.5723e-01, 1.4648e-01, 2.4531e+00},
			},
			{
				{2.4688e+00, negInf, negInf, negInf, negInf},
				{1.3281e+00, 3.0859e-01, negInf, negInf, negInf},
				{1.6250e+00, 7.4219e-01, -1.6719e+00, negInf, negInf},
				{1.5156e+00, 1.0781e+00, -1.3984e+00, 2.8320e-01, negInf},
				{1.1953e+00, 1.2109e+00, -1.1094e+00, 8.3984e-01, -1.7656e+00},
			},
			{
				{6.9922e-01, negInf, negInf, negInf, negInf},
				{1.5781e+00, 1.6797e+00, negInf, negInf, negInf},
				{1.5703e+00, 1.7812e+00, 1.2734e+00, negInf, negInf},
				{1.6094e+00, 1.8047e+00, 1.3750e+00, 1.4688e+00, negInf},
				{1.4844e+00, 1.6250e+00, 1.1953e+00, 1.4219e+00, 1.1953e+00},
			},
			{
				{-1.3828e+00, negInf, negInf, negInf, negInf},
				{-1.4922e+00, 8.8281e-01, negInf, negInf, negInf},
				{-7.0703e-01, 1.7656e+00, 1.9531e+00, negInf, negInf},
				{-7.3438e-01, 1.3672e+00, 1.7578e+00, 1.7578e+00, negInf},
				{-1.7773e-01, 1.6797e+00, 2.2031e+00, 2.7188e+00, 3.4062e+00},
			},

			{{2.0312e-01, negInf, negInf, negInf, negInf},
				{9.0234e-01, 6.8359e-01, negInf, negInf, negInf},
				{1.4062e+00, 8.4375e-01, 1.6328e+00, negInf, negInf},
				{2.0000e+00, 1.0625e+00, 1.0938e+00, -1.7773e-01, negInf},
				{2.0000e+00, 1.1484e+00, 8.1641e-01, 6.9336e-02, 1.4062e+00},
			},
			{
				{4.4062e+00, negInf, negInf, negInf, negInf},
				{1.1016e+00, 1.5234e-01, negInf, negInf, negInf},
				{2.0625e+00, 8.5547e-01, 1.6094e+00, negInf, negInf},
				{1.4453e+00, 7.8516e-01, 1.6797e+00, 1.6875e+00, negInf},
				{2.4062e+00, 1.1875e+00, 1.9219e+00, 2.1094e+00, 2.3906e+00},
			},
			{
				{1.8438e+00, negInf, negInf, negInf, negInf},
				{5.4688e+00, 1.9141e+00, negInf, negInf, negInf},
				{5.8750e+00, 2.6094e+00, 1.6016e+00, negInf, negInf},
				{5.4688e+00, 2.5312e+00, 1.6562e+00, 1.6719e+00, negInf},
				{5.6250e+00, 2.4688e+00, 2.4375e+00, 1.8984e+00, 2.1250e+00},
			},
			{
				{-2.5781e+00, negInf, negInf, negInf, negInf},
				{-2.1250e+00, -6.1328e-01, negInf, negInf, negInf},
				{-1.9688e+00, -5.0000e-01, 1.1875e+00, negInf, negInf},
				{-1.2578e+00, -6.0938e-01, 1.0625e+00, -2.7656e+00, negInf},
				{-8.3984e-01, -5.9766e-01, 1.0859e+00, -2.5312e+00, -4.6680e-01},
			},
			{
				{1.0812e+01, negInf, negInf, negInf, negInf},
				{1.2891e+00, 1.0469e+00, negInf, negInf, negInf},
				{9.7656e-01, 1.8203e+00, 4.1211e-01, negInf, negInf},
				{4.3555e-01, 6.8359e-01, 5.6641e-01, 1.0469e+00, negInf},
				{6.8359e-01, 5.3516e-01, 5.2344e-01, 1.8750e+00, 9.2969e-01},
			},
		}
		if actualScores, err = ml.Add(actualScores, mask); err != nil { // shape=[32,5,5]
			t.Fatal(err)
		}
		if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedScoresPlusMask, expectedScoresPlusMaskSize, actualScores, 30*common.THRESHOLD_BF16, false); err != nil {
			t.Fatal(err)
		}
	}

	/*
		Goal in Python manner:
		scores = F.softmax(scores.float(), dim=-1).type_as(xq)
	*/

	expectedScoresSoftmaxSize := []int{32, 5, 5}
	expectedScoresSoftmax := [][][]float32{
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.9766, 0.0228, 0.0000, 0.0000, 0.0000},
			{0.9258, 0.0197, 0.0530, 0.0000, 0.0000},
			{0.3750, 0.0219, 0.0391, 0.5625, 0.0000},
			{0.4707, 0.0132, 0.0217, 0.1602, 0.3340},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.6641, 0.3340, 0.0000, 0.0000, 0.0000},
			{0.4102, 0.3398, 0.2490, 0.0000, 0.0000},
			{0.2793, 0.2539, 0.2100, 0.2578, 0.0000},
			{0.1836, 0.2246, 0.2031, 0.2520, 0.1357},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.6719, 0.3301, 0.0000, 0.0000, 0.0000},
			{0.4512, 0.4121, 0.1377, 0.0000, 0.0000},
			{0.3789, 0.2422, 0.1621, 0.2168, 0.0000},
			{0.2832, 0.2793, 0.0947, 0.2773, 0.0640},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.7734, 0.2285, 0.0000, 0.0000, 0.0000},
			{0.5156, 0.1787, 0.3047, 0.0000, 0.0000},
			{0.4590, 0.1514, 0.2412, 0.1484, 0.0000},
			{0.3457, 0.1157, 0.1846, 0.1445, 0.2100},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.0547, 0.9453, 0.0000, 0.0000, 0.0000},
			{0.0137, 0.5547, 0.4336, 0.0000, 0.0000},
			{0.0060, 0.1191, 0.1128, 0.7617, 0.0000},
			{0.0034, 0.1001, 0.0869, 0.7422, 0.0688},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.1855, 0.8164, 0.0000, 0.0000, 0.0000},
			{0.1533, 0.3945, 0.4531, 0.0000, 0.0000},
			{0.0996, 0.2793, 0.3281, 0.2930, 0.0000},
			{0.0928, 0.2051, 0.2500, 0.2051, 0.2480},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.0737, 0.9258, 0.0000, 0.0000, 0.0000},
			{0.1309, 0.3906, 0.4785, 0.0000, 0.0000},
			{0.1240, 0.0923, 0.1196, 0.6641, 0.0000},
			{0.1250, 0.1514, 0.2256, 0.2793, 0.2178},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.7109, 0.2891, 0.0000, 0.0000, 0.0000},
			{0.5000, 0.2480, 0.2500, 0.0000, 0.0000},
			{0.3965, 0.2637, 0.3047, 0.0342, 0.0000},
			{0.3105, 0.2051, 0.3379, 0.0304, 0.1167},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.3359, 0.6641, 0.0000, 0.0000, 0.0000},
			{0.1875, 0.4941, 0.3184, 0.0000, 0.0000},
			{0.0986, 0.1826, 0.2188, 0.5000, 0.0000},
			{0.1113, 0.2969, 0.1201, 0.2314, 0.2393},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.3359, 0.6641, 0.0000, 0.0000, 0.0000},
			{0.2178, 0.5508, 0.2314, 0.0000, 0.0000},
			{0.2334, 0.4766, 0.2617, 0.0288, 0.0000},
			{0.1318, 0.1641, 0.1973, 0.0215, 0.4863},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.5312, 0.4668, 0.0000, 0.0000, 0.0000},
			{0.4160, 0.4395, 0.1445, 0.0000, 0.0000},
			{0.1816, 0.2412, 0.1797, 0.3984, 0.0000},
			{0.2080, 0.2148, 0.1108, 0.3633, 0.1030},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.0986, 0.9023, 0.0000, 0.0000, 0.0000},
			{0.0356, 0.3281, 0.6367, 0.0000, 0.0000},
			{0.0364, 0.2695, 0.5430, 0.1523, 0.0000},
			{0.0146, 0.0815, 0.1904, 0.0508, 0.6641},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.4453, 0.5547, 0.0000, 0.0000, 0.0000},
			{0.3477, 0.1777, 0.4746, 0.0000, 0.0000},
			{0.1494, 0.2275, 0.1719, 0.4512, 0.0000},
			{0.1992, 0.1167, 0.2754, 0.1367, 0.2715},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.4062, 0.5938, 0.0000, 0.0000, 0.0000},
			{0.2930, 0.4668, 0.2402, 0.0000, 0.0000},
			{0.2451, 0.3848, 0.2109, 0.1592, 0.0000},
			{0.2305, 0.3027, 0.1934, 0.1592, 0.1152},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.5391, 0.4609, 0.0000, 0.0000, 0.0000},
			{0.3203, 0.4180, 0.2598, 0.0000, 0.0000},
			{0.2354, 0.3438, 0.2168, 0.2031, 0.0000},
			{0.1797, 0.2412, 0.2500, 0.1865, 0.1426},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.5547, 0.4434, 0.0000, 0.0000, 0.0000},
			{0.4395, 0.3730, 0.1865, 0.0000, 0.0000},
			{0.3066, 0.2598, 0.1621, 0.2715, 0.0000},
			{0.2598, 0.2197, 0.1475, 0.2539, 0.1177},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.4277, 0.5703, 0.0000, 0.0000, 0.0000},
			{0.1138, 0.6055, 0.2793, 0.0000, 0.0000},
			{0.1914, 0.4590, 0.1465, 0.2031, 0.0000},
			{0.0417, 0.3008, 0.1719, 0.3848, 0.1006},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.3633, 0.6367, 0.0000, 0.0000, 0.0000},
			{0.1855, 0.3945, 0.4199, 0.0000, 0.0000},
			{0.1289, 0.2324, 0.3281, 0.3125, 0.0000},
			{0.0874, 0.1787, 0.2480, 0.2598, 0.2256},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.2217, 0.7773, 0.0000, 0.0000, 0.0000},
			{0.2539, 0.4883, 0.2578, 0.0000, 0.0000},
			{0.0972, 0.2070, 0.4688, 0.2266, 0.0000},
			{0.1328, 0.1846, 0.1572, 0.3555, 0.1699},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.9805, 0.0187, 0.0000, 0.0000, 0.0000},
			{0.4434, 0.0061, 0.5508, 0.0000, 0.0000},
			{0.4453, 0.0057, 0.5195, 0.0298, 0.0000},
			{0.2715, 0.0041, 0.3184, 0.0210, 0.3848},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.5078, 0.4922, 0.0000, 0.0000, 0.0000},
			{0.2197, 0.4199, 0.3594, 0.0000, 0.0000},
			{0.2988, 0.2324, 0.2773, 0.1914, 0.0000},
			{0.0874, 0.1777, 0.1982, 0.2139, 0.3223},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.3730, 0.6289, 0.0000, 0.0000, 0.0000},
			{0.3809, 0.4824, 0.1357, 0.0000, 0.0000},
			{0.3320, 0.3633, 0.1108, 0.1934, 0.0000},
			{0.2871, 0.2891, 0.1387, 0.2139, 0.0713},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.1631, 0.8359, 0.0000, 0.0000, 0.0000},
			{0.0583, 0.0796, 0.8633, 0.0000, 0.0000},
			{0.0664, 0.0223, 0.1641, 0.7461, 0.0000},
			{0.0518, 0.0056, 0.0659, 0.0903, 0.7852},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.3633, 0.6367, 0.0000, 0.0000, 0.0000},
			{0.1689, 0.1289, 0.7031, 0.0000, 0.0000},
			{0.2793, 0.1797, 0.0635, 0.4785, 0.0000},
			{0.1846, 0.0635, 0.0471, 0.0640, 0.6406},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.7344, 0.2656, 0.0000, 0.0000, 0.0000},
			{0.6875, 0.2852, 0.0255, 0.0000, 0.0000},
			{0.5039, 0.3242, 0.0272, 0.1465, 0.0000},
			{0.3496, 0.3535, 0.0349, 0.2441, 0.0181},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.4746, 0.5234, 0.0000, 0.0000, 0.0000},
			{0.3359, 0.4141, 0.2500, 0.0000, 0.0000},
			{0.2578, 0.3145, 0.2041, 0.2246, 0.0000},
			{0.2178, 0.2500, 0.1631, 0.2051, 0.1631},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.0850, 0.9141, 0.0000, 0.0000, 0.0000},
			{0.0369, 0.4375, 0.5273, 0.0000, 0.0000},
			{0.0300, 0.2451, 0.3633, 0.3633, 0.0000},
			{0.0138, 0.0884, 0.1494, 0.2500, 0.4980},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.5547, 0.4453, 0.0000, 0.0000, 0.0000},
			{0.3535, 0.2021, 0.4434, 0.0000, 0.0000},
			{0.5234, 0.2051, 0.2119, 0.0593, 0.0000},
			{0.4121, 0.1758, 0.1260, 0.0596, 0.2275},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.7227, 0.2793, 0.0000, 0.0000, 0.0000},
			{0.5156, 0.1543, 0.3281, 0.0000, 0.0000},
			{0.2471, 0.1279, 0.3125, 0.3145, 0.0000},
			{0.2754, 0.0811, 0.1689, 0.2041, 0.2715},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.9727, 0.0278, 0.0000, 0.0000, 0.0000},
			{0.9492, 0.0364, 0.0132, 0.0000, 0.0000},
			{0.9102, 0.0483, 0.0201, 0.0204, 0.0000},
			{0.8789, 0.0374, 0.0364, 0.0211, 0.0265},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.1807, 0.8203, 0.0000, 0.0000, 0.0000},
			{0.0347, 0.1504, 0.8164, 0.0000, 0.0000},
			{0.0752, 0.1436, 0.7656, 0.0166, 0.0000},
			{0.0928, 0.1182, 0.6367, 0.0171, 0.1348},
		},
		{
			{1.0000, 0.0000, 0.0000, 0.0000, 0.0000},
			{0.5586, 0.4395, 0.0000, 0.0000, 0.0000},
			{0.2559, 0.5977, 0.1465, 0.0000, 0.0000},
			{0.1904, 0.2432, 0.2168, 0.3496, 0.0000},
			{0.1377, 0.1182, 0.1172, 0.4512, 0.1758},
		},
	}

	actualScores32, err := actualScores.ToFloat32() // shape=[32,5,5] dtype=DT_F32
	if err != nil {
		t.Fatal(err)
	}
	if actualScores32, err = ml.Softmax(actualScores32, len(actualScores32.Size)-1); err != nil { // shape=[32,5,5] dtype=DT_F32
		t.Fatal(err)
	}
	if actualScores, err = actualScores32.ToBFloat16(); err != nil { // shape=[32,5,5] (N_Heads, sequenceLength, sequenceLength) dtype=DT_BF16
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedScoresSoftmax, expectedScoresSoftmaxSize, actualScores, 2*common.THRESHOLD_BF16, false); err != nil {
		t.Fatal(err)
	}

	/*
		Goal in Python manner:
		output = torch.matmul(scores, values)
		output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)
	*/

	expectedOutputBeforeWeightsSize := []int{5, 4096}
	expectedOutputBeforeWeights := [][]float32{
		{-0.0008, -0.0205, -0.0021 /*...,*/, 0.0026, 0.0074, 0.0125},
		{-0.0008, -0.0203, -0.0021 /*...,*/, 0.0009, -0.0007, 0.0098},
		{-0.0007, -0.0192, -0.0020 /*...,*/, -0.0014, -0.0019, 0.0063},
		{0.0008, -0.0129, 0.0068 /*...,*/, -0.0021, 0.0015, 0.0024},
		{-0.0029, -0.0090, 0.0013 /*...,*/, -0.0022, 0.0031, 0.0028},
	}

	actualOutput, err := ml.MatMul(actualScores, actualValues)
	if err != nil {
		t.Fatal(err)
	}
	if actualOutput, err = actualOutput.Transpose(0, 1); err != nil {
		t.Fatal(err)
	}
	outputTrailingSize := actualOutput.GetElementCount() / sequenceLength
	if actualOutput, err = actualOutput.Reshape([]int{sequenceLength, outputTrailingSize}); err != nil {
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedOutputBeforeWeights, expectedOutputBeforeWeightsSize, actualOutput, common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	expectedOutputAfterWeightsSize := []int{5, 4096}
	expectedOutputAfterWeights := [][]float32{
		{-0.0123, -0.0359, -0.0091 /*...,*/, 0.0242, 0.0024, 0.0251},
		{-0.0201, -0.0116, -0.0104 /*...,*/, 0.0159, -0.0003, 0.0132},
		{-0.0074, -0.0097, 0.0061 /*...,*/, 0.0086, 0.0021, 0.0205},
		{-0.0053, -0.0055, -0.0024 /*...,*/, 0.0055, -0.0081, 0.0129},
		{-0.0111, -0.0150, -0.0038 /*...,*/, -0.0024, 0.0064, 0.0203},
	}
	/*
		Apply lat.attn_wo weights to output
	*/

	// lat.attn_wo: [out_features, in_features] -> shape: [4096 4096] -> [N_Heads * HeadDim, Dim]
	if actualOutput, err = ml.LinearTransformation(actualOutput, attention.attn_wo); err != nil {
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedOutputAfterWeights, expectedOutputAfterWeightsSize, actualOutput, common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	return actualOutput
}

func testTransformerBlock_FeedForward_Forward(t *testing.T, skipCompareTestTensor bool, infContext *InferenceContext, feedForward *LlamaFeedForward, x *ml.Tensor) *ml.Tensor {
	/*
		Goal in Python manner:
		self.w2(F.silu(self.w1(x)) * self.w3(x))
		-->
		self.ffn_down(F.silu(self.ffn_gate(x)) * self.ffn_up(x))
	*/
	h, err := ml.LinearTransformation(x, feedForward.ffn_gate)
	if err != nil {
		t.Fatal(err)
	}
	if h, err = ml.Silu(h); err != nil {
		t.Fatal(err)
	}
	ffnUpX, err := ml.LinearTransformation(x, feedForward.ffn_up)
	if err != nil {
		t.Fatal(err)
	}
	if h, err = ml.MultiplyElementwise(h, ffnUpX); err != nil {
		t.Fatal(err)
	}
	actualOutput, err := ml.LinearTransformation(h, feedForward.ffn_down)
	if err != nil {
		t.Fatal(err)
	}
	return actualOutput
}

func testTransformerBlock_Forward(t *testing.T, skipCompareTestTensor bool, infContext *InferenceContext, transformerBlock *LlamaTransformerBlock, x *ml.Tensor, startPos int, freqsCis *ml.Tensor, mask *ml.Tensor) *ml.Tensor {
	/*
		h, err := ltb.attention.Forward(infContext, normalizedX, startPos, freqsCis, mask)
	*/
	normalizedX := testTransformerBlock_AttnNorm_Forward(t, skipCompareTestTensor, transformerBlock, x)
	h := testTransformerBlock_Attention_Forward(t, skipCompareTestTensor, infContext, transformerBlock.attention, normalizedX, startPos, freqsCis, mask)

	expectedHBeforeFeedForwardSize := []int{5, 4096}
	expectedHBeforeFeedForward := [][]float32{
		{-0.0105, -0.0398, -0.0081 /*...,*/, 0.0151, 0.0051, 0.0214},
		{0.0172, -0.0154, -0.0042 /*...,*/, -0.0022, 0.0023, 0.0291},
		{-0.0205, -0.0244, 0.0171 /*...,*/, 0.0156, -0.0262, 0.0256},
		{-0.0067, -0.0153, -0.0118 /*...,*/, 0.0020, -0.0132, 0.0025},
		{-0.0162, -0.0132, 0.0056 /*...,*/, 0.0074, 0.0017, 0.0117},
	}

	var err error
	if h, err = ml.Add(x, h); err != nil {
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedHBeforeFeedForward, expectedHBeforeFeedForwardSize, h, common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}

	/*
		h, err = ltb.ffn_norm.Forward(context, h)
	*/
	expectedOutputSize := []int{5, 4096}
	expectedOutput := [][]float32{
		{-1.2817e-02, 8.7891e-03, 5.7373e-02 /*...,*/, -1.5991e-02, -5.5176e-02, 4.1992e-02},
		{5.9082e-02, 8.3008e-03, -9.1553e-05 /*...,*/, 3.4180e-02, 2.2430e-03, 3.1006e-02},
		{-1.3611e-02, -1.6724e-02, 3.0518e-02 /*...,*/, 2.3926e-02, -4.9072e-02, 3.0518e-02},
		{-2.3193e-03, -1.4832e-02, -2.8320e-02 /*...,*/, 9.3994e-03, -6.0120e-03, -7.1411e-03},
		{-7.2632e-03, -7.4158e-03, 1.7456e-02 /*...,*/, 1.1230e-02, 3.4485e-03, 1.1414e-02},
	}
	normalizedH, err := transformerBlock.ffn_norm.Forward(infContext, h)
	if err != nil {
		t.Fatal(err)
	}
	ffnOutput := testTransformerBlock_FeedForward_Forward(t, skipCompareTestTensor, infContext, transformerBlock.feedForward, normalizedH)
	actualOutput, err := ml.Add(h, ffnOutput)
	if err != nil {
		t.Fatal(err)
	}
	if err := ml.CompareTestTensorSkippable(skipCompareTestTensor, expectedOutput, expectedOutputSize, actualOutput, common.THRESHOLD_BF16, true); err != nil {
		t.Fatal(err)
	}
	return actualOutput
}

func testTransformer_Forward(t *testing.T, onlyFirstLayer bool, infContext *InferenceContext, transformer *LlamaTransformer, inputTokens *ml.Tensor, startPos int) *ml.Tensor {
	skipCompareTestTensor := !onlyFirstLayer || startPos > 0
	var err error
	actualInputTensor, actualFreqsCis, actualMask := testTransformer_Prepare(t, skipCompareTestTensor, transformer, inputTokens, startPos)

	currentTensor := actualInputTensor
	if onlyFirstLayer {
		firstLayer := transformer.Layers[0]
		currentTensor = testTransformerBlock_Forward(t, skipCompareTestTensor, infContext, firstLayer, currentTensor, startPos, actualFreqsCis, actualMask)
	} else {
		for layerIdx, layer := range transformer.Layers {
			infContext.Logf("Running transformer block layer: %d / %d\n", layerIdx+1, len(transformer.Layers))
			currentTensor = testTransformerBlock_Forward(t, true, infContext, layer, currentTensor, startPos, actualFreqsCis, actualMask)
		}
	}
	if currentTensor, err = transformer.output_norm.Forward(infContext, currentTensor); err != nil {
		t.Fatal(err)
	}
	output, err := ml.LinearTransformation(currentTensor, transformer.output)
	if err != nil {
		t.Fatal(err)
	}
	if output, err = output.ToFloat32(); err != nil {
		t.Fatal(err)
	}
	return output
}

func testSimulatedLog(format string, v ...any) {
	fmt.Printf(format, v...)
}

func testSimulatedInternal(t *testing.T, onlyFirstLayer bool) {
	modelDir := "../../models-original/7B-chat"
	if _, err := os.Stat(modelDir); err != nil {
		t.Skipf("Model directory \"%s\" is not found, passing this test: %s", modelDir, "TestSimulated")
		return
	}
	var err error
	common.GLogger, err = common.NewLogger(os.Stdout, nil)
	if err != nil {
		panic(err)
	}
	defer common.GLogger.Close()

	llamaModel, err := LoadModel(modelDir)
	if err != nil {
		t.Fatal(err)
	}
	defer llamaModel.Free()

	// promptTokens: "<BOS>My name is"
	promptTokens := []TokenId{1, 15043, 590, 1024, 338}

	inferenceArgs := common.NewInferenceArgs()
	inferenceArgs.SequenceLength = 8
	infContext := NewInferenceContext(llamaModel, inferenceArgs, testSimulatedLog)

	tokens, err := ml.Full([]int{infContext.SequenceLength}, ml.DT_INT32, int32(llamaModel.Vocabulary.PadId))
	if err != nil {
		t.Fatal(err)
	}
	for i, token := range promptTokens {
		if err := tokens.SetItem([]int{i}, int32(token)); err != nil {
			t.Fatal(err)
		}
	}

	prevPos := 0
	minPromptLength := len(promptTokens)
	isFirstIteration := true
	for curPos := minPromptLength; curPos < infContext.SequenceLength; curPos++ {
		inputTokensSlice, err := tokens.Slice([]int{prevPos}, []int{curPos})
		if err != nil {
			t.Fatal(err)
		}

		actualLogits := testTransformer_Forward(t, onlyFirstLayer, infContext, llamaModel.Transformer, inputTokensSlice, prevPos)
		if onlyFirstLayer && isFirstIteration {
			// Although it is a valid and significant output as a tensor, it isn't a meaningful outcome,
			// because while producing this output, only first attention layer was run, not completely 32 of them.
			// But it is valuable to validate the whole model flow mathematically,
			// even if we had some precision differences between ours and original LLaMA Python code.

			expectedLogitsOnlyFirstLayerSize := []int{5, 32000}
			expectedLogitsOnlyFirstLayer := [][]float32{
				{1.4141, 5.5000, 1.1797 /*...,*/, 2.1719, 1.9375, -1.9766},
				{1.1562, -1.2734, -2.8125 /*...,*/, -1.8984, 1.3672, -0.5781},
				{0.1797, 0.3613, -1.7031 /*...,*/, -0.4492, 3.0156, -1.9141},
				{-0.1689, -1.5000, -2.5000 /*...,*/, -1.4922, 2.3594, -2.2656},
				{-0.7773, -1.4453, -0.3828 /*...,*/, -1.6562, 2.1094, -1.1641}}
			if err := ml.CompareTestTensorSkippable(!onlyFirstLayer, expectedLogitsOnlyFirstLayer, expectedLogitsOnlyFirstLayerSize, actualLogits, 30*common.THRESHOLD_BF16, true); err != nil {
				t.Fatal(err)
			}
		}
		if actualLogits, err = actualLogits.Slice([]int{actualLogits.Size[0] - 1}, []int{actualLogits.Size[0]}); err != nil {
			t.Fatal(err)
		}
		if onlyFirstLayer && isFirstIteration {
			expectedLogitsLastRowOnlyFirstLayerSize := []int{1, 32000}
			expectedLogitsLastRowOnlyFirstLayer := [][]float32{
				{-0.7773, -1.4453, -0.3828 /*...,*/, -1.6562, 2.1094, -1.1641},
			}
			if err := ml.CompareTestTensorSkippable(!onlyFirstLayer, expectedLogitsLastRowOnlyFirstLayer, expectedLogitsLastRowOnlyFirstLayerSize, actualLogits, 30*common.THRESHOLD_BF16, true); err != nil {
				t.Fatal(err)
			}
		}
		actualNextToken, err := ml.Argmax(actualLogits, len(actualLogits.Size)-1) // shape=[1,1] dtype=DT_INT32
		if err != nil {
			t.Fatal(err)
		}
		if onlyFirstLayer && isFirstIteration {
			expectedNextTokenOnlyFirstLayerSize := []int{1}
			expectedNextTokenOnlyFirstLayer := []float32{
				21961,
			}
			if err := ml.CompareTestTensorSkippable(!onlyFirstLayer, expectedNextTokenOnlyFirstLayer, expectedNextTokenOnlyFirstLayerSize, actualNextToken, common.THRESHOLD_EXACT, true); err != nil {
				t.Fatal(err)
			}
		}
		actualNextTokenId := TokenId(actualNextToken.Item().(int32))
		// Comment in original Python code: only replace token if prompt has already been generated
		existingToken, err := tokens.GetItem([]int{curPos})
		if err != nil {
			t.Fatal(err)
		}
		existingTokenId := TokenId(existingToken.(int32))
		if existingTokenId != llamaModel.Vocabulary.PadId {
			actualNextTokenId = existingTokenId
		}
		if err = tokens.SetItem([]int{curPos}, int32(actualNextTokenId)); err != nil {
			t.Fatal(err)
		}
		actualEosReached := actualNextTokenId == llamaModel.Vocabulary.EndOfSentenceId
		if onlyFirstLayer && isFirstIteration {
			expectedEosReached := false
			if actualEosReached != expectedEosReached {
				t.Fatalf("expected eosReached %v, but got %v", expectedEosReached, actualEosReached)
			}
		}
		prevPos = curPos
		isFirstIteration = false
	}

	if onlyFirstLayer {
		expectedOutputTokenIds := []TokenId{21961, 6604, 22697}
		actualOutputTokenIds := make([]TokenId, 0)
		for i := minPromptLength; i < infContext.SequenceLength; i++ {
			tokenItem, err := tokens.GetItem([]int{i})
			if err != nil {
				t.Fatal(err)
			}
			actualOutputTokenIds = append(actualOutputTokenIds, TokenId(tokenItem.(int32)))
		}
		if !reflect.DeepEqual(expectedOutputTokenIds, actualOutputTokenIds) {
			t.Fatalf("expected actualOutputTokenIds %v, but got %v", expectedOutputTokenIds, actualOutputTokenIds)
		}
	}
}

func TestSimulatedOnlyFirstLayer(t *testing.T) {
	t.Setenv("test.timeout", "10m")
	testSimulatedInternal(t, true)
}

const runTestSimulatedFull = false

func TestSimulatedFull(t *testing.T) {
	if !runTestSimulatedFull {
		t.Skip("Skipping TestSimulatedFull because runTestSimulatedFull is set to false")
	}
	testSimulatedInternal(t, false)
}

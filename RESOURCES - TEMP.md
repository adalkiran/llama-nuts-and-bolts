Maybe a tip to show .pth file is uncompressed zip file and can be memory mapped:
unzip -vl consolidated.00.pth


LLaMA 2 model
https://github.com/facebookresearch/llama
https://huggingface.co/meta-llama/Llama-2-7b-hf/tree/main

https://github.com/hkproj/pytorch-llama ????
https://www.youtube.com/watch?v=Mn_9W1nCFLo (Umar Jamil - LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU)

Pickle Format
https://github.com/ggerganov/llama.cpp/blob/master/convert.py
https://github.com/python/cpython/blob/main/Lib/pickle.py
https://github.com/nlpodyssey/gopickle


Vocab Format (Protobuf)

https://github.com/google/sentencepiece
https://github.com/protocolbuffers/protobuf
https://github.com/protocolbuffers/protoscope
https://protobuf.dev/getting-started/gotutorial/ ????
https://protobuf.dev/programming-guides/encoding/ ????
https://github.com/protocolbuffers/protoscope/blob/main/writer.go


Videos

https://www.youtube.com/watch?v=kCc8FmEb1nY (Andrej Karpathy - Let's build GPT: from scratch, in code, spelled out.)
https://www.youtube.com/watch?v=OxCpWwDCDFQ (Serrano.Academy - The Attention Mechanism in Large Language Models)
https://www.youtube.com/watch?v=UPtG_38Oq8o (Serrano.Academy - The math behind Attention: Keys, Queries, and Values matrices)
https://www.youtube.com/watch?v=qaWMOYf4ri8 (Serrano.Academy - What are Transformer Models and how do they work?)
https://www.youtube.com/watch?v=bQ5BoolX9Ag (StatQuest with Josh Starmer - Decoder-Only Transformers, ChatGPTs specific Transformer, Clearly Explained!!!)